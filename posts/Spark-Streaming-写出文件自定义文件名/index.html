<!DOCTYPE html><html lang="zh-CN" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="day-prompt" content="天前"><meta name="hour-prompt" content="小时前"><meta name="minute-prompt" content="分钟前"><meta name="justnow-prompt" content="刚刚"><meta name="generator" content="Jekyll v4.3.4" /><meta property="og:title" content="Spark Streaming - 写出文件自定义文件名" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="1.背景" /><meta property="og:description" content="1.背景" /><link rel="canonical" href="https://zcyoop.github.io/posts/Spark-Streaming-%E5%86%99%E5%87%BA%E6%96%87%E4%BB%B6%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E4%BB%B6%E5%90%8D/" /><meta property="og:url" content="https://zcyoop.github.io/posts/Spark-Streaming-%E5%86%99%E5%87%BA%E6%96%87%E4%BB%B6%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E4%BB%B6%E5%90%8D/" /><meta property="og:site_name" content="Zcyoop" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2019-05-30T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Spark Streaming - 写出文件自定义文件名" /><meta name="twitter:site" content="@twitter_username" /><meta name="google-site-verification" content="bYaaJN_P1KEMPHSc3NYLCjx75e0bMEvAtt54VYZ3jJw" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2019-05-30T00:00:00+08:00","datePublished":"2019-05-30T00:00:00+08:00","description":"1.背景","headline":"Spark Streaming - 写出文件自定义文件名","mainEntityOfPage":{"@type":"WebPage","@id":"https://zcyoop.github.io/posts/Spark-Streaming-%E5%86%99%E5%87%BA%E6%96%87%E4%BB%B6%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E4%BB%B6%E5%90%8D/"},"url":"https://zcyoop.github.io/posts/Spark-Streaming-%E5%86%99%E5%87%BA%E6%96%87%E4%BB%B6%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E4%BB%B6%E5%90%8D/"}</script><title>Spark Streaming - 写出文件自定义文件名 | Zcyoop</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Zcyoop"><meta name="application-name" content="Zcyoop"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="https://cdn.jsdelivr.net"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1.1.0/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end" lang="zh-CN"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="https://blog-1253533258.cos.ap-shanghai.myqcloud.com/piggo/39327876.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Zcyoop</a></div><div class="site-subtitle font-italic">Zcyoop Blog</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>归档</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/zcyoop" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['zcy2nn','qq.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>Spark Streaming - 写出文件自定义文件名</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Spark Streaming - 写出文件自定义文件名</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="semi-bold"> zcyoop </span> 发表于 <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="2019-05-30, 00:00 +0800" >2019-05-30<i class="unloaded">2019-05-30T00:00:00+08:00</i> </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2166 字">12 分钟 阅读</span></div></div><div class="post-content"><h3 id="1背景">1.背景</h3><p>​ 在工作中碰到了个需求，需要将<code class="language-plaintext highlighter-rouge">Spark Streaming</code>中的文件写入到<code class="language-plaintext highlighter-rouge">Hive</code>表中，但是<code class="language-plaintext highlighter-rouge">Spark Streaming</code>中的<code class="language-plaintext highlighter-rouge">saveAsTextFiles</code>会自己定义很多文件夹，不符合<code class="language-plaintext highlighter-rouge">Hive</code>读取文件的规范且<code class="language-plaintext highlighter-rouge">saveAsTextFiles</code>中的参数只能定义文件夹的名字，第二个是采用<code class="language-plaintext highlighter-rouge">Spark Streaming</code>中的<code class="language-plaintext highlighter-rouge">foreachRDD</code>，这个方法会将<code class="language-plaintext highlighter-rouge">DStream</code>转成再进行操作，但是<code class="language-plaintext highlighter-rouge">Spark Streaming</code>中的是多批次处理的结构，也就是很多RDD，每个RDD的<code class="language-plaintext highlighter-rouge">saveAsTextFile</code>都会将前面的数据覆盖，所以最终采用的方法是重写<code class="language-plaintext highlighter-rouge">saveAsTextFile</code>输出时的文件名</p><h3 id="2分析">2.分析</h3><h4 id="21-分析代码">2.1 分析代码</h4><p>既然是重写<code class="language-plaintext highlighter-rouge">saveAsTextFile</code>输出逻辑，那先看看他是如何实现输出的</p><div class="language-scala highlighter-rouge"><div class="code-header"> <span text-data=" Scala "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">saveAsTextFile</span><span class="o">(</span><span class="n">path</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="n">withScope</span> <span class="o">{</span>
    <span class="c1">// https://issues.apache.org/jira/browse/SPARK-2075</span>
    <span class="c1">//</span>
    <span class="c1">// NullWritable is a `Comparable` in Hadoop 1.+, so the compiler cannot find an implicit</span>
    <span class="c1">// Ordering for it and will use the default `null`. However, it's a `Comparable[NullWritable]`</span>
    <span class="c1">// in Hadoop 2.+, so the compiler will call the implicit `Ordering.ordered` method to create an</span>
    <span class="c1">// Ordering for `NullWritable`. That's why the compiler will generate different anonymous</span>
    <span class="c1">// classes for `saveAsTextFile` in Hadoop 1.+ and Hadoop 2.+.</span>
    <span class="c1">//</span>
    <span class="c1">// Therefore, here we provide an explicit Ordering `null` to make sure the compiler generate</span>
    <span class="c1">// same bytecodes for `saveAsTextFile`.</span>
    <span class="k">val</span> <span class="nv">nullWritableClassTag</span> <span class="k">=</span> <span class="n">implicitly</span><span class="o">[</span><span class="kt">ClassTag</span><span class="o">[</span><span class="kt">NullWritable</span><span class="o">]]</span>
    <span class="k">val</span> <span class="nv">textClassTag</span> <span class="k">=</span> <span class="n">implicitly</span><span class="o">[</span><span class="kt">ClassTag</span><span class="o">[</span><span class="kt">Text</span><span class="o">]]</span>
    <span class="k">val</span> <span class="nv">r</span> <span class="k">=</span> <span class="k">this</span><span class="o">.</span><span class="py">mapPartitions</span> <span class="o">{</span> <span class="n">iter</span> <span class="k">=&gt;</span>
      <span class="k">val</span> <span class="nv">text</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Text</span><span class="o">()</span>
      <span class="nv">iter</span><span class="o">.</span><span class="py">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span>
        <span class="nv">text</span><span class="o">.</span><span class="py">set</span><span class="o">(</span><span class="nv">x</span><span class="o">.</span><span class="py">toString</span><span class="o">)</span>
        <span class="o">(</span><span class="nv">NullWritable</span><span class="o">.</span><span class="py">get</span><span class="o">(),</span> <span class="n">text</span><span class="o">)</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="nv">RDD</span><span class="o">.</span><span class="py">rddToPairRDDFunctions</span><span class="o">(</span><span class="n">r</span><span class="o">)(</span><span class="n">nullWritableClassTag</span><span class="o">,</span> <span class="n">textClassTag</span><span class="o">,</span> <span class="kc">null</span><span class="o">)</span>
      <span class="o">.</span><span class="py">saveAsHadoopFile</span><span class="o">[</span><span class="kt">TextOutputFormat</span><span class="o">[</span><span class="kt">NullWritable</span>, <span class="kt">Text</span><span class="o">]](</span><span class="n">path</span><span class="o">)</span>
  <span class="o">}</span>
</pre></table></code></div></div><p>可以看出<code class="language-plaintext highlighter-rouge">saveAsTextFile</code>是依赖<code class="language-plaintext highlighter-rouge">saveAsHadoopFile</code>进行输出，因为<code class="language-plaintext highlighter-rouge">saveAsHadoopFile</code>接受<code class="language-plaintext highlighter-rouge">PairRDD</code>，所以在<code class="language-plaintext highlighter-rouge">saveAsTextFile</code>中通过<code class="language-plaintext highlighter-rouge">rddToPairRDDFunctions</code>转成(NullWritable,Text)类型的RDD，再通过<code class="language-plaintext highlighter-rouge">saveAsHadoopFile</code>进行输出</p><p>可以看出输出的逻辑还是Hadoop的那一套，所以我们可以通过重写<code class="language-plaintext highlighter-rouge">TextOutputFormat</code>来解决输出文件名的相同的问题</p><h4 id="22-代码编写">2.2 代码编写</h4><h5 id="221-saveashadoopfile算子">2.2.1 saveAsHadoopFile算子</h5><p>首先先看下官方提供的<code class="language-plaintext highlighter-rouge">saveAsHadoopFile</code>算子说明</p><div class="language-scala highlighter-rouge"><div class="code-header"> <span text-data=" Scala "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
</pre><td class="rouge-code"><pre><span class="cm">/**
   * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class
   * supporting the key and value types K and V in this RDD.
   */</span>
  <span class="k">def</span> <span class="nf">saveAsHadoopFile</span><span class="o">[</span><span class="kt">F</span> <span class="k">&lt;:</span> <span class="kt">OutputFormat</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]](</span>
      <span class="n">path</span><span class="k">:</span> <span class="kt">String</span><span class="o">)(</span><span class="k">implicit</span> <span class="n">fm</span><span class="k">:</span> <span class="kt">ClassTag</span><span class="o">[</span><span class="kt">F</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="nv">self</span><span class="o">.</span><span class="py">withScope</span> <span class="o">{</span>
    <span class="nf">saveAsHadoopFile</span><span class="o">(</span><span class="n">path</span><span class="o">,</span> <span class="n">keyClass</span><span class="o">,</span> <span class="n">valueClass</span><span class="o">,</span> <span class="nv">fm</span><span class="o">.</span><span class="py">runtimeClass</span><span class="o">.</span><span class="py">asInstanceOf</span><span class="o">[</span><span class="kt">Class</span><span class="o">[</span><span class="kt">F</span><span class="o">]])</span>
  <span class="o">}</span>
<span class="cm">/**
   * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class
   * supporting the key and value types K and V in this RDD. Compress the result with the
   * supplied codec.
   */</span>
  <span class="k">def</span> <span class="nf">saveAsHadoopFile</span><span class="o">[</span><span class="kt">F</span> <span class="k">&lt;:</span> <span class="kt">OutputFormat</span><span class="o">[</span><span class="kt">K</span>, <span class="kt">V</span><span class="o">]](</span>
      <span class="n">path</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
      <span class="n">codec</span><span class="k">:</span> <span class="kt">Class</span><span class="o">[</span><span class="k">_</span> <span class="k">&lt;:</span> <span class="kt">CompressionCodec</span><span class="o">])(</span><span class="k">implicit</span> <span class="n">fm</span><span class="k">:</span> <span class="kt">ClassTag</span><span class="o">[</span><span class="kt">F</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="nv">self</span><span class="o">.</span><span class="py">withScope</span> <span class="o">{</span>
    <span class="k">val</span> <span class="nv">runtimeClass</span> <span class="k">=</span> <span class="nv">fm</span><span class="o">.</span><span class="py">runtimeClass</span>
    <span class="nf">saveAsHadoopFile</span><span class="o">(</span><span class="n">path</span><span class="o">,</span> <span class="n">keyClass</span><span class="o">,</span> <span class="n">valueClass</span><span class="o">,</span> <span class="nv">runtimeClass</span><span class="o">.</span><span class="py">asInstanceOf</span><span class="o">[</span><span class="kt">Class</span><span class="o">[</span><span class="kt">F</span><span class="o">]],</span> <span class="n">codec</span><span class="o">)</span>
  <span class="o">}</span>
 <span class="cm">/**
   * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class
   * supporting the key and value types K and V in this RDD. Compress with the supplied codec.
   */</span>
<span class="k">def</span> <span class="nf">saveAsHadoopFile</span><span class="o">(</span>
      <span class="n">path</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
      <span class="n">keyClass</span><span class="k">:</span> <span class="kt">Class</span><span class="o">[</span><span class="k">_</span><span class="o">],</span>
      <span class="n">valueClass</span><span class="k">:</span> <span class="kt">Class</span><span class="o">[</span><span class="k">_</span><span class="o">],</span>
      <span class="n">outputFormatClass</span><span class="k">:</span> <span class="kt">Class</span><span class="o">[</span><span class="k">_</span> <span class="k">&lt;:</span> <span class="kt">OutputFormat</span><span class="o">[</span><span class="k">_</span>, <span class="k">_</span><span class="o">]],</span>
      <span class="n">codec</span><span class="k">:</span> <span class="kt">Class</span><span class="o">[</span><span class="k">_</span> <span class="k">&lt;:</span> <span class="kt">CompressionCodec</span><span class="o">])</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="nv">self</span><span class="o">.</span><span class="py">withScope</span> <span class="o">{</span>
    <span class="nf">saveAsHadoopFile</span><span class="o">(</span><span class="n">path</span><span class="o">,</span> <span class="n">keyClass</span><span class="o">,</span> <span class="n">valueClass</span><span class="o">,</span> <span class="n">outputFormatClass</span><span class="o">,</span>
      <span class="k">new</span> <span class="nc">JobConf</span><span class="o">(</span><span class="nv">self</span><span class="o">.</span><span class="py">context</span><span class="o">.</span><span class="py">hadoopConfiguration</span><span class="o">),</span> <span class="nc">Some</span><span class="o">(</span><span class="n">codec</span><span class="o">))</span>
  <span class="o">}</span>
<span class="cm">/**
   * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class
   * supporting the key and value types K and V in this RDD.
   *
   * Note that, we should make sure our tasks are idempotent when speculation is enabled, i.e. do
   * not use output committer that writes data directly.
   * There is an example in https://issues.apache.org/jira/browse/SPARK-10063 to show the bad
   * result of using direct output committer with speculation enabled.
   */</span>
  <span class="k">def</span> <span class="nf">saveAsHadoopFile</span><span class="o">(</span>
      <span class="n">path</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span>
      <span class="n">keyClass</span><span class="k">:</span> <span class="kt">Class</span><span class="o">[</span><span class="k">_</span><span class="o">],</span>
      <span class="n">valueClass</span><span class="k">:</span> <span class="kt">Class</span><span class="o">[</span><span class="k">_</span><span class="o">],</span>
      <span class="n">outputFormatClass</span><span class="k">:</span> <span class="kt">Class</span><span class="o">[</span><span class="k">_</span> <span class="k">&lt;:</span> <span class="kt">OutputFormat</span><span class="o">[</span><span class="k">_</span>, <span class="k">_</span><span class="o">]],</span>
      <span class="n">conf</span><span class="k">:</span> <span class="kt">JobConf</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">JobConf</span><span class="o">(</span><span class="nv">self</span><span class="o">.</span><span class="py">context</span><span class="o">.</span><span class="py">hadoopConfiguration</span><span class="o">),</span>
      <span class="n">codec</span><span class="k">:</span> <span class="kt">Option</span><span class="o">[</span><span class="kt">Class</span><span class="o">[</span><span class="k">_</span> <span class="k">&lt;:</span> <span class="kt">CompressionCodec</span><span class="o">]]</span> <span class="k">=</span> <span class="nc">None</span><span class="o">)</span><span class="k">:</span> <span class="kt">Unit</span> <span class="o">=</span> <span class="nv">self</span><span class="o">.</span><span class="py">withScope</span> <span class="o">{...}</span>
</pre></table></code></div></div><p>这里我们使用的是<code class="language-plaintext highlighter-rouge">def saveAsHadoopFile(path: String, keyClass: Class[_],valueClass: Class[_], outputFormatClass: Class[_ &lt;: OutputFormat[_, _]], codec: Class[_ &lt;: CompressionCodec]): Unit = self.withScope { }</code></p><p>依次传入 path：路径、keyClass：key类型、valueClass：value类型、outputFormatClass：outformat方式，剩下两个参数为默认值</p><h5 id="222-multipletextoutputformat分析">2.2.2 MultipleTextOutputFormat分析</h5><div class="language-scala highlighter-rouge"><div class="code-header"> <span text-data=" Scala "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
192
193
194
195
196
197
198
</pre><td class="rouge-code"><pre><span class="cm">/**
 * This abstract class extends the FileOutputFormat, allowing to write the
 * output data to different output files. There are three basic use cases for
 * this class.
 * 
 * Case one: This class is used for a map reduce job with at least one reducer.
 * The reducer wants to write data to different files depending on the actual
 * keys. It is assumed that a key (or value) encodes the actual key (value)
 * and the desired location for the actual key (value).
 * 
 * Case two: This class is used for a map only job. The job wants to use an
 * output file name that is either a part of the input file name of the input
 * data, or some derivation of it.
 * 
 * Case three: This class is used for a map only job. The job wants to use an
 * output file name that depends on both the keys and the input file name,
 */</span>
<span class="nd">@InterfaceAudience</span><span class="o">.</span><span class="py">Public</span>
<span class="nd">@InterfaceStability</span><span class="o">.</span><span class="py">Stable</span>
<span class="n">public</span> <span class="k">abstract</span> <span class="k">class</span> <span class="nc">MultipleOutputFormat</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span>
<span class="k">extends</span> <span class="nc">FileOutputFormat</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="o">{</span>

  <span class="cm">/**
   * Create a composite record writer that can write key/value data to different
   * output files
   * 
   * @param fs
   *          the file system to use
   * @param job
   *          the job conf for the job
   * @param name
   *          the leaf file name for the output file (such as part-00000")
   * @param arg3
   *          a progressable for reporting progress.
   * @return a composite record writer
   * @throws IOException
   */</span>
  <span class="n">public</span> <span class="nc">RecordWriter</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="nf">getRecordWriter</span><span class="o">(</span><span class="nc">FileSystem</span> <span class="n">fs</span><span class="o">,</span> <span class="nc">JobConf</span> <span class="n">job</span><span class="o">,</span>
      <span class="nc">String</span> <span class="n">name</span><span class="o">,</span> <span class="nc">Progressable</span> <span class="n">arg3</span><span class="o">)</span> <span class="n">throws</span> <span class="nc">IOException</span> <span class="o">{</span>

    <span class="k">final</span> <span class="nc">FileSystem</span> <span class="n">myFS</span> <span class="k">=</span> <span class="n">fs</span><span class="o">;</span>
    <span class="k">final</span> <span class="nc">String</span> <span class="n">myName</span> <span class="k">=</span> <span class="nf">generateLeafFileName</span><span class="o">(</span><span class="n">name</span><span class="o">);</span>
    <span class="k">final</span> <span class="nc">JobConf</span> <span class="n">myJob</span> <span class="k">=</span> <span class="n">job</span><span class="o">;</span>
    <span class="k">final</span> <span class="nc">Progressable</span> <span class="n">myProgressable</span> <span class="k">=</span> <span class="n">arg3</span><span class="o">;</span>

    <span class="k">return</span> <span class="k">new</span> <span class="nc">RecordWriter</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;()</span> <span class="o">{</span>

      <span class="c1">// a cache storing the record writers for different output files.</span>
      <span class="nc">TreeMap</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">RecordWriter</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;&gt;</span> <span class="n">recordWriters</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">TreeMap</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">RecordWriter</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;&gt;();</span>

      <span class="n">public</span> <span class="n">void</span> <span class="nf">write</span><span class="o">(</span><span class="n">K</span> <span class="n">key</span><span class="o">,</span> <span class="n">V</span> <span class="n">value</span><span class="o">)</span> <span class="n">throws</span> <span class="nc">IOException</span> <span class="o">{</span>

        <span class="c1">// get the file name based on the key</span>
        <span class="nc">String</span> <span class="n">keyBasedPath</span> <span class="k">=</span> <span class="nf">generateFileNameForKeyValue</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">value</span><span class="o">,</span> <span class="n">myName</span><span class="o">);</span>

        <span class="c1">// get the file name based on the input file name</span>
        <span class="nc">String</span> <span class="n">finalPath</span> <span class="k">=</span> <span class="nf">getInputFileBasedOutputFileName</span><span class="o">(</span><span class="n">myJob</span><span class="o">,</span> <span class="n">keyBasedPath</span><span class="o">);</span>

        <span class="c1">// get the actual key</span>
        <span class="n">K</span> <span class="n">actualKey</span> <span class="k">=</span> <span class="nf">generateActualKey</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">value</span><span class="o">);</span>
        <span class="n">V</span> <span class="n">actualValue</span> <span class="k">=</span> <span class="nf">generateActualValue</span><span class="o">(</span><span class="n">key</span><span class="o">,</span> <span class="n">value</span><span class="o">);</span>

        <span class="nc">RecordWriter</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="n">rw</span> <span class="k">=</span> <span class="k">this</span><span class="o">.</span><span class="py">recordWriters</span><span class="o">.</span><span class="py">get</span><span class="o">(</span><span class="n">finalPath</span><span class="o">);</span>
        <span class="nf">if</span> <span class="o">(</span><span class="n">rw</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
          <span class="c1">// if we don't have the record writer yet for the final path, create</span>
          <span class="c1">// one</span>
          <span class="c1">// and add it to the cache</span>
          <span class="n">rw</span> <span class="k">=</span> <span class="nf">getBaseRecordWriter</span><span class="o">(</span><span class="n">myFS</span><span class="o">,</span> <span class="n">myJob</span><span class="o">,</span> <span class="n">finalPath</span><span class="o">,</span> <span class="n">myProgressable</span><span class="o">);</span>
          <span class="k">this</span><span class="o">.</span><span class="py">recordWriters</span><span class="o">.</span><span class="py">put</span><span class="o">(</span><span class="n">finalPath</span><span class="o">,</span> <span class="n">rw</span><span class="o">);</span>
        <span class="o">}</span>
        <span class="nv">rw</span><span class="o">.</span><span class="py">write</span><span class="o">(</span><span class="n">actualKey</span><span class="o">,</span> <span class="n">actualValue</span><span class="o">);</span>
      <span class="o">};</span>

      <span class="n">public</span> <span class="n">void</span> <span class="nf">close</span><span class="o">(</span><span class="nc">Reporter</span> <span class="n">reporter</span><span class="o">)</span> <span class="n">throws</span> <span class="nc">IOException</span> <span class="o">{</span>
        <span class="nc">Iterator</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">keys</span> <span class="k">=</span> <span class="k">this</span><span class="o">.</span><span class="py">recordWriters</span><span class="o">.</span><span class="py">keySet</span><span class="o">().</span><span class="py">iterator</span><span class="o">();</span>
        <span class="nf">while</span> <span class="o">(</span><span class="nv">keys</span><span class="o">.</span><span class="py">hasNext</span><span class="o">())</span> <span class="o">{</span>
          <span class="nc">RecordWriter</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="n">rw</span> <span class="k">=</span> <span class="k">this</span><span class="o">.</span><span class="py">recordWriters</span><span class="o">.</span><span class="py">get</span><span class="o">(</span><span class="nv">keys</span><span class="o">.</span><span class="py">next</span><span class="o">());</span>
          <span class="nv">rw</span><span class="o">.</span><span class="py">close</span><span class="o">(</span><span class="n">reporter</span><span class="o">);</span>
        <span class="o">}</span>
        <span class="k">this</span><span class="o">.</span><span class="py">recordWriters</span><span class="o">.</span><span class="py">clear</span><span class="o">();</span>
      <span class="o">};</span>
    <span class="o">};</span>
  <span class="o">}</span>

  <span class="cm">/**
   * Generate the leaf name for the output file name. The default behavior does
   * not change the leaf file name (such as part-00000)
   * 
   * @param name
   *          the leaf file name for the output file
   * @return the given leaf file name
   */</span>
  <span class="k">protected</span> <span class="nc">String</span> <span class="nf">generateLeafFileName</span><span class="o">(</span><span class="nc">String</span> <span class="n">name</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">name</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="cm">/**
   * Generate the file output file name based on the given key and the leaf file
   * name. The default behavior is that the file name does not depend on the
   * key.
   * 
   * @param key
   *          the key of the output data
   * @param name
   *          the leaf file name
   * @return generated file name
   */</span>
  <span class="k">protected</span> <span class="nc">String</span> <span class="nf">generateFileNameForKeyValue</span><span class="o">(</span><span class="n">K</span> <span class="n">key</span><span class="o">,</span> <span class="n">V</span> <span class="n">value</span><span class="o">,</span> <span class="nc">String</span> <span class="n">name</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">name</span><span class="o">;</span>
  <span class="o">}</span>

  <span class="cm">/**
   * Generate the actual key from the given key/value. The default behavior is that
   * the actual key is equal to the given key
   * 
   * @param key
   *          the key of the output data
   * @param value
   *          the value of the output data
   * @return the actual key derived from the given key/value
   */</span>
  <span class="k">protected</span> <span class="n">K</span> <span class="nf">generateActualKey</span><span class="o">(</span><span class="n">K</span> <span class="n">key</span><span class="o">,</span> <span class="n">V</span> <span class="n">value</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">key</span><span class="o">;</span>
  <span class="o">}</span>
  
  <span class="cm">/**
   * Generate the actual value from the given key and value. The default behavior is that
   * the actual value is equal to the given value
   * 
   * @param key
   *          the key of the output data
   * @param value
   *          the value of the output data
   * @return the actual value derived from the given key/value
   */</span>
  <span class="k">protected</span> <span class="n">V</span> <span class="nf">generateActualValue</span><span class="o">(</span><span class="n">K</span> <span class="n">key</span><span class="o">,</span> <span class="n">V</span> <span class="n">value</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">return</span> <span class="n">value</span><span class="o">;</span>
  <span class="o">}</span>
  

  <span class="cm">/**
   * Generate the outfile name based on a given anme and the input file name. If
   * the {@link JobContext#MAP_INPUT_FILE} does not exists (i.e. this is not for a map only job),
   * the given name is returned unchanged. If the config value for
   * "num.of.trailing.legs.to.use" is not set, or set 0 or negative, the given
   * name is returned unchanged. Otherwise, return a file name consisting of the
   * N trailing legs of the input file name where N is the config value for
   * "num.of.trailing.legs.to.use".
   * 
   * @param job
   *          the job config
   * @param name
   *          the output file name
   * @return the outfile name based on a given anme and the input file name.
   */</span>
  <span class="k">protected</span> <span class="nc">String</span> <span class="nf">getInputFileBasedOutputFileName</span><span class="o">(</span><span class="nc">JobConf</span> <span class="n">job</span><span class="o">,</span> <span class="nc">String</span> <span class="n">name</span><span class="o">)</span> <span class="o">{</span>
    <span class="nc">String</span> <span class="n">infilepath</span> <span class="k">=</span> <span class="nv">job</span><span class="o">.</span><span class="py">get</span><span class="o">(</span><span class="nv">MRJobConfig</span><span class="o">.</span><span class="py">MAP_INPUT_FILE</span><span class="o">);</span>
    <span class="nf">if</span> <span class="o">(</span><span class="n">infilepath</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
      <span class="c1">// if the {@link JobContext#MAP_INPUT_FILE} does not exists,</span>
      <span class="c1">// then return the given name</span>
      <span class="k">return</span> <span class="n">name</span><span class="o">;</span>
    <span class="o">}</span>
    <span class="n">int</span> <span class="n">numOfTrailingLegsToUse</span> <span class="k">=</span> <span class="nv">job</span><span class="o">.</span><span class="py">getInt</span><span class="o">(</span><span class="s">"mapred.outputformat.numOfTrailingLegs"</span><span class="o">,</span> <span class="mi">0</span><span class="o">);</span>
    <span class="nf">if</span> <span class="o">(</span><span class="n">numOfTrailingLegsToUse</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">return</span> <span class="n">name</span><span class="o">;</span>
    <span class="o">}</span>
    <span class="nc">Path</span> <span class="n">infile</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Path</span><span class="o">(</span><span class="n">infilepath</span><span class="o">);</span>
    <span class="nc">Path</span> <span class="n">parent</span> <span class="k">=</span> <span class="nv">infile</span><span class="o">.</span><span class="py">getParent</span><span class="o">();</span>
    <span class="nc">String</span> <span class="n">midName</span> <span class="k">=</span> <span class="nv">infile</span><span class="o">.</span><span class="py">getName</span><span class="o">();</span>
    <span class="nc">Path</span> <span class="n">outPath</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Path</span><span class="o">(</span><span class="n">midName</span><span class="o">);</span>
    <span class="nf">for</span> <span class="o">(</span><span class="n">int</span> <span class="n">i</span> <span class="k">=</span> <span class="mi">1</span><span class="o">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">numOfTrailingLegsToUse</span><span class="o">;</span> <span class="n">i</span><span class="o">++)</span> <span class="o">{</span>
      <span class="nf">if</span> <span class="o">(</span><span class="n">parent</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="n">break</span><span class="o">;</span>
      <span class="n">midName</span> <span class="k">=</span> <span class="nv">parent</span><span class="o">.</span><span class="py">getName</span><span class="o">();</span>
      <span class="nf">if</span> <span class="o">(</span><span class="nv">midName</span><span class="o">.</span><span class="py">length</span><span class="o">()</span> <span class="o">==</span> <span class="mi">0</span><span class="o">)</span> <span class="n">break</span><span class="o">;</span>
      <span class="n">parent</span> <span class="k">=</span> <span class="nv">parent</span><span class="o">.</span><span class="py">getParent</span><span class="o">();</span>
      <span class="n">outPath</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Path</span><span class="o">(</span><span class="n">midName</span><span class="o">,</span> <span class="n">outPath</span><span class="o">);</span>
    <span class="o">}</span>
    <span class="k">return</span> <span class="nv">outPath</span><span class="o">.</span><span class="py">toString</span><span class="o">();</span>
  <span class="o">}</span>

  <span class="cm">/**
   * 
   * @param fs
   *          the file system to use
   * @param job
   *          a job conf object
   * @param name
   *          the name of the file over which a record writer object will be
   *          constructed
   * @param arg3
   *          a progressable object
   * @return A RecordWriter object over the given file
   * @throws IOException
   */</span>
  <span class="k">abstract</span> <span class="k">protected</span> <span class="nc">RecordWriter</span><span class="o">&lt;</span><span class="n">K</span><span class="o">,</span> <span class="n">V</span><span class="o">&gt;</span> <span class="nf">getBaseRecordWriter</span><span class="o">(</span><span class="nc">FileSystem</span> <span class="n">fs</span><span class="o">,</span>
      <span class="nc">JobConf</span> <span class="n">job</span><span class="o">,</span> <span class="nc">String</span> <span class="n">name</span><span class="o">,</span> <span class="nc">Progressable</span> <span class="n">arg3</span><span class="o">)</span> <span class="n">throws</span> <span class="nc">IOException</span><span class="o">;</span>
<span class="o">}</span>

</pre></table></code></div></div><p>可以看出，在写每条记录之前，MultipleOutputFormat将调用generateFileNameForKeyValue方法来确定文件名，所以在只需要重写generateFileNameForKeyValue方法即可</p><h5 id="223-multipleoutformat重写">2.2.3 MultipleOutFormat重写</h5><div class="language-scala highlighter-rouge"><div class="code-header"> <span text-data=" Scala "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">RDDMultipleTextOutputFormat</span> <span class="k">extends</span> <span class="nc">MultipleTextOutputFormat</span><span class="o">[</span><span class="kt">Any</span>, <span class="kt">Any</span><span class="o">]</span> <span class="o">{</span>

  <span class="k">private</span> <span class="k">val</span> <span class="nv">start_time</span> <span class="k">=</span> <span class="nv">System</span><span class="o">.</span><span class="py">currentTimeMillis</span><span class="o">()</span>

  <span class="k">override</span> <span class="k">def</span> <span class="nf">generateFileNameForKeyValue</span><span class="o">(</span><span class="n">key</span><span class="k">:</span> <span class="kt">Any</span><span class="o">,</span> <span class="n">value</span><span class="k">:</span> <span class="kt">Any</span><span class="o">,</span> <span class="n">name</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=</span> <span class="o">{</span>
    
    <span class="k">val</span> <span class="nv">service_date</span> <span class="k">=</span>  <span class="n">start_time</span> <span class="o">+</span> <span class="s">"-"</span> <span class="o">+</span> <span class="n">name</span> <span class="c1">//.split("-")(0)</span>
    <span class="n">service_date</span>
  <span class="o">}</span>
<span class="o">}</span>
</pre></table></code></div></div><p>Spark Streaming 代码修改</p><div class="language-scala highlighter-rouge"><div class="code-header"> <span text-data=" Scala "><i class="fa-fw fas fa-code small"></i></span> <button aria-label="copy" title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="o">...</span><span class="c1">//业务代码</span>
<span class="o">.</span><span class="py">map</span><span class="o">(</span><span class="n">x</span><span class="o">=&gt;(</span><span class="n">x</span><span class="o">,</span><span class="s">""</span><span class="o">))</span><span class="c1">//由于saveAsHadoopFile接受PariRDD，所以需要转成这样 </span>
<span class="o">.</span><span class="py">saveAsHadoopFile</span><span class="o">(</span><span class="n">finallpath</span><span class="o">,</span><span class="n">classOf</span><span class="o">[</span><span class="kt">String</span><span class="o">],</span><span class="n">classOf</span><span class="o">[</span><span class="kt">String</span><span class="o">],</span><span class="n">classOf</span><span class="o">[</span><span class="kt">RDDMultipleTextOutputFormat</span><span class="o">])</span>
</pre></table></code></div></div><p>到此，已经可以解决覆盖问题</p><p><img data-proofer-ignore data-src="https://blog-1253533258.cos.ap-shanghai.myqcloud.com/2019-5-30/TIM%E6%88%AA%E5%9B%BE20190530222709.jpg" alt="" /></p><h4 id="参考">参考</h4><p><a href="http://ileaf.tech/bigdata/2018/07/02/Spark-Streaming-%E5%86%99%E5%85%A5%E6%95%B0%E6%8D%AE%E5%88%B0%E6%96%87%E4%BB%B6-%E5%85%B3%E9%94%AE%E4%B8%BASpark%E6%A0%B9%E6%8D%AE%E6%95%B0%E6%8D%AE%E5%86%85%E5%AE%B9%E8%BE%93%E5%87%BA%E5%88%B0%E4%B8%8D%E5%90%8C%E8%87%AA%E5%AE%9A%E4%B9%89%E5%90%8D%E7%A7%B0%E6%96%87%E4%BB%B6-saveAsHadoopFile%E4%BB%A5%E5%8F%8A%E8%87%AA%E5%AE%9A%E4%B9%89MultipleOutputFormat/">Spark(Streaming)写入数据到文件</a></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A1%86%E6%9E%B6/'>大数据框架</a>, <a href='/categories/spark/'>spark</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/spark/" class="post-tag no-text-decoration" >spark</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Spark Streaming - 写出文件自定义文件名 - Zcyoop&url=https://zcyoop.github.io/posts/Spark-Streaming-%E5%86%99%E5%87%BA%E6%96%87%E4%BB%B6%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E4%BB%B6%E5%90%8D/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Spark Streaming - 写出文件自定义文件名 - Zcyoop&u=https://zcyoop.github.io/posts/Spark-Streaming-%E5%86%99%E5%87%BA%E6%96%87%E4%BB%B6%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E4%BB%B6%E5%90%8D/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Spark Streaming - 写出文件自定义文件名 - Zcyoop&url=https://zcyoop.github.io/posts/Spark-Streaming-%E5%86%99%E5%87%BA%E6%96%87%E4%BB%B6%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E4%BB%B6%E5%90%8D/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>最近更新</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Python-%E7%BC%BA%E5%B0%91C++%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/">Python - 缺少C++构建工具</a><li><a href="/posts/Docker-Remote-API%E7%9A%84TLS%E8%AE%A4%E8%AF%81&Portainer%E9%85%8D%E7%BD%AE/">Docker Remote API的TLS认证&Portainer配置</a><li><a href="/posts/Kafka-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6/">Kafka - 副本机制</a><li><a href="/posts/Flink-Window-%E6%A6%82%E5%BF%B5&%E4%BD%BF%E7%94%A8/">Flink - Window 概念&使用</a><li><a href="/posts/Java%E5%B9%B6%E5%8F%91-CompletableFuture%E8%AF%A6%E8%A7%A3/">Java 并发 - CompletableFuture详解</a></ul></div><div id="access-tags"> <span>热门标签</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/java/">java</a> <a class="post-tag" href="/tags/hadoop/">hadoop</a> <a class="post-tag" href="/tags/spark/">spark</a> <a class="post-tag" href="/tags/flink/">flink</a> <a class="post-tag" href="/tags/hive/">hive</a> <a class="post-tag" href="/tags/mysql/">mysql</a> <a class="post-tag" href="/tags/jvm/">jvm</a> <a class="post-tag" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">文章内容</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Spark%E5%85%A5%E9%97%A8/"><div class="card-body"> <span class="timeago small" >2018-12-24<i class="unloaded">2018-12-24T08:10:05+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Spark入门</h3><div class="text-muted small"><p> Spark入门 1.什么是Sark Apache Spark是一个开源集群运算框架。 相对于Hadoop的MapReduce会在运行完工作后将中介数据存放到磁盘中，Spark使用了存储器内运算技术，能在数据尚未写入硬盘时即在存储器内分析运算。Spark在存储器内运行程序的运算速度能做到比Hadoop MapReduce的运算速度快上100倍，即便是运行程序于硬盘时，Spark...</p></div></div></a></div><div class="card"> <a href="/posts/Spark%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86/"><div class="card-body"> <span class="timeago small" >2018-12-26<i class="unloaded">2018-12-26T06:43:50+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Spark内部原理</h3><div class="text-muted small"><p> 1.Shuffle 1.1 什么是Shuffle Spark是分布式计算系统，数据块在不同节点执行，但是一些操作，例如join，需要将不同节点上相同的Key对应的Value聚集到一起，Shuffle便应运而生。 Shuffle是连接map和reduce之间的桥梁，它将map的输出对应到reduce输入中，这期间涉及到序列化反序列化、跨节点网络IO以及磁盘读写IO等，所以说Shuffle...</p></div></div></a></div><div class="card"> <a href="/posts/spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-shell/"><div class="card-body"> <span class="timeago small" >2018-12-28<i class="unloaded">2018-12-28T05:58:14+08:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>spark源码分析 - shell</h3><div class="text-muted small"><p> spark-shell function main() { # 对当前系统进行判断，通过spark-submits.sh 启动 org.apache.spark.repl.Main if $cygwin; then stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1 export SPARK_SUBMIT_OPTS...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Unicode&UTF&%E7%A0%81%E7%82%B9%E5%85%B3%E7%B3%BB/" class="btn btn-outline-primary" prompt="上一篇"><p>Unicode&UTF&码点关系</p></a> <a href="/posts/Linux-nohup%E5%92%8C&%E7%9A%84%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/" class="btn btn-outline-primary" prompt="下一篇"><p>nohup和&的使用说明</p></a></div></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/zcyoop">zcyoop</a>. <span data-toggle="tooltip" data-placement="top" title="除非另有说明，本网站上的博客文章均由作者按照知识共享署名 4.0 国际 (CC BY 4.0) 许可协议进行授权。">保留部分权利。</span></p></div><div class="footer-right"><p class="mb-0"> 本站由 <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> 生成，采用 <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> 主题。</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">热门标签</h4><a class="post-tag" href="/tags/java/">java</a> <a class="post-tag" href="/tags/hadoop/">hadoop</a> <a class="post-tag" href="/tags/spark/">spark</a> <a class="post-tag" href="/tags/flink/">flink</a> <a class="post-tag" href="/tags/hive/">hive</a> <a class="post-tag" href="/tags/mysql/">mysql</a> <a class="post-tag" href="/tags/jvm/">jvm</a> <a class="post-tag" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/">设计模式</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/lozad/dist/lozad.min.js,npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.16.1,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-WR4TKVL8XR"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-WR4TKVL8XR'); }); </script>
