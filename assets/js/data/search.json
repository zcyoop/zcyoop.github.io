[ { "title": "Python - 缺少C++构建工具", "url": "/posts/Python-%E7%BC%BA%E5%B0%91C++%E6%9E%84%E5%BB%BA%E5%B7%A5%E5%85%B7/", "categories": "Python", "tags": "Python", "date": "2024-12-04 00:00:00 +0800", "snippet": "背景在Windows使用pip安装包时报错error: Microsoft Visual C++ 14.0 or greater is required. Get it with &quot;Microsoft C++ Build Tools&quot;: https://visualstudio.microsoft.com/visual-cpp-build-tools/这表示你正在安装的库包含用其他语言编写的代码，并且需要额外的工具来完成安装。解决为了安装这些工具，请按照以下步骤操作（需要6GB以上的磁盘空间）： 打开网址 https://visualstudio.microsoft.com/visual-cpp-build-tools/ 蓝奏云 密码:ed55 点击“下载生成工具”。 vs_BuildTools.exe的文件应该开始下载 运行已下载的文件。点击继续以进行下一步 选择C++ build tools，点安装一个更简单的办法，下载完文件后使用终端运行vs_BuildTools.exe --norestart --passive --downloadThenInstall --includeRecommended --add Microsoft.VisualStudio.Workload.NativeDesktop --add Microsoft.VisualStudio.Workload.VCTools --add Microsoft.VisualStudio.Workload.MSBuildTools参考 How to solve “error: Microsoft Visual C++ 14.0 or greater is required” when installing Python packages? Python Discord - Microsoft Visual C++ Build Tools Python 3 Microsoft Visual C++ 14.0 is required" }, { "title": "机器学习 - Optuna超参优化", "url": "/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-Optuna%E8%B6%85%E5%8F%82%E4%BC%98%E5%8C%96/", "categories": "机器学习", "tags": "机器学习", "date": "2024-07-09 00:00:00 +0800", "snippet": "1. Optuna简介Optuna 是一个特别为机器学习设计的自动超参数优化软件框架2. 组件概念 Study: 基于目标函数的优化过程 Trial: 目标函数的单次执行过程 3. 基本使用 3.1 参数采样方法 optuna.trial.Trial.suggest_categorical() ：用于类别参数 optuna.trial.Trial.suggest_int() ：用于整形参数 optuna.trial.Trial.suggest_float() ：用于浮点型参数 3.2 参数采样算法 optuna.samplers.TPESampler ：实现的 Tree-structured Parzen Estimator 算法 optuna.samplers.CmaEsSampler： 实现的 CMA-ES 算法 optuna.samplers.GridSampler ：实现的网格搜索 optuna.samplers.RandomSampler： 实现的随机搜索默认的采样器是 optuna.samplers.TPESampler3.3 使用例子import optunaimport lightgbm as lgbimport sklearn.datasetsimport sklearn.metricsfrom sklearn.model_selection import train_test_splitdef objective(trial): data, target = sklearn.datasets.load_breast_cancer(return_X_y=True) train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25) dtrain = lgb.Dataset(train_x, label=train_y) dvalid = lgb.Dataset(valid_x, label=valid_y) param = { &quot;objective&quot;: &quot;binary&quot;, &quot;metric&quot;: &quot;auc&quot;, &quot;verbosity&quot;: -1, &quot;boosting_type&quot;: &quot;gbdt&quot;, &quot;lambda_l1&quot;: trial.suggest_float(&quot;lambda_l1&quot;, 1e-8, 10.0, log=True), &quot;lambda_l2&quot;: trial.suggest_float(&quot;lambda_l2&quot;, 1e-8, 10.0, log=True), &quot;num_leaves&quot;: trial.suggest_int(&quot;num_leaves&quot;, 2, 256), &quot;feature_fraction&quot;: trial.suggest_float(&quot;feature_fraction&quot;, 0.4, 1.0), &quot;bagging_fraction&quot;: trial.suggest_float(&quot;bagging_fraction&quot;, 0.4, 1.0), &quot;bagging_freq&quot;: trial.suggest_int(&quot;bagging_freq&quot;, 1, 7), &quot;min_child_samples&quot;: trial.suggest_int(&quot;min_child_samples&quot;, 5, 100), } # Add a callback for pruning. pruning_callback = optuna.integration.LightGBMPruningCallback(trial, &quot;auc&quot;) evals_result = {} evals_result_callback = lgb.record_evaluation(evals_result) gbm = lgb.train(param, dtrain, valid_sets=[dvalid], callbacks=[pruning_callback, evals_result_callback]) preds = gbm.predict(valid_x) accuracy = sklearn.metrics.roc_auc_score(valid_y, preds) return accuracy # return max(evals_result[&#39;valid_0&#39;][&#39;auc&#39;])if __name__ == &quot;__main__&quot;: study = optuna.create_study( study_name=&quot;lightGBM 01&quot;, storage=&quot;sqlite:///db.sqlite3&quot;, pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=&quot;maximize&quot; ) # study = optuna.create_study(pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=&quot;maximize&quot;) study.optimize(objective, n_trials=500) # study.optimize(objective) print(&quot;Number of finished trials: {}&quot;.format(len(study.trials))) print(&quot;Best trial:&quot;) trial = study.best_trial print(&quot; Value: {}&quot;.format(trial.value)) print(&quot; Params: &quot;) for key, value in trial.params.items(): print(&quot; {}: {}&quot;.format(key, value)) 写一个objective方法，返回机器学习算法的Metrics，例如例子中的AUC值 使用optuna.create_study创建一个Study study_name：改Study任务的名称，一般与storage一起使用 storage：数据存储地址 pruner：剪枝算法 direction：机器学习算法**Metric**的优化方向，例如AUC是越大越好，则传入maximize，如果存在多个metrics，则传入数组即可 调用study.optimize优化参数 objective：上面定义的方法 n_trials：训练次数 callbacks：回调方法，例如提前停止等 参考 Optuna: 一个超参数优化框架 — Optuna 2.7.0 文档 GitHub - optuna/optuna-integration" }, { "title": "机器学习 - 特征缩放详解", "url": "/posts/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%89%B9%E5%BE%81%E7%BC%A9%E6%94%BE%E8%AF%A6%E8%A7%A3/", "categories": "机器学习", "tags": "机器学习", "date": "2024-06-02 00:00:00 +0800", "snippet": "1. 什么是特征缩放在机器学习中特种缩放通常是指在数据预处理阶段，将不同量纲或量程的数据转换到同一量纲下或某个特定区间内。例如，存在某一组特征范围：[0,100k] ，将其缩放至[0,1]之间，就是特征缩放进行特征缩放的意义 改善算法性能：一些机器学习算法，特别是那些依赖于特征间距离的算法（如K-近邻、支持向量机等），对特征的尺度非常敏感。特征缩放确保了所有特征在同一尺度上，从而提高了这些算法的性能。举例说明：特征x：[0,0.1] 特征y：[10k,100k]，此时计算特征之间的欧式距离是很大的，但是如果都将缩放至[0,1]之间，特征之间的距离会大幅减少，可以有效增加算法性能，加速训练过程同理。 加速训练过程：某些基于梯度的优化算法（如梯度下降）在所有特征尺度相同时更容易找到最优解，从而加快训练过程。 增强模型解释性：当所有特征都在相同的尺度上时，可以更容易地解释模型，了解哪些特征对预测更有影响。 处理离群点和极端值：通过标准化，可以减小离群点和极端值对模型的影响。2. 特征缩放的两大类方法常用的特征缩放方法主要包括归一化（Normalization）和标准化（Standardization）。归一化通常将特征值转换到[0, 1]区间，而标准化则是将特征转化为具有零均值和单位方差的标准正态分布2.1 归一化和标准化的对比2.1.1 归一化 (Normalization) 计算方式: 将特征缩放到一个指定的范围，通常是[0, 1]。 适用场景: 当特征的量纲或数量级相差较大时，或当需要特征在一个固定范围内时。 优点: 将特征值约束在一个相同的范围内，有助于梯度下降等优化算法的收敛。 对于基于距离度量的算法如K-近邻、支持向量机等，归一化有助于量纲的消除。 缺点: 可能会丢失一些特征之间的相对差异信息。 受极端值的影响较大。 2.1.2 标准化 (Standardization) 计算方式: 将特征的均值移除并将其缩放到单位方差。 适用场景: 当特征分布不符合标准正态分布，或当算法假设输入特征是零均值和单位方差时。 优点: 通过消除均值和方差的影响，使得不同特征具有可比性，特别适合回归、逻辑回归、支持向量机等算法。 不受极端值的影响，更加稳定。 缺点: 不会将特征值限制在一个特定的范围内，对于某些需要固定范围输入的神经网络等算法，可能不太适合。 如果数据的分布远离正态分布，标准化可能效果不佳。 2.1.3 对比总结 使用归一化还是标准化取决于具体算法和数据的特性。 如果数据分布大致符合正态分布，标准化通常更有效。 如果特征的最小值和最大值已知并且重要，归一化可能更合适。 在不确定的情况下，可以尝试使用归一化和标准化，并使用交叉验证等方法选择效果更好的特征缩放方法。 2.2 归一化和标准化适用算法 归一化和标准化的对比表格，包括了各自适用的算法类型。 特性 归一化 (Normalization) 标准化 (Standardization) 计算方式 将特征缩放到[0, 1]范围内 移除均值并缩放到单位方差 适用算法 K-近邻、神经网络、支持向量机等 线性回归、逻辑回归、支持向量机等 优点 量纲消除、有助于梯度下降等优化算法的收敛 不受极端值影响、特征可比性强 缺点 可能丢失相对差异、受极端值影响 不会将值限制在特定范围、非正态分布下效果可能不佳 注意：这些都是一般的指导原则，具体的选择可能需要根据具体数据和问题进行实验验证。适用算法解释： 归一化: K-近邻: 基于距离的算法，需要特征在相同的量纲上。 神经网络: 有时需要输入特征在固定范围内以便更好地训练。 支持向量机: 对特征的量纲敏感。 标准化: 线性回归、逻辑回归: 假设特征是零均值和单位方差，标准化能满足这一假设。 支持向量机: 可以增强算法的稳定性和收敛性。 各类算法是否需要进行特征缩放 算法类型 是否需要缩放 实例算法 基于梯度下降的算法（Gradient Descent Based Algorithms) 需要 逻辑回归、线性回归、神将网络等 基于距离的算法（Distance-Based Algorithms) 需要 K近邻、K-Means、SVM、PCA等 基于树的算法（Tree-Based Algorithms) 不需要 决策树等 线性判别分析、朴素贝叶斯等算法 不需要 线性判别分析、朴素贝叶斯等 3. 常见特征缩放器功能&amp;amp;对数据的影响 数据来源：SKlearn中的加州住房数据 x轴：街区收入中位数 y轴：平均房屋占用率 3.1 原始数据 左侧图显示整个数据集，右侧图放大以显示没有边缘异常值的数据集。绝大多数样本都被压缩到特定范围，[0, 10] 代表收入中位数，[0, 6] 代表平均住房占用率存在一些边际异常值（某些街区的平均入住人数超过 1200 人）需要注意3.2 标准化 / Z值归一化（Standardization / Z-Score Normalization）将数值缩放到0附近，且数据的分布变为均值为0，标准差为1的标准正态分布（先减去均值来对特征进行中心化 mean centering处理，再除以标准差进行缩放）。公式\\(Z = \\frac{X - \\mu}{\\sigma}\\) z 表示标准化后的值 x 是原始特征值 u 是该特征的均值（mean） _σ _是该特征的标准差（standard deviation）转换后的数据： 减去平均值并将数据缩放到单位方差，缩小了特征值的范围[0,120]，如左图所示。然而，在计算经验平均值和标准差时，同样会受到异常值的影响，如右图数据被压缩到了 [-0.2, 0.2] 3.3 最大最小值归一化（Min-Max Normalization）将数值范围缩放到指定范围区间里\\[x X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}} \\cdot (max - min) + min\\] X_scaled 是缩放后的值 X 是原始特征值 X_min 和 X_max 分别是该特征在整个数据集中的最小值和最大值 max 和 min 是用户指定的缩放范围，默认通常是0和1重新缩放数据集，使所有特征值都在 [0, 1] 范围内，如下右图所示。然而，这种缩放将所有内点压缩到转换后的平均房屋占用率的狭窄范围 [0, 0.005] 内Z-Score Normalization和 Min-Max Normalization 对异常值的存在非常敏感3.4 鲁棒标准化（Robust Standardization）先减去中位数，再除以四分位间距（interquartile range），因为不涉及极值，因此在数据里有异常值的情况下表现比较稳健\\(X_{scaled} = \\frac{X - Q_1}{Q_3 - Q_1}\\) X_scaled 是缩放后的值 X 是原始特征值 Q_1 是该特征的第一四分位数（25%分位点） Q_3 是该特征的第三四分位数（75%分位点）与之前的缩放器不同， Robust Standardization 的居中和缩放统计数据基于百分位数，因此不会受到少量非常大的边缘异常值的影响。因此，变换后的特征值的结果范围比之前的缩放器更大，更重要的是，近似相似； 对于这两个特征，大多数变换后的值都位于 [-2, 3] 范围内，如放大后的图像所示。图中。请注意，异常值本身仍然存在于转换后的数据中。如果需要单独的异常值剪切，则需要非线性变换参考 Compare the effect of different scalers on data with outliers 特征缩放（Feature Scaling） - HuZihu - 博客园 【机器学习】系统介绍特征缩放" }, { "title": "Docker Remote API的TLS认证&amp;Portainer配置", "url": "/posts/Docker-Remote-API%E7%9A%84TLS%E8%AE%A4%E8%AF%81&Portainer%E9%85%8D%E7%BD%AE/", "categories": "容器", "tags": "Docker", "date": "2023-04-23 00:00:00 +0800", "snippet": "前言Docker 自带 API 功能，支持对 Docker 进行管理，但是默认并不启用，而且一般在没有开启加密通信的时候直接启用 API 会有安全风险。直接开启非加密的 Docker API 是个非常危险的行为！因此我们需要开启 TLS 加密来保障 Docker API 通信的安全性。通过开启 TLS 防止未经许可的客户端访问服务端节点，保障其系统安全性。而 Docker API 也适用于在一个 Portainer 示例上来远程管理多台机器，这也是我最初使用这一功能的原因。接下来将介绍如何完成 TLS 的设置，来保障 Docker API 通信的安全性开启加密通信步骤第一步：创建 TLS 证书#!/bin/bash# # -------------------------------------------------------------# 自动创建 Docker TLS 证书# -------------------------------------------------------------# 以下是配置信息# --[BEGIN]------------------------------PASSWORD=&quot;your code&quot;COUNTRY=&quot;CN&quot;STATE=&quot;your state&quot;CITY=&quot;your city&quot;ORGANIZATION=&quot;your org&quot;ORGANIZATIONAL_UNIT=&quot;your org unit&quot;EMAIL=&quot;your email&quot;# --[END]--CODE=&quot;docker_api&quot;IP=`curl ip.sb -4`COMMON_NAME=&quot;$IP&quot;# Generate CA keyopenssl genrsa -aes256 -passout &quot;pass:$PASSWORD&quot; -out &quot;ca-key-$CODE.pem&quot; 4096# Generate CAopenssl req -new -x509 -days 365 -key &quot;ca-key-$CODE.pem&quot; -sha256 -out &quot;ca-$CODE.pem&quot; -passin &quot;pass:$PASSWORD&quot; -subj &quot;/C=$COUNTRY/ST=$STATE/L=$CITY/O=$ORGANIZATION/OU=$ORGANIZATIONAL_UNIT/CN=$COMMON_NAME/emailAddress=$EMAIL&quot;# Generate Server keyopenssl genrsa -out &quot;server-key-$CODE.pem&quot; 4096# Generate Server Certs.openssl req -subj &quot;/CN=$COMMON_NAME&quot; -sha256 -new -key &quot;server-key-$CODE.pem&quot; -out server.csrecho &quot;subjectAltName = IP:$IP,IP:127.0.0.1&quot; &amp;gt;&amp;gt; extfile.cnfecho &quot;extendedKeyUsage = serverAuth&quot; &amp;gt;&amp;gt; extfile.cnfopenssl x509 -req -days 365 -sha256 -in server.csr -passin &quot;pass:$PASSWORD&quot; -CA &quot;ca-$CODE.pem&quot; -CAkey &quot;ca-key-$CODE.pem&quot; -CAcreateserial -out &quot;server-cert-$CODE.pem&quot; -extfile extfile.cnf# Generate Client Certs.rm -f extfile.cnfopenssl genrsa -out &quot;key-$CODE.pem&quot; 4096openssl req -subj &#39;/CN=client&#39; -new -key &quot;key-$CODE.pem&quot; -out client.csrecho extendedKeyUsage = clientAuth &amp;gt;&amp;gt; extfile.cnfopenssl x509 -req -days 365 -sha256 -in client.csr -passin &quot;pass:$PASSWORD&quot; -CA &quot;ca-$CODE.pem&quot; -CAkey &quot;ca-key-$CODE.pem&quot; -CAcreateserial -out &quot;cert-$CODE.pem&quot; -extfile extfile.cnfrm -vf client.csr server.csrchmod -v 0400 &quot;ca-key-$CODE.pem&quot; &quot;key-$CODE.pem&quot; &quot;server-key-$CODE.pem&quot;chmod -v 0444 &quot;ca-$CODE.pem&quot; &quot;server-cert-$CODE.pem&quot; &quot;cert-$CODE.pem&quot;# 打包客户端证书mkdir -p &quot;tls-client-certs-$CODE&quot;cp -f &quot;ca-$CODE.pem&quot; &quot;cert-$CODE.pem&quot; &quot;key-$CODE.pem&quot; &quot;tls-client-certs-$CODE/&quot;cd &quot;tls-client-certs-$CODE&quot;tar zcf &quot;tls-client-certs-$CODE.tar.gz&quot; *mv &quot;tls-client-certs-$CODE.tar.gz&quot; ../cd ..rm -rf &quot;tls-client-certs-$CODE&quot;# 拷贝服务端证书mkdir -p /srv/certs.dcp &quot;ca-$CODE.pem&quot; &quot;server-cert-$CODE.pem&quot; &quot;server-key-$CODE.pem&quot; &quot;tls-client-certs-$CODE.tar.gz&quot; /srv/certs.d/修改上面的配置信息，将上述脚本保存为一个 .sh 文件，并在命令行执行 bash xxx.sh 以运行该脚本。运行后会在当前目录生成服务端和客户端证书信息，同时会在将在 /srv/certs.d 目录下生成证书，其中包括 tls-client-certs-docker_api.tar.gz ，该文件保存着客户端访问 Docker API 所需的证书，请下载下来并安全和妥善地保存。第二步：开启 Docker API首先打开终端并执行以下命令：vim /lib/systemd/system/docker.service在打开的 Docker 服务文件中查找 ExecStart 行在 ExecStart 行的后面添加以下选项：-H=tcp://0.0.0.0:2376 --tlsverify --tlscacert=/srv/certs.d/ca-docker_api.pem --tlscert=/srv/certs.d/server-cert-docker_api.pem --tlskey=/srv/certs.d/server-key-docker_api.pem最后，执行以下命令重新加载服务并重启 Docker：systemctl daemon-reload &amp;amp;&amp;amp; service docker restart第三步：验证此时，我们需要验证 Docker API 是否能够访问，且是否只能通过加密访问。首先执行 docker -H=127.0.0.1:2376 info，一般来说会返回：Client:Context: defaultDebug Mode: falsePlugins:app: Docker App (Docker Inc., v0.9.1-beta3)buildx: Docker Buildx (Docker Inc., v0.9.1-docker)Server:ERROR: Error response from daemon: Client sent an HTTP request to an HTTPS server.errors pretty printing info这是因为没有通过 tls 去访问，此时改用 docker -H=127.0.0.1:2376 --tlsverify info，会出现下面的错误：unable to resolve docker endpoint: open /root/.docker/ca.pem: no such file or directory这是由于目前没有在对应的用户文件夹下配置证书，我们可以执行以下命令：mkdir ~/.docker &amp;amp;&amp;amp; \\tar -zxvf /srv/certs.d/tls-client-certs-docker_api.tar.gz -C ~/.docker &amp;amp;&amp;amp; \\mv ~/.docker/ca-docker_api.pem ~/.docker/ca.pem &amp;amp;&amp;amp; \\mv ~/.docker/cert-docker_api.pem ~/.docker/cert.pem &amp;amp;&amp;amp; \\mv ~/.docker/key-docker_api.pem ~/.docker/key.pem完成后再执行一遍 docker -H=127.0.0.1:2376 --tlsverify info 即可获取信息了，至此验证完成。第四步：配置 Portainer 打开 Portainer，进入 Environments，点击右上角的 Add environment 添加节点，并在随后的向导中选择 Docker 在Docker environment中选择API模式并输入Name和Docker API URL 将 Environment URL 或者 Docker API URL 中的端口从 2375 改为 2376。然后将 TLS 的 Switch 打开，即可看到相关的 TLS 配置项，我们在第一步的时候下载了一份文件，解压后根据名称匹配即可。 参考 如何开启 Docker Remote API 的 TLS 认证，并在 Portainer 上进行配置" }, { "title": "Kafka - 副本机制", "url": "/posts/Kafka-%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6/", "categories": "大数据框架", "tags": "kafka", "date": "2022-10-26 00:00:00 +0800", "snippet": "1. Kafka副本简介在 Kafka 0.8.0 之前，Kafka 是没有副本的概念的，那时候人们只会用 Kafka 存储一些不重要的数据，因为没有副本，数据很可能会丢失。但是随着业务的发展，支持副本的功能越来越强烈，所以为了保证数据的可靠性，Kafka 从 0.8.0 版本开始引入了分区副本。也就是说每个分区可以人为的配置几个副本（比如创建主题的时候指定 replication-factor，也可以在 Broker 级别进行配置 default.replication.factor），一般会设置为3。Kafka中的副本同一分区的不同副本保存的消息是相同的（在同一时刻，副本之间并非完全一样，后面的一些内容就是为了解决这个问题），副本之间是“一主多从”的关系，其中Leader副本负责处理读写请求，Follower副本只负责与Leader进行消息同步。副本处于不同的Broker中，当Leader副本出现故障时，会从Follower副本中重新选举新的Leader副本对外提供服务。Kafka通过多副本机制实现了故障自动转移1.1 副本机制中的一些概念如图所示，Kafka中有4个Broker，其中某个主题有3个分区，且副本因子也为3，如此每个分区便有1个Leader和2个Follower。其中生产者只与Leader进行交互，而Follower只负责消息同步，但是当有很多Follower时，Follower副本中的消息就有可能相对Leader有一定滞后分区中所有的副本（Leader和Followers）统称AR（Assigned Replicas）。所有与Leader副本保持一定程度同步的副本（包括Leader）组成ISR（In-Sync Replicas），ISR集合是AR集合的一个子集。所有消息发送到Leader，然后Follower副本才能从Leader副本中进行消息同步，而同步期间Follower会有一定程度消息滞后，这个“一定程度消息滞”是指可容忍的滞后范围，这个可以通过参数进行配置。与Leader同步之后过多的Follower组成OSR（Out-of-Sync Replicas），所以AR = ISR+OSR，不过正常情况下AR = ISR。ISR与HW和LEO也有紧密联系。HW（High Watermark，高水位），它标识了一个特定的消息偏移量，消费者只能拉渠道这个offset之前的消息。如下图所示，消费者只能拉取offset在0到5之间的数据，而offset为6的消息对消费者来说是不可见的。LEO（Log End Offset)，它标识当前日志文件下一条待写入消息的offset，即下图offset为9的位置，LEO的大小相当于当前日志分区中最后一条消息的offset+1.2. ISR集合的伸缩2.1 副本失效正常情况下，分区的所有副本都处于ISR集合当中，但是Follower副本难免会有一些异常情况的发生，也就是简介中描述的“超过一定滞后范围”，或者是机器出现异常，从而某些副本会被剥离出ISR集合当中这些信息可以通过kafka-topics.sh脚本中的under-replicated-partitions参数来显示主题中所包含失效副本的信息如果是机器发生故障， 那么肯定会直接被归为异常副本，所以这里主要讨论的是机器由于性能或其他原因导致的同步失效状态。Kafka从0.9x版本开始，就通过唯一的broker端参数replica.lag.time.max.ms来进行抉择，当ISR集合中的一个Follwer副本滞后Leaders的时间超过此参数指定的值则判定为同步失败，需要将该Follower剔除出ISR集合，如下图所示ps：一般这两种情况会导致副本失效 follower副本进程卡住，在一段时间内根本没有向leader副本发出同步请求，例如频繁的Full GC follower副本进程同步过慢，在一段时间内都无法追赶上leader副本，比如I/O开销过大2.2 副本恢复副本恢复的条件很简单，随着Follower副本不断与Leader进行消息同步，Follower副本的LEO也会逐渐后移，最终追赶上Follower副本就有资格进入ISR集合。追赶Leader的判断标准是此副本的LEO是否小于Leader副本的HW，而不是Leader的LEO如下图所示，Leader副本LEO为9，Follower1的LEO为7，而Follower的LEO为6，如果判定这3个副本都属于ISR集合中，那么这个分区的HW则为6；如果Follower2被判断为失效副本剥离出ISR集合，那么此时HW为72.3 LEO&amp;amp;HW上文介绍了副本的恢复是通过Follower副本的LEO和Leader的HW来进行判断，下面介绍LEO与HW之间的变化关系如下图所示，假如生产者一直往Leader（带阴影的方框）写入消息，其中某一时刻，Leader的LEO为5，其他所有Followers副本（不带阴影）HW都是0之后Followers向Leader拉去消息，在拉去的请求中会带有自身的LEO信息。Leader返回给Followers副本相信的消息，并带有Leader自身的HW信息此时两个Follower各自拉取到了消息，并更新各自的LEO分别为3，4。与此同时，Follower还会更新自己的HW，更新HW的算法是比较当前LEO和Leader传送过来的HW值，去较小值为自己的HW值。两个Followers副本HW值为0（min(0,0)=0）此时Leader再次受到Followers的请求信息，并带有自身的LEO信息（3，4），Leader通过自身的LEO和Follower的LEO信息进行比对，取出最小值3最为最新的HW。然后将最新的HW信息连同消息一起返回给Followers然后Followers再次拉去到Leader的HW信息和数据信息，分别跟新自己的LEO和HW信息33. Leader Epoch的介入在Kafka 0.11之前，Kafka都是基于HW的同步机制来进行的，但是会出现L数据丢失或是Leader副本和Follower副本数据不一致的问题。例如，带L的是Leader。此时处于RB刚刚更新完自己的HW，但是RA还未获取到最新的HW，RA这个时候机器重启，此时RA中大于HW的数据就会被截断，在RA重启后RB接着重启，RA就会被选作Leader这样就会导致m2这条消息丢失想要解决这样的问题，可以等待所有Follower副本更新完HW之后再更新Leader的HW，但这样会增加多一轮FetchRequest/FetchResponse延迟，所以不是太妥当。Kafka的方式是引入Leader Epoch（简称LE），LE会在每次Leader发生变化后+1，每条数据都带有LE信息，那么再看下上述问题在引入LE后的情况同样是RA先重启，但是重启后不是直接截断大于HW的数据，而是会像当前的Leader发送自己的当前的LE信息（假设LE_A)，如果此时LE_A和B中的不同，那么B此时会查找LeaderEpoch为LE_A+1对应的StartOffset并返回给A，也就是LE_A对应的LEO。但是在上面的例子中，会发现A收到的LEO相同，则就不需要截断HW后面的数据。之后和上面的例子一样，RB宕机，A成为Leader，LE从0变为1，对应的m2消息也得到了保留。参考 《深入理解Kafka：核心设计与实践原理》" }, { "title": "Flink - Window 概念&amp;使用", "url": "/posts/Flink-Window-%E6%A6%82%E5%BF%B5&%E4%BD%BF%E7%94%A8/", "categories": "大数据框架", "tags": "flink", "date": "2022-06-23 00:00:00 +0800", "snippet": "1. Window概念 窗口（Window）是处理无界流的关键所在。窗口可以将数据流装入大小有限的“桶”中，再对每个“桶”加以处理。通常来讲，window就是对一个无限的流数据加上边界，使其成为多个有界的集合，在对每个集合进行处理，和Spark Streaming类似2. Window结构在给一个流添加window时，WIndowAssigner是必须指定的，而evictor和trigger则是可选的。数据流向图如下Keyed Windowsstream .keyBy(...) &amp;lt;- keyed versus non-keyed windows .window(...) &amp;lt;- required: &quot;assigner&quot; [.trigger(...)] &amp;lt;- optional: &quot;trigger&quot; (else default trigger) [.evictor(...)] &amp;lt;- optional: &quot;evictor&quot; (else no evictor) [.allowedLateness(...)] &amp;lt;- optional: &quot;lateness&quot; (else zero) [.sideOutputLateData(...)] &amp;lt;- optional: &quot;output tag&quot; (else no side output for late data) .reduce/aggregate/apply() &amp;lt;- required: &quot;function&quot; [.getSideOutput(...)] &amp;lt;- optional: &quot;output tag&quot;Non-Keyed Windowsstream .windowAll(...) &amp;lt;- required: &quot;assigner&quot; [.trigger(...)] &amp;lt;- optional: &quot;trigger&quot; (else default trigger) [.evictor(...)] &amp;lt;- optional: &quot;evictor&quot; (else no evictor) [.allowedLateness(...)] &amp;lt;- optional: &quot;lateness&quot; (else zero) [.sideOutputLateData(...)] &amp;lt;- optional: &quot;output tag&quot; (else no side output for late data) .reduce/aggregate/apply() &amp;lt;- required: &quot;function&quot; [.getSideOutput(...)] &amp;lt;- optional: &quot;output tag&quot;p.s. keyby和sql里面的groupby类似，即是否将数据分组后处理。如果进行分组，不通过key既可以将任务分发到集群上进行并发处理3. Window Assigners在flink中内置了许多window assigner，可以分为以下两类 基于时间的assigner tumbling windows sliding windows session windows 不基于时间的assigner global windows 基于时间的窗口比较好理解，一般是随着时间的推进进行窗口划分（ tumbling windows、sliding windows），或是基于时间间隔进行划分（session windows）比较特殊的是不基于时间的assigner（global windows）。使用global windows时，需要自己定义Trigger来触发计算，以及缓存数据的清理。3.1 滚动窗口（Tumbling Windows）滚动窗口的 assigner 分发元素到指定大小的窗口。滚动窗口的大小是固定的，且各自范围之间不重叠。 比如说，如果你指定了滚动窗口的大小为 5 分钟，那么每 5 分钟就会有一个窗口被计算，且一个新的窗口被创建（如下图所示）DataStream&amp;lt;T&amp;gt; input = ...;// 时间间隔可以用 Time.milliseconds(x)、Time.seconds(x)、Time.minutes(x) 等来指定。// 滚动 event-time 窗口input .keyBy(&amp;lt;key selector&amp;gt;) .window(TumblingEventTimeWindows.of(Time.seconds(5))) .&amp;lt;windowed transformation&amp;gt;(&amp;lt;window function&amp;gt;);// 滚动 processing-time 窗口input .keyBy(&amp;lt;key selector&amp;gt;) .window(TumblingProcessingTimeWindows.of(Time.seconds(5))) .&amp;lt;windowed transformation&amp;gt;(&amp;lt;window function&amp;gt;);// 长度为一天的滚动 event-time 窗口， 偏移量为 -8 小时。input .keyBy(&amp;lt;key selector&amp;gt;) .window(TumblingEventTimeWindows.of(Time.days(1), Time.hours(-8))) .&amp;lt;windowed transformation&amp;gt;(&amp;lt;window function&amp;gt;);3.2 滑动窗口（Sliding Windows）与滚动窗口类似，滑动窗口的 assigner 分发元素到指定大小的窗口，窗口大小通过 window size 参数设置。 滑动窗口需要一个额外的滑动距离（window slide）参数来控制生成新窗口的频率。 因此，如果 slide 小于窗口大小，滑动窗口可以允许窗口重叠。这种情况下，一个元素可能会被分发到多个窗口。比如说，你设置了大小为 10 分钟，滑动距离 5 分钟的窗口，你会在每 5 分钟得到一个新的窗口， 里面包含之前 10 分钟到达的数据（如下图所示）。DataStream&amp;lt;T&amp;gt; input = ...;// 滑动 event-time 窗口input .keyBy(&amp;lt;key selector&amp;gt;) .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&amp;lt;windowed transformation&amp;gt;(&amp;lt;window function&amp;gt;);// 滑动 processing-time 窗口input .keyBy(&amp;lt;key selector&amp;gt;) .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5))) .&amp;lt;windowed transformation&amp;gt;(&amp;lt;window function&amp;gt;);// 滑动 processing-time 窗口，偏移量为 -8 小时input .keyBy(&amp;lt;key selector&amp;gt;) .window(SlidingProcessingTimeWindows.of(Time.hours(12), Time.hours(1), Time.hours(-8))) .&amp;lt;windowed transformation&amp;gt;(&amp;lt;window function&amp;gt;);3.3 会话窗口（Session Windows）会话窗口的 assigner 会把数据按活跃的会话分组。 与滚动窗口和滑动窗口不同，会话窗口不会相互重叠，且没有固定的开始或结束时间。 会话窗口在一段时间没有收到数据之后会关闭，即在一段不活跃的间隔之后。 会话窗口的 assigner 可以设置固定的会话间隔（session gap）或 用 session gap extractor 函数来动态地定义多长时间算作不活跃。 当超出了不活跃的时间段，当前的会话就会关闭，并且将接下来的数据分发到新的会话窗口。DataStream&amp;lt;T&amp;gt; input = ...;// 设置了固定间隔的 event-time 会话窗口input .keyBy(&amp;lt;key selector&amp;gt;) .window(EventTimeSessionWindows.withGap(Time.minutes(10))) .&amp;lt;windowed transformation&amp;gt;(&amp;lt;window function&amp;gt;); // 设置了动态间隔的 event-time 会话窗口input .keyBy(&amp;lt;key selector&amp;gt;) .window(EventTimeSessionWindows.withDynamicGap((element) -&amp;gt; { // 决定并返回会话间隔 })) .&amp;lt;windowed transformation&amp;gt;(&amp;lt;window function&amp;gt;);// 设置了固定间隔的 processing-time session 窗口input .keyBy(&amp;lt;key selector&amp;gt;) .window(ProcessingTimeSessionWindows.withGap(Time.minutes(10))) .&amp;lt;windowed transformation&amp;gt;(&amp;lt;window function&amp;gt;); // 设置了动态间隔的 processing-time 会话窗口input .keyBy(&amp;lt;key selector&amp;gt;) .window(ProcessingTimeSessionWindows.withDynamicGap((element) -&amp;gt; { // 决定并返回会话间隔 })) .&amp;lt;windowed transformation&amp;gt;(&amp;lt;window function&amp;gt;);3.4 全局窗口全局窗口的 assigner 将拥有相同 key 的所有数据分发到一个全局窗口。 这样的窗口模式仅在你指定了自定义的 trigger 时有用。 否则，计算不会发生，因为全局窗口没有天然的终点去触发其中积累的数据。DataStream&amp;lt;T&amp;gt; input = ...;input .keyBy(&amp;lt;key selector&amp;gt;) .window(GlobalWindows.create()) .&amp;lt;windowed transformation&amp;gt;(&amp;lt;window function&amp;gt;);4. TriggerTrigger 决定了一个窗口（由 window assigner 定义）何时可以被 window function 处理。 每个 WindowAssigner 都有一个默认的 Trigger。 如果默认 trigger 无法满足你的需要，你可以在 trigger(…) 调用中指定自定义的 trigger。Trigger 接口提供了五个方法来响应不同的事件： onElement() 方法在每个元素被加入窗口时调用。 onEventTime() 方法在注册的 event-time timer 触发时调用。 onProcessingTime() 方法在注册的 processing-time timer 触发时调用。 onMerge() 方法与有状态的 trigger 相关。该方法会在两个窗口合并时， 将窗口对应 trigger 的状态进行合并，比如使用会话窗口时。 最后，clear() 方法处理在对应窗口被移除时所需的逻辑。有两点需要注意： 前三个方法通过返回 TriggerResult 来决定 trigger 如何应对到达窗口的事件。应对方案有以下几种： CONTINUE: 什么也不做 FIRE: 触发计算 PURGE: 清空窗口内的元素 FIRE_AND_PURGE: 触发计算，计算结束后清空窗口内的元素 上面的任意方法都可以用来注册 processing-time 或 event-time timer。 4.1 默认Trigger 从window代码结构可以看出，Trigger是一个可选项，那是因为除非Window Assigners不是使用Flink已经定义好的，否则都是会绑定有有默认的Trigger实现类 EventTimeTrigger：当水印通过窗口末尾时触发的触发器 ProcessingTimeTrigger：当系统时间通过窗口末尾时触发的触发器 DeltaTrigger：一种基于DeltaFunction和阈值触发的触发器 CountTrigger：一旦窗口中的元素数量达到给定数量时就触发的触发器 PurgingTrigger：一种触发器，可以将任何触发器转换为清除触发器 ContinuousProcessingTimeTrigger：触发器根据给定的时间间隔连续触发，时间间隔依赖于Job所在机器系统时间 ContinuousEventTimeTrigger：触发器根据给定的时间间隔连续触发，时间间隔依赖于水印时间戳 NeverTrigger：一个从来不触发的触发器，作为GlobalWindow的默认触发器4.2 自定义Trigger通常来讲，在碰到无法通过时间来划分的Window时，一般不会选择去重写Window Assigner，而是使用Global WIndow和自定义Trigger来处理。拿CountTrigger源码为例public class CountTrigger&amp;lt;W extends Window&amp;gt; extends Trigger&amp;lt;Object, W&amp;gt; { // 定义属性 private final long maxCount; private final ReducingStateDescriptor&amp;lt;Long&amp;gt; stateDesc = new ReducingStateDescriptor&amp;lt;&amp;gt;(&quot;count&quot;, new Sum(), LongSerializer.INSTANCE); // 构造方法 private CountTrigger(long maxCount) { this.maxCount = maxCount; } // 每个元素被加入窗口时调用 @Override public TriggerResult onElement(Object element, long timestamp, W window, TriggerContext ctx) throws Exception { // 通过state获取当前count数量 ReducingState&amp;lt;Long&amp;gt; count = ctx.getPartitionedState(stateDesc); // 每个元素进来时进行+1操作 count.add(1L); // 将count与maxCount进行对比，如果大于等于maxCount就触发计算并清空count值 if (count.get() &amp;gt;= maxCount) { count.clear(); return TriggerResult.FIRE; } // 小于则不进行操作 return TriggerResult.CONTINUE; } //在注册的 event-time timer 触发时调用，由于该Trigger与时间无关，所以返回CONTINUE即可 @Override public TriggerResult onEventTime(long time, W window, TriggerContext ctx) { return TriggerResult.CONTINUE; } //在注册的 process-time timer 触发时调用，由于该Trigger与时间无关，所以返回CONTINUE即可 @Override public TriggerResult onProcessingTime(long time, W window, TriggerContext ctx) throws Exception { return TriggerResult.CONTINUE; } // clear() 方法处理在对应窗口被移除时所需的逻辑 窗口被移除时清空state数据 @Override public void clear(W window, TriggerContext ctx) throws Exception { ctx.getPartitionedState(stateDesc).clear(); }}5. EvictorFlink 的窗口模型允许在 WindowAssigner 和 Trigger 之外指定可选的 Evictor。 如本文开篇的代码中所示，通过 evictor(…) 方法传入 Evictor。 Evictor 可以在 trigger 触发后、调用窗口函数之前或之后从窗口中删除元素。 Evictor 接口提供了两个方法实现此功能： /** * Optionally evicts elements. Called before windowing function. * * @param elements The elements currently in the pane. * @param size The current number of elements in the pane. * @param window The {@link Window} * @param evictorContext The context for the Evictor */ void evictBefore( Iterable&amp;lt;TimestampedValue&amp;lt;T&amp;gt;&amp;gt; elements, int size, W window, EvictorContext evictorContext); /** * Optionally evicts elements. Called after windowing function. * * @param elements The elements currently in the pane. * @param size The current number of elements in the pane. * @param window The {@link Window} * @param evictorContext The context for the Evictor */ void evictAfter( Iterable&amp;lt;TimestampedValue&amp;lt;T&amp;gt;&amp;gt; elements, int size, W window, EvictorContext evictorContext);evictBefore() 包含在调用窗口函数前的逻辑，而 evictAfter() 包含在窗口函数调用之后的逻辑。 在调用窗口函数之前被移除的元素不会被窗口函数计算。Flink 内置有三个 evictor： CountEvictor: 仅记录用户指定数量的元素，一旦窗口中的元素超过这个数量，多余的元素会从窗口缓存的开头移除 DeltaEvictor: 接收 DeltaFunction 和 threshold 参数，计算最后一个元素与窗口缓存中所有元素的差值， 并移除差值大于或等于 threshold 的元素。 TimeEvictor: 接收 interval 参数，以毫秒表示。 它会找到窗口中元素的最大 timestamp max_ts 并移除比 max_ts - interval 小的所有元素。p.s. 指定一个 evictor 可以避免预聚合，因为窗口中的所有元素在计算前都必须经过 evictor。5.1 自定义Evictor相较于自定义Trigger，Evictor实现起来更为简单，但是使用场景也比较少，拿CountEvictor为例public class CountEvictor&amp;lt;W extends Window&amp;gt; implements Evictor&amp;lt;Object, W&amp;gt; { private static final long serialVersionUID = 1L; // 定义最大数量 private final long maxCount; // 定义执行位置 private final boolean doEvictAfter; // 构造函数 private CountEvictor(long count, boolean doEvictAfter) { this.maxCount = count; this.doEvictAfter = doEvictAfter; } private CountEvictor(long count) { this.maxCount = count; this.doEvictAfter = false; } // 重写调用窗口函数前 @Override public void evictBefore( Iterable&amp;lt;TimestampedValue&amp;lt;Object&amp;gt;&amp;gt; elements, int size, W window, EvictorContext ctx) { // 根据条件判断是否执行 if (!doEvictAfter) { evict(elements, size, ctx); } } // 重写调用窗口函数后 @Override public void evictAfter( Iterable&amp;lt;TimestampedValue&amp;lt;Object&amp;gt;&amp;gt; elements, int size, W window, EvictorContext ctx) { if (doEvictAfter) { evict(elements, size, ctx); } } private void evict(Iterable&amp;lt;TimestampedValue&amp;lt;Object&amp;gt;&amp;gt; elements, int size, EvictorContext ctx) { // 如果元素数量大于设定值则将超过的元素抛弃 if (size &amp;lt;= maxCount) { return; } else { int evictedCount = 0; for (Iterator&amp;lt;TimestampedValue&amp;lt;Object&amp;gt;&amp;gt; iterator = elements.iterator(); iterator.hasNext(); ) { iterator.next(); evictedCount++; if (evictedCount &amp;gt; size - maxCount) { break; } else { iterator.remove(); } } } }}6. 窗口函数（Window Function）Window Assigner、Trigger、Evictor都只是对数据本身、以及触发计算的逻辑进行了处理，而如何对数据进行计算，则就是window function的任务了窗口函数有三种：ReduceFunction、AggregateFunction 或 ProcessWindowFunction。其中ReduceFunction和AggregateFunction执行效率会更高，因为他们会就地聚合到达的元素， 且每个窗口仅储存一个值6.1 ProcessWindowFunctionReduceFunction、AggregateFunction相对来说比较简单，查看下接口代码就能够很好的理解。ProcessWindowFunction其实也比较简单，就是将整个窗口的数据返回给你（windowAll返回所有数据，keyby&amp;amp;window按key返回数据），然后自行对数据进行处理和输出DataStream&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;&amp;gt; input = ...;input .keyBy(t -&amp;gt; t.f0) .window(TumblingEventTimeWindows.of(Time.minutes(5))) .process(new MyProcessWindowFunction());/* ... */public class MyProcessWindowFunction extends ProcessWindowFunction&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;, String, String, TimeWindow&amp;gt; { /** * 重写process方式来处理数据 * @param s key 参数由 keyBy() 中指定的 KeySelector 选出 * @param context 上下文信息，可用于获取窗口、state等相关信息 * @param elements 该窗口的数据 * @param out A collector for emitting elements. * @throws Exception */ @Override public void process(String key, Context context, Iterable&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;&amp;gt; input, Collector&amp;lt;String&amp;gt; out) { long count = 0; for (Tuple2&amp;lt;String, Long&amp;gt; in: input) { count++; } out.collect(&quot;Window: &quot; + context.window() + &quot;count: &quot; + count); }}ProcessWindowFunction还有一个值得区分的概念per-window state，该state可以通过process()中的context获取到。context可以获取到以下两种状态 globalState()，访问不受window影响的全局 keyed state，与getRuntimeContext().getState()功能一致 windowState(), 访问作用域仅限于当前窗口的 keyed state6.2 增量聚合的 ProcessWindowFunctionProcessWindowFunction 可以与 ReduceFunction 或 AggregateFunction 搭配使用， 使其能够在数据到达窗口的时候进行增量聚合。当窗口关闭时，ProcessWindowFunction 将会得到聚合的结果。这样在AggregateFunction 完成后可以再进一步进行处理。DataStream&amp;lt;SensorReading&amp;gt; input = ...;input .keyBy(&amp;lt;key selector&amp;gt;) .window(&amp;lt;window assigner&amp;gt;) // 使用起来也很简单，就是在原有的ReuduceFunction/AggregationFunction后加上 // ProcessWindowFunction即可 .reduce(new MyReduceFunction(), new MyProcessWindowFunction());// Function definitionsprivate static class MyReduceFunction implements ReduceFunction&amp;lt;SensorReading&amp;gt; { public SensorReading reduce(SensorReading r1, SensorReading r2) { return r1.value() &amp;gt; r2.value() ? r2 : r1; }}private static class MyProcessWindowFunction extends ProcessWindowFunction&amp;lt;SensorReading, Tuple2&amp;lt;Long, SensorReading&amp;gt;, String, TimeWindow&amp;gt; { public void process(String key, Context context, Iterable&amp;lt;SensorReading&amp;gt; minReadings, Collector&amp;lt;Tuple2&amp;lt;Long, SensorReading&amp;gt;&amp;gt; out) { SensorReading min = minReadings.iterator().next(); out.collect(new Tuple2&amp;lt;Long, SensorReading&amp;gt;(context.window().getStart(), min)); }}p.s. WindowFunction是旧版的ProcessWindowFunction，只能提供更少的环境信息且缺少一些高级的功能，比如 per-window state。7. Allowed Lateness VS WatermarkAllowed Lateness就是字面含义，允许数据迟到的时间，这个概念很容易和Watermark混淆。从Flink WIndow的生命周期来看，一个窗口被完全删除的时间 = 结束时间戳 + 用户定义的 allowed lateness。如果是event time，则是结束时间戳 + watermark +用户定义的 allowed lateness。可以看出Allowed Lateness和Watermark都可以用于处理迟到的数据，但是他们在概念上和触发逻辑上是不同的 Watermark 是只在Event Time下才存在的概念，用于标识一个结束时间戳的正式结束。例如，在一个周期为10s，watermark为3s的滚动窗口中，[2000-01-01 00:00:00 ,2000-01-01 00:00:10)窗口本应该在event time为00:00:10的时候该窗口触发计算，但是因为watermark为3，所以一直到00:00:13的时候才表示该窗口结束并触发计算 同样以上面的滑动窗口为例，如果Allowed Lateness为默认值0时，在00:00:13时窗口触发计算并删除，但如果设置Allowed Lateness为5s，则是窗口会在00:00:13后继续等待5s，而在这等待的5s内，每次有数据进来都会再次触发一致窗口函数的计算， 这种触发被称作 late firing，与表示第一次触发窗口的 main firing 相区别。 如果是使用会话窗口的情况，late firing 可能会进一步合并已有的窗口，因为他们可能会连接现有的、未被合并的窗口。 综上，watermark是一个在event-time下的结束标识，而Allowed Lateness 则是允许迟到的最大时间7.1 超过Allowed Lateness数据的处理在配置Allowed Lateness之后，还是有可能存在超过配置的数据，如果需要获取到那一部分数据，则需要用到旁侧数据，代码如下// 声明标识final OutputTag&amp;lt;T&amp;gt; lateOutputTag = new OutputTag&amp;lt;T&amp;gt;(&quot;late-data&quot;){};DataStream&amp;lt;T&amp;gt; input = ...;SingleOutputStreamOperator&amp;lt;T&amp;gt; result = input .keyBy(&amp;lt;key selector&amp;gt;) .window(&amp;lt;window assigner&amp;gt;) .allowedLateness(&amp;lt;time&amp;gt;) // 将标识与迟到数据进行绑定 .sideOutputLateData(lateOutputTag) .&amp;lt;windowed transformation&amp;gt;(&amp;lt;window function&amp;gt;);// 通过标识获取到迟到数据DataStream&amp;lt;T&amp;gt; lateStream = result.getSideOutput(lateOutputTag);参考 Windows Flink Window 机制深入理解 Flink的时间语义" }, { "title": "Flink SQL — 基本使用", "url": "/posts/Flink-SQL-%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/", "categories": "大数据框架", "tags": "flink", "date": "2022-06-08 00:00:00 +0800", "snippet": "1. Flink SQL的运行环境在Flink1.15版本下，Flink对Scala依赖做了一定处理，参考Scala Free in One Fifteen。总而言之依赖不再受Scala版本的影响，所以对于Flink SQL的依赖也同样带来了一点变化，参考官网如下 您要使用的 API 您需要添加的依赖项 DataStream flink-streaming-java DataStream Scala 版 flink-streaming-scala_2.12 Table API flink-table-api-java Table API Scala 版 flink-table-api-scala_2.12 Table API + DataStream flink-table-api-java-bridge Table API + DataStream Scala 版 flink-table-api-scala-bridge_2.12 Flink SQL运行环境如下：// flinkVersion = 1.15.0implementation &quot;org.apache.flink:flink-table-api-java-bridge:${flinkVersion}&quot;// for table API runtimeimplementation &quot;org.apache.flink:flink-runtime:${flinkVersion}&quot;implementation &quot;org.apache.flink:flink-table-runtime:${flinkVersion}&quot;implementation &quot;org.apache.flink:flink-table-planner-loader:${flinkVersion}&quot;// execute in IDEimplementation &quot;org.apache.flink:flink-clients:${flinkVersion}&quot;更多配置详细参考，项目配置2. 程序骨架// 初始化环境配置EnvironmentSettings settings = EnvironmentSettings.newInstance() // 批处理模式 //.inBatchMode() // 流处理模式 .inStreamingMode() .build();//根据环境配置创建Table Environment环境TableEnvironment tableEnv = TableEnvironment.create(settings);// 使用Table API创建SourceTable临时表tableEnv.createTemporaryTable(&quot;SourceTable&quot;, TableDescriptor.forConnector(&quot;datagen&quot;) .schema(Schema.newBuilder() .column(&quot;f0&quot;, DataTypes.STRING()) .build()) .option(DataGenConnectorOptions.ROWS_PER_SECOND, 2L) .option(&quot;fields.f0.length&quot;, &quot;1&quot;) .build());// 使用SQL创建SinkTable临时表tableEnv.executeSql(&quot;CREATE TEMPORARY TABLE SinkTable(f0 String,num BIGINT) WITH (&#39;connector&#39; = &#39;print&#39;)&quot;);// 执行SQL语句Table table = tableEnv.sqlQuery(&quot;SELECT f0,COUNT(1) AS num FROM SourceTable GROUP BY f0&quot;);// 将执行结果输出TablePipeline insertPipeline = table.insertInto(&quot;SinkTable&quot;);insertPipeline.execute();// 等同于上面两句table.executeInsert(&quot;SinkTable&quot;);从整个程序看来，一个Table API&amp;amp;SQL程序主要包含以下几个步骤： 创建表环境（TableEnvironment） 获取表信息 使用Table API或SQL对表数据进行操作 将结果输出到外部系统（调用execute()，调用这一步也可被整合到一起）3. 创建TableEnvironmentTableEnvironment是Table API &amp;amp; SQL编程中最基础的类，也是整个程序的入口，它包含了程序的核心上下文信息，除此以外TableEnvironment核心功能还包括： 向Catelog中注册Table或者获取Table，即表管理 Catelog管理 注册用户自定义函数 加载可插拔Module 执行SQL脚本 DataStream 和 Table（Table\\SQL API 的查询结果）之间进⾏转换创建TableEnvironment具体代码 通过环境配置之间进行创建```javaimport org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.TableEnvironment;EnvironmentSettings settings = EnvironmentSettings .newInstance() .inStreamingMode() //.inBatchMode() .build();TableEnvironment tEnv = TableEnvironment.create(settings);2. 通过`StreamExecutionEnvironment`转换过来```javaimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;import org.apache.flink.table.api.EnvironmentSettings;import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);4. 创建Table⼀个表的全名（标识）会由三个部分组成： Catalog名称.数据库名称.表名称 ，如果 Catalog名称或者数据库名称没有指明，就会使⽤默认值 default例如，下⾯这个SQL创建的Table的全名为default.default.table1tableEnv.executeSql(&quot;CREATE TEMPORARY TABLE table1 ... WITH ( &#39;connector&#39; = ... )&quot;);表根据来源又分为视图（虚拟表）和普通表（外部表） 视图 VIEW (virtual table)：从已经存在的表中创建，视图⼀般是⼀个 SQL 逻辑的查询结果 TableEnvironment tableEnv = ...; // 通过对表X进行一定的查询Table projTable = tableEnv.from(&quot;X&quot;).select(...);// 将查询结果注册成临时视图projectedTable1tableEnv.createTemporaryView(&quot;projectedTable1&quot;, projTable); 外部表 Connector Tables：描述的是外部数据，例如⽂件（HDFS）、消息队列（Kafka）等。具体外部表创建可参考Table &amp;amp; SQL Connectors // 可以通过Table API继续创建，也可以通过SQL进行创建// Table API，使用TableDescriptor对表信息进行描述final TableDescriptor sourceDescriptor = TableDescriptor.forConnector(&quot;datagen&quot;) .schema(Schema.newBuilder() .column(&quot;f0&quot;, DataTypes.STRING()) .build()) .option(DataGenOptions.ROWS_PER_SECOND, 100) .build();// 将表信息注册成表，以下为临时外部表和永久外部表tableEnv.createTable(&quot;SourceTableA&quot;, sourceDescriptor);tableEnv.createTemporaryTable(&quot;SourceTableB&quot;, sourceDescriptor);// 通过SQL对表进行创建tableEnv.executeSql(&quot;CREATE [TEMPORARY] TABLE MyTable (...) WITH (...)&quot;); 临时表&amp;amp;永久表表（视图、外部表）可以是临时的，并与单个 Flink session（可以理解为 Flink 任务运⾏⼀次就是⼀个 session） 的⽣命周期绑定，也可以是永久的，并且在所有的 Flink session 都⽣效 临时表：通常保存于内存中并且仅在创建它们的 Flink session（可以理解为⼀次 Flink 任务的运⾏）持续期间存在。这些表对于其它 session（即其他Flink任务或⾮此次运⾏的 Flink 任务）是不可⻅的。因为 这个表的元数据没有被持久化 永久表：需要外部 Catalog来持久化表的元数据。⼀旦永久表被创建，它将对任何连接到这个Catalog的 Flink session 可⻅且持续存在，直⾄从 Catalog 中被明确删除 如果临时表和永久表使⽤了相同的名称（Catalog名.数据库名.表名）。那么在这个 Flink session 中，你的任务访问到这个表时，访问到的永远是临时表（即相同名称的表，临时表会屏蔽永久表）5.在Table上进行查询5.1 Table API基于Table，我们可以调用Table API或者SQL来查询其中的数据。Table API和编程语言结合更紧密，我们可以在Table类上使用链式调用，调用Table类中的各种方法，执行各类关系型操作。如下//获取Table Environment环境TableEnvironment tableEnv =...//创建表（也可以忽略此步直接获取表信息）//对表进行查询Table table = tableEnv .from(&quot;SourceTable&quot;) .groupBy($(&quot;f0&quot;)) .select($(&quot;f0&quot;), $(&quot;f0&quot;).count().as(&quot;num&quot;));更多Table API，参考Table API5.2 SQL我们也可以直接在Table执行SQL语句。SQL标准中定义了一系列语法和关键字，开发者可以基于SQL标准来编写SQL语句。与Table API中函数调用的方式不同，SQL语句是纯文本形式的。Flink SQL基于Apache Calcite（以下简称Calcite），将SQL语句转换为Flink可执行程序。Calcite支持SQL标准，因此Flink SQL也支持SQL标准。//获取Table Environment环境TableEnvironment tableEnv =...//创建表（也可以忽略此步直接获取表信息）//对表进行查询Table table = tableEnv.sqlQuery(&quot;SELECT f0,COUNT(1) AS num FROM SourceTable GROUP BY f0&quot;);6. 结果输出我们可以将查询结果通过TableSink输出到外部系统。TableSink和之前提到的Sink很像，它是一个数据输出的统一接口，可以将数据以CSV、Parquet、Avro等格式序列化，并将数据发送到关系型数据库、KV数据库、消息队列或文件系统上。TableSink与Catalog、Schema等概念紧密相关。//获取Table Environment环境TableEnvironment tableEnv =...//创建表（也可以忽略此步直接获取表信息）//对表进行查询Table table = ...// 创建用于输出的结果表tableEnv.connect(new FileSystem().path(&quot;...&quot;)) .withFormat(new Csv().fieldDelimiter(&#39;|&#39;)) .withSchema(schema) .createTemporaryTable(&quot;CsvSinkTable&quot;);// 结果输出table.insertInto(&quot;CsvSinkTable&quot;);7. 执行作业以上部分是一个Table API &amp;amp; SQL作业的核心代码编写阶段，但千万不要忘记调用execute方法来执行这个作业，以下为作业执行的大致流程 Table API &amp;amp; SQL从调用到执行的大致流程。一个Table API或者SQL调用经过Planner最终转化为一个JobGraph，Planner在中间起到一个转换和优化的作用。对于流作业和批作业，Blink Planner分别有相应的优化规则也可以使用TableEnvironment.explain(table)来将查询转化为物理执行计划。System.out.println(table.explain());== Abstract Syntax Tree ==LogicalProject(f0=[$0], num=[$1])+- LogicalAggregate(group=[{0}], EXPR$0=[COUNT($0)]) +- LogicalTableScan(table=[[default_catalog, default_database, SourceTable]])== Optimized Physical Plan ==GroupAggregate(groupBy=[f0], select=[f0, COUNT(f0) AS EXPR$0])+- Exchange(distribution=[hash[f0]]) +- TableSourceScan(table=[[default_catalog, default_database, SourceTable]], fields=[f0])== Optimized Execution Plan ==GroupAggregate(groupBy=[f0], select=[f0, COUNT(f0) AS EXPR$0])+- Exchange(distribution=[hash[f0]]) +- TableSourceScan(table=[[default_catalog, default_database, SourceTable]], fields=[f0])可以得到相应的语法树（未优化的逻辑执行计划）、优化后的逻辑执行计划以及最终的物理执行计划以上就是Flink SQL的基本使用参考 Concepts &amp;amp; Common API Table API &amp;amp; SQL综述" }, { "title": "Java - Guava Cache使用", "url": "/posts/Java-Guava-Cache%E4%BD%BF%E7%94%A8/", "categories": "编程语言, java", "tags": "java", "date": "2022-04-27 00:00:00 +0800", "snippet": "1. 概述1.1 缓存缓存在现有业务系统中非常常见，主要是通过保存一个计算或索引代价很高的值且会被多次使用的值，从而减少该数据获取时间，最终提高系统整体速度。缓存常分为本地缓存和远端缓存，常见的远端缓存有Redis、Memcache，本地缓存一般是使用Map的方式保存在本地内存中。1.2 Guava CacheGuava Cache 与 ConcurrentMap 很相似，但也不完全一样，前者增加了更多的元素失效策略，后者只能显示的移除元素，所以如果不需要 Cache 中的特性，使用 ConcurrentHashMap 有更好的内存效率2. Guava Cache的使用以下代码展示了如何创建一个缓存对象并使用public static void main(String[] args) { LoadingCache&amp;lt;String, String&amp;gt; build = CacheBuilder.newBuilder() .concurrencyLevel(8) //设置缓存容器的初始容量为10 .initialCapacity(10) //设置缓存最大容量为100，超过100之后就会按照LRU最近虽少使用算法来移除缓存项 .maximumSize(100) //是否需要统计缓存情况,该操作消耗一定的性能,生产环境应该去除 .recordStats() //设置写缓存后n秒钟过期 .expireAfterWrite(60, TimeUnit.SECONDS) //设置读写缓存后n秒钟过期,实际很少用到,类似于expireAfterWrite //.expireAfterAccess(17, TimeUnit.SECONDS) //只阻塞当前数据加载线程，其他线程返回旧值 //.refreshAfterWrite(13, TimeUnit.SECONDS) //设置缓存的移除通知 .removalListener(notification -&amp;gt; { System.out.println(notification.getKey() + &quot; &quot; + notification.getValue() + &quot; 被移除,原因:&quot; + notification.getCause()); }) //build方法中可以指定CacheLoader，在缓存不存在时通过CacheLoader的实现自动加载缓存 .build(getCacheLoader()); System.out.println(build.getUnchecked(&quot;Jack&quot;));}public static CacheLoader&amp;lt;String, String&amp;gt; getCacheLoader() { return new CacheLoader&amp;lt;String, String&amp;gt;() { @Override public String load(String key) throws Exception { return &quot;Hello &quot; + key; } };}LoadingCache 是 Cache 的子接口，相比较于 Cache，当从 LoadingCache 中读取一个指定 key 的记录时，如果该记录不存在，则 LoadingCache 可以自动执行加载数据到缓存的操作在调用 CacheBuilder 的 build 方法时，代码中有传递一个 CacheLoader 类型的参数，CacheLoader 的 load 方法需要我们提供实现。当调用 LoadingCache 的 get 方法时，如果缓存不存在对应 key 的记录，则 CacheLoader 中的 load 方法会被自动调用从外存加载数据，load 方法的返回值会作为 key 对应的 value 存储到 LoadingCache 中，并从 get 方法返回。当然如果你不想指定重建策略，那么你可以使用无参的 build() 方法，它将返回 Cache 类型的构建对象。2.1 可选配置解析2.1.1 缓存的并发级别CacheBuilder.newBuilder() // 设置并发级别为cpu核心数 .concurrencyLevel(Runtime.getRuntime().availableProcessors()) .build();Guava 提供了设置并发级别的 api，使得缓存支持并发的写入和读取。同 ConcurrentHashMap 类似 Guava cache 的并发也是通过分离锁实现。在一般情况下，将并发级别设置为服务器 cpu 核心数是一个比较不错的选择。2.1.2 缓存的初始容量设置CacheBuilder.newBuilder() // 设置初始容量为100 .initialCapacity(100) .build();我们在构建缓存时可以为缓存设置一个合理大小初始容量，由于 Guava 的缓存使用了分离锁的机制，扩容的代价非常昂贵，所以合理的初始容量能够减少缓存容器的扩容次数。2.1.3 设置最大存储Guava Cache 可以在构建缓存对象时指定缓存所能够存储的最大记录数量。当 Cache 中的记录数量达到最大值后再调用 put 方法向其中添加对象，Guava 会先从当前缓存的对象记录中选择一条删除掉，腾出空间后再将新的对象存储到 Cache 中。 基于容量的清除 (size-based eviction): 通过 CacheBuilder.maximumSize(long) 方法可以设置 Cache 的最大容量数，当缓存数量达到或接近该最大值时，Cache 将清除掉那些最近最少使用的缓存; ** 基于权重的清除: ** 使用 CacheBuilder.weigher(Weigher) 指定一个权重函数，并且用 CacheBuilder.maximumWeight(long) 指定最大总重。比如每一项缓存所占据的内存空间大小都不一样，可以看作它们有不同的 “权重”（weights）。2.1.4 缓存清除策略 基于存活时间的清除 expireAfterWrite 写缓存后多久过期 expireAfterAccess 读写缓存后多久过期 refreshAfterWrite 写入数据后多久过期, 只阻塞当前数据加载线程, 其他线程返回旧值 这几个策略时间可以单独设置, 也可以组合配置。 上面提到的基于容量的清除 显式清除 个别清除：Cache.invalidate(key) 批量清除：Cache.invalidateAll(keys) 清除所有缓存项：Cache.invalidateAll() 基于引用的清除（Reference-based Eviction）：在构建 Cache 实例过程中，通过设置使用弱引用的键、或弱引用的值、或软引用的值，从而使 JVM 在 GC 时顺带实现缓存的清除，不过一般不轻易使用这个特性。 CacheBuilder.weakKeys()：使用弱引用存储键。当键没有其它（强或软）引用时，缓存项可以被垃圾回收。因为垃圾回收仅依赖恒等式，使用弱引用键的缓存用而不是 equals 比较键。 CacheBuilder.weakValues()：使用弱引用存储值。当值没有其它（强或软）引用时，缓存项可以被垃圾回收。因为垃圾回收仅依赖恒等式，使用弱引用值的缓存用而不是 equals 比较值。 CacheBuilder.softValues()：使用软引用存储值。软引用只有在响应内存需要时，才按照全局最近最少使用的顺序回收。考虑到使用软引用的性能影响，我们通常建议使用更有性能预测性的缓存大小限定（见上文，基于容量回收）。使用软引用值的缓存同样用 == 而不是 equals 比较值。 2.2 缓存数据清理 使用 CacheBuilder 构建的缓存不会 “自动” 执行清理和回收工作，也不会在某个缓存项过期后马上清理，也没有诸如此类的清理机制。相反，它会在写操作时顺带做少量的维护工作，或者偶尔在读操作时做——如果写操作实在太少的话。 这样做的原因在于：如果要自动地持续清理缓存，就必须有一个线程，这个线程会和用户操作竞争共享锁。此外，某些环境下线程创建可能受限制，这样 CacheBuilder 就不可用了。LoadingCache&amp;lt;String, String&amp;gt; build = CacheBuilder.newBuilder() //设置写缓存后n秒钟过期 .expireAfterWrite(3, TimeUnit.SECONDS) .build(new CacheLoader&amp;lt;String, String&amp;gt;() { @Override public String load(String key) throws Exception { return key + &quot; value&quot;; } });Thread t1 = new Thread(() -&amp;gt; { try { build.put(&quot;a&quot;, &quot;va&quot;); Thread.sleep(6 * 1000); build.put(&quot;b&quot;, &quot;vb&quot;); } catch (InterruptedException e) { e.printStackTrace(); }});t1.start();for (int i = 0; i &amp;lt; 10; i++) { System.out.println(&quot;当前缓存长度：&quot; + build.size()); Thread.sleep(1000);}System.out.println(&quot;a值：&quot;+build.getIfPresent(&quot;a&quot;));System.out.println(&quot;b值：&quot;+build.get(&quot;b&quot;));====================输出结果=========================当前缓存长度：0当前缓存长度：1当前缓存长度：1当前缓存长度：1当前缓存长度：1当前缓存长度：1当前缓存长度：2当前缓存长度：2当前缓存长度：2当前缓存长度：2a值：nullb值：b value上面代码说明了，如果数据清理并不会主动发生，而是在使用到该条数据时才会进行清理或刷新2.3 给移出操作添加一个监视器CacheBuilder.newBuilder() //设置缓存的移除通知 .removalListener((notification) -&amp;gt; { System.out.println(notification.getKey() + &quot; &quot; + notification.getValue() + &quot; 被移除,原因:&quot; + notification.getCause()); }) .build();但是要注意的是：默认情况下，监听器方法是在移除缓存时同步调用的。因为缓存的维护和请求响应通常是同时进行的，代价高昂的监听器方法在同步模式下会拖慢正常的缓存请求。在这种情况下，你可以使用**RemovalListeners.asynchronous(RemovalListener, Executor)**把监听器装饰为异步操作。2.4 调用统计Cache&amp;lt;String, String&amp;gt; cache = CacheBuilder.newBuilder() .maximumSize(3) .recordStats() //开启统计信息开关 .build();cache.put(&quot;1&quot;, &quot;v1&quot;);cache.put(&quot;2&quot;, &quot;v2&quot;);cache.put(&quot;3&quot;, &quot;v3&quot;);cache.put(&quot;4&quot;, &quot;v4&quot;);cache.getIfPresent(&quot;1&quot;);cache.getIfPresent(&quot;2&quot;);cache.getIfPresent(&quot;3&quot;);cache.getIfPresent(&quot;4&quot;);cache.getIfPresent(&quot;5&quot;);cache.getIfPresent(&quot;6&quot;);System.out.println(cache.stats()); //获取统计信息参考 Guava cache使用总结 CachesExplained" }, { "title": "Java 并发 - CompletableFuture详解", "url": "/posts/Java%E5%B9%B6%E5%8F%91-CompletableFuture%E8%AF%A6%E8%A7%A3/", "categories": "编程语言, java", "tags": "java", "date": "2022-04-02 00:00:00 +0800", "snippet": "1. 概述1.1 Java8 之前的异步编程在Java8之前，异步编程主要使用Future 类来描述一个异步计算的结果。你可以使用 isDone() 方法检查计算是否完成，或者使用 get() 方法阻塞住调用线程，直到计算完成返回结果，也可以使用 cancel() 方法停止任务的执行。虽然 Future 提供了异步执行任务的能力，但是对于结果的获取却是很不方便，只能通过阻塞或者轮询的方式得到任务的结果。阻塞的方式显然和我们异步编程的初衷相违背，轮询的方式又会耗费无谓的 CPU 资源，而且也不能及时的获取结果。1.2 CompletableFuture基本概述在 Java 8 中，新增了一个包含 50 多个方法的类：CompletableFuture，提供了非常强大的 Future 扩展功能，可以帮助我们简化异步编程的复杂性，提供函数式编程的能力。在 Java 8 中，新增了一个包含 50 多个方法的类：CompletableFuture，提供了非常强大的 Future 扩展功能，可以帮助我们简化异步编程的复杂性，提供函数式编程的能力。2. CompletableFuture 类功能概览public class CompletableFuture&amp;lt;T&amp;gt; implements Future&amp;lt;T&amp;gt;, CompletionStage&amp;lt;T&amp;gt;CompletableFuture 实现了 Future 接口，拥有 Future 所有的特性，比如可以使用 get() 方法获取返回值等；还实现了 CompletionStage 接口，这个接口有超过 40 个方法，功能太丰富了，它主要是为了编排任务的工作流。CompletableFuture提供了方法大约有50多个，单纯一个个记忆，是很麻烦的，因此将其划分为以下几类创建类 completeFuture 可以用于创建默认返回值 runAsync 异步执行，无返回值 supplyAsync 异步执行，有返回值 anyOf 任意一个执行完成，就可以进行下一步动作 allOf 全部完成所有任务，才可以进行下一步任务状态取值类 join 合并结果，等待 get 合并等待结果，可以增加超时时间;get和join区别，join只会抛出unchecked异常，get会返回具体的异常 getNow 如果结果计算完成或者异常了，则返回结果或异常；否则，返回valueIfAbsent的值 isCancelled isCompletedExceptionally isDone控制类（用于主动控制CompletableFuture的完成行为） complete completeExceptionally cancel接续类（ CompletableFuture 最重要的特性，没有这个的话，CompletableFuture就没意义了，用于注入回调行为） thenApply, thenApplyAsync thenAccept, thenAcceptAsync thenRun, thenRunAsync thenCombine, thenCombineAsync thenAcceptBoth, thenAcceptBothAsync runAfterBoth, runAfterBothAsync applyToEither, applyToEitherAsync acceptEither, acceptEitherAsync runAfterEither, runAfterEitherAsync thenCompose, thenComposeAsync whenComplete, whenCompleteAsync handle, handleAsync exceptionally上面方法很多，但是都有规律可循： 带有Async，都是异步方法，对应的没有Async则是同步方法 带有Async后缀结尾的方法，都有两个重载的方法，一个是使用内置的forkjoin线程池，另一个需传入线程池 带有run开头的方法，其入口参数一定是无参的，并且没有返回值，类似于执行Runnable方法。 带有supply的方法，入口也是没有参数的，但是有返回值 带有Accept开头或者结尾的方法，入口参数是有参数，但是没有返回值 带有Apply开头或者结尾的方法，入口有参数，有返回值 带有either后缀的方法，表示谁先完成就消费谁3 CompletableFuture 接口详解3.1 提交执行的静态方法 方法名 描述 runAsync(Runnable runnable) 执行异步代码，使用 ForkJoinPool.commonPool() 作为它的线程池 runAsync(Runnable runnable, Executor executor) 执行异步代码，使用指定的线程池 supplyAsync(Supplier supplier) 异步执行代码，有返回值，使用 ForkJoinPool.commonPool() 作为它的线程池 supplyAsync(Supplier supplier, Executor executor) 异步执行代码，有返回值，使用指定的线程池执行 上述四个方法，都是提交任务的，runAsync 方法需要传入一个实现了 Runnable 接口的方法，supplyAsync 需要传入一个实现了 Supplier 接口的方法，实现 get 方法，返回一个值。3.1.1 run 和 supply 的区别run 就是执行一个方法，没有返回值，supply 执行一个方法，有返回值。3.1.2 一个参数和两个参数的区别第二个参数是线程池，如果没有传，则使用自带的 ForkJoinPool.commonPool() 作为线程池，这个线程池默认创建的线程数是 CPU 的核数（也可以通过 JVM option:-Djava.util.concurrent.ForkJoinPool.common.parallelism 来设置 ForkJoinPool 线程池的线程数）3.2 串行关系 api 一个任务接着一个任务执行。例如，接水-&amp;gt;烧水这些 api 之间主要是能否获得前一个任务的返回值与自己是否有返回值的区别。 api 是否可获得前一个任务的返回值 是否有返回值 thenApply 能 有 thenAccept 能 无 thenRun 不能 无 thenCompose 能 有 3.2.1 thenApply 和 thenApplyAsync 使用public static void main(String[] args) throws Exception { CompletableFuture.supplyAsync(() -&amp;gt; &quot;Hello&quot;) .thenApply(x-&amp;gt;x+&quot; World&quot;) .thenApplyAsync(y-&amp;gt;y+&quot;!&quot;);}3.2.2 thenApply 和 thenApplyAsync 的区别这两个方法的区别，在于谁去执行任务。如果使用 thenApplyAsync，那么执行的线程是从 ForkJoinPool.commonPool() 或者自己定义的线程池中取线程去执行。如果使用 thenApply，又分两种情况，如果 supplyAsync 方法执行速度特别快，那么 thenApply 任务就使用主线程执行，如果 supplyAsync 执行速度特别慢，就是和 supplyAsync 执行线程一样。public static void main(String[] args) throws ExecutionException, InterruptedException { System.out.println(&quot;----------supplyAsync 执行很快&quot;); CompletableFuture&amp;lt;String&amp;gt; future1 = CompletableFuture.supplyAsync(() -&amp;gt; { System.out.println(Thread.currentThread().getName()); return &quot;1&quot;; }).thenApply(s -&amp;gt; { System.out.println(Thread.currentThread().getName()); return &quot;2&quot;; }); System.out.println(future1.get()); System.out.println(&quot;----------supplyAsync 执行很慢&quot;); CompletableFuture&amp;lt;String&amp;gt; future2 = CompletableFuture.supplyAsync(() -&amp;gt; { try { Thread.sleep(1000); } catch (InterruptedException e) { } System.out.println(Thread.currentThread().getName()); return &quot;1&quot;; }).thenApply(s -&amp;gt; { System.out.println(Thread.currentThread().getName()); return &quot;2&quot;; }); System.out.println(future2.get());}执行结果----------supplyAsync 执行很快ForkJoinPool.commonPool-worker-1main2----------supplyAsync 执行很慢ForkJoinPool.commonPool-worker-1ForkJoinPool.commonPool-worker-123.2.3 thenCompose 的使用假设有两个异步任务，第二个任务想要获取第一个任务的返回值，并且做运算，我们可以用 thenCompose。public static void main(String[] args) throws Exception { CompletableFuture&amp;lt;String&amp;gt; future = CompletableFuture.supplyAsync(() -&amp;gt; &quot;Hello&quot;) .thenCompose(x -&amp;gt; CompletableFuture.supplyAsync(() -&amp;gt; x + &quot; World&quot;));}3.3 And 汇聚关系 Api 等待两件事同时做好后再执行后面的事。例如，烧水+拿茶 =&amp;gt;泡茶public static void main(String[] args) throws ExecutionException, InterruptedException { CompletableFuture&amp;lt;Integer&amp;gt; thenComposeOne = CompletableFuture.supplyAsync(() -&amp;gt; 192); CompletableFuture&amp;lt;Integer&amp;gt; thenComposeTwo = CompletableFuture.supplyAsync(() -&amp;gt; 196); CompletableFuture&amp;lt;Integer&amp;gt; thenComposeCount = thenComposeOne .thenCombine(thenComposeTwo, (s, y) -&amp;gt; s + y); thenComposeOne.thenAcceptBoth(thenComposeTwo,(s,y)-&amp;gt; System.out.println(&quot;thenAcceptBoth&quot;)); thenComposeOne.runAfterBoth(thenComposeTwo, () -&amp;gt; System.out.println(&quot;runAfterBoth&quot;)); System.out.println(thenComposeCount.get());}3.3.1 thenCombine 的使用加入我们要计算两个异步方法返回值的和，就必须要等到两个异步任务都计算完才能求和，此时可以用 thenCombine 来完成。3.3.2 thenAcceptBoth接收前面两个异步任务的结果，执行一个回调函数，但是这个回调函数没有返回值。3.3.3 runAfterBoth接收前面两个异步任务的结果，但是回调函数，不接收参数，也不返回值。3.4 Or 汇聚关系 Apipublic static void main(String[] args) throws ExecutionException, InterruptedException { CompletableFuture&amp;lt;Integer&amp;gt; thenComposeOne = CompletableFuture.supplyAsync(() -&amp;gt; 192); CompletableFuture&amp;lt;Integer&amp;gt; thenComposeTwo = CompletableFuture.supplyAsync(() -&amp;gt; 196); CompletableFuture&amp;lt;Integer&amp;gt; thenComposeCount = thenComposeOne .applyToEither(thenComposeTwo, s -&amp;gt; s + 1); thenComposeOne.acceptEither(thenComposeTwo,s -&amp;gt; {}); thenComposeOne.runAfterEither(thenComposeTwo,()-&amp;gt;{}); System.out.println(thenComposeCount.get());}3.4.1 applyToEither任何一个执行完就执行回调方法，回调方法接收一个参数，有返回值3.4.2 acceptEither任何一个执行完就执行回调方法，回调方法接收一个参数，无返回值3.4.3 runAfterEither任何一个执行完就执行回调方法，回调方法不接收参数，也无返回值3.5 处理异常public static void main(String[] args) throws ExecutionException, InterruptedException { CompletableFuture.supplyAsync(() -&amp;gt; { System.out.println(&quot;execute one &quot;); return 100; }) .thenApply(s -&amp;gt; 10 / 0) .thenRun(() -&amp;gt; System.out.println(&quot;thenRun&quot;)) .thenAccept(s -&amp;gt; System.out.println(&quot;thenAccept&quot;)) .exceptionally(s -&amp;gt; { System.out.println(&quot;异常处理&quot;); return null; }); CompletableFuture.runAsync(() -&amp;gt; System.out.println(&quot;other&quot;));}可以使用 exceptionally 来处理异常。使用 handle() 方法也可以处理异常。但是 handle() 方法的不同之处在于，即使没有发生异常，也会执行。参考 Java 8 异步编程 CompletableFuture 全解析 CompletableFuture使用大全，简单易懂" }, { "title": "Java 基础 - 函数式编程详解", "url": "/posts/Java-%E5%9F%BA%E7%A1%80-%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E8%AF%A6%E8%A7%A3/", "categories": "编程语言, java", "tags": "java", "date": "2022-03-29 00:00:00 +0800", "snippet": "1. 概述1.1背景在日常Java编码中，函数式编程使用的频率是非常高的，但是回想一下，自己虽然能很好的使用，但是却没能系统的了解过函数式编程，所以以下为最近学习的一个总结。1.2 概念函数式接口在Java中是指：有且仅有一个抽象方法的接口函数式接口，即适用于函数式编程场景的接口。而Java中的函数式编程体现就是Lambda，所以函数式接口就是可以适用于Lambda使用的接口。只有确保接口中有且仅有一个抽象方法，Java中的Lambda才能顺利地进行推导。2. 函数式接口介绍&amp;amp;基本使用2.1 格式修饰符 interface 接口名称 { public abstract 返回值 方法名称(参数列表) // 其他方式 }// public abstract 可以不写 编译器自动加上修饰符 interface 接口名称 { 返回值 方法名称(参数列表) // 其他方式 }以最常用的Function为例：@FunctionalInterfacepublic interface Function&amp;lt;T, R&amp;gt; { R apply(T t); default &amp;lt;V&amp;gt; Function&amp;lt;V, R&amp;gt; compose(Function&amp;lt;? super V, ? extends T&amp;gt; before) { Objects.requireNonNull(before); return (V v) -&amp;gt; apply(before.apply(v)); } default &amp;lt;V&amp;gt; Function&amp;lt;T, V&amp;gt; andThen(Function&amp;lt;? super R, ? extends V&amp;gt; after) { Objects.requireNonNull(after); return (T t) -&amp;gt; after.apply(apply(t)); } static &amp;lt;T&amp;gt; Function&amp;lt;T, T&amp;gt; identity() { return t -&amp;gt; t; }}2.2 @FunctionalInterface注解// 标明为函数式接口@FunctionalInterfacepublic interface MyFunction { // 抽象方法 void method();}一旦使用该注解来定义接口，编译器将会强制检查该接口是否确实有且仅有一个抽象方法，否则将会报错。需要注意的是，即使不使用该注解，只要满足函数式接口的定义，这仍然是一个函数式接口，使用起来都一样。(该接口是一个标记接口)2.3 自定义函数式接口使用 public static void executeMyFunction(MyFunction function) { function.method(); } public static void main(String[] args) { executeMyFunction(()-&amp;gt; System.out.println(&quot;Hello World&quot;)); }2.4 Java自带函数式接口常用函数式接口 Supplier 你要作为一个供应者,自己生产数据 Consumer 你要作为一个消费者,利用已经准备数据 Function 输入一个或者两个不同或者相同的值转为另一个值 Predicate 输入一个或者两个不同或者相同的值总是输出boolean UnaryOperator 输入一个值转换为相同值输出 BinaryOperator 输入两个相同类型的值 转为相同类型的值输出2.4 主要语法 () -&amp;gt; 代表了 lambda的一个表达式 单行代码无需写return (无论函数式接口有没有返回值),花括号 多行代码必须写花括号,有返回值的一定要写返回值 单行代码且有参数的情况下可以不写 () 如 s-&amp;gt;System.out.println(s) (T t)中的参数类型可写可不写3. 常用函数式接口3.1 Supplier接口(供应接口) 接口名：java.util.function.Supplier 抽象方法：接口仅包含一个无参的方法： T get() 作用：用来获取一个泛型参数指定类型的对象数据。由于这是一个函数式接口，这也就意味着对应的Lambda表达式需要“对外提供”一个符合泛型类型的对象数据。 使用 private static String supplierFunction(Supplier&amp;lt;String&amp;gt; stringSupplier){ return stringSupplier.get(); } public static void main(String[] args) { System.out.println(supplierFunction(()-&amp;gt;&quot;Hello World&quot;)); } 3.2 Consumer接口 接口名：java.util.function.Consumer 抽象方法：void accept(T t) 作用：接口则正好与Supplier接口相反，它不是生产一个数据，而是消费一个数据，其数据类型由泛型决定 默认方法：andThen 作用：如果一个方法的参数和返回值全都是 Consumer 类型，那么就可以实现效果：消费数据的时候，首先做一个操作，然后再做一个操作，实现组合 default Consumer&amp;lt;T&amp;gt; andThen(Consumer&amp;lt;? super T&amp;gt; after) { //1: 返回值为Consumer 那么需要 ()-&amp;gt; 表示函数式接口 //2: accept(t);为生产一个数据供应给 (T t)中的t //3: after.accept(t);为利用这个t再次生成新的函数式接口 实现类始于builder的设计模式 Objects.requireNonNull(after); return (T t) -&amp;gt; { accept(t); after.accept(t); }; } 3.3 Predicate接口 接口名：java.util.function.Predicate 抽象方法：_boolean _test(T t) 作用：需要对某种类型的数据进行判断，从而得到一个boolean值结果 默认方法： Predicate and(Predicate&amp;lt;? _super _T&amp;gt; other) _default _Predicate or(Predicate&amp;lt;? _super _T&amp;gt; other) _default _Predicate negate() (取反) 作用：既然是条件判断，就会存在与、或、非三种常见的逻辑关系。 静态方法：_static _ Predicate isEqual(Object targetRef) 作用：返回值为Predicate，返回一个是否与targetRef相等的函数表达式3.4 Function接口 接口名：java.util.function.Function&amp;lt;T, R&amp;gt; 抽象方法：R apply(T t) 作用：根据一个类型的数据得到另一个类型的数据 默认方法： _default _ Function&amp;lt;V, R&amp;gt; compose(Function&amp;lt;? _super _V, ? _extends _T&amp;gt; before) _default _ Function&amp;lt;T, V&amp;gt; andThen(Function&amp;lt;? _super _R, ? _extends _V&amp;gt; after) 作用：与另一个Function进行结合，一个方法为before，另一个为after default &amp;lt;V&amp;gt; Function&amp;lt;V, R&amp;gt; compose(Function&amp;lt;? super V, ? extends T&amp;gt; before) { Objects.requireNonNull(before); return (V v) -&amp;gt; apply(before.apply(v));} default &amp;lt;V&amp;gt; Function&amp;lt;T, V&amp;gt; andThen(Function&amp;lt;? super R, ? extends V&amp;gt; after) { Objects.requireNonNull(after); return (T t) -&amp;gt; after.apply(apply(t));} 参考 详解JAVA8函数式接口{全} 详细介绍 Java 8 中的 default 关键字与 @FunctionalInterface 注解" }, { "title": "Flink项目构建模板 - Gradle", "url": "/posts/Flink%E9%A1%B9%E7%9B%AE%E6%9E%84%E5%BB%BA%E6%A8%A1%E6%9D%BF-Gradle/", "categories": "大数据框架, flink", "tags": "flink", "date": "2022-01-10 00:00:00 +0800", "snippet": "Flink项目构建模板 - Gradle 环境 Gradle:7.x Java:8 相较于官方模板 升级Gradle Shadow版本 移出过时仓库JCenter 修改过时语法 仓库改为阿里镜像 buildscript { repositories { maven { // 源仓库地址：https://plugins.gradle.org/m2/ url &quot;https://maven.aliyun.com/repository/gradle-plugin&quot; } } dependencies { classpath &quot;gradle.plugin.com.github.johnrengelman:shadow:7.1.2&quot; }}plugins { id &#39;java&#39; id &#39;application&#39; // shadow plugin to produce fat JARs id &#39;com.github.johnrengelman.shadow&#39; version &#39;7.1.2&#39;}//group &#39;org.example&#39;version &#39;1.0-SNAPSHOT&#39;mainClassName = &#39;org.myorg.quickstart.StreamingJob&#39;ext { javaVersion = &#39;1.8&#39; flinkVersion = &#39;1.14.2&#39; scalaBinaryVersion = &#39;2.11&#39; slf4jVersion = &#39;1.7.32&#39; log4jVersion = &#39;2.17.1&#39;}sourceCompatibility = javaVersiontargetCompatibility = javaVersionrepositories { maven { url &#39;https://maven.aliyun.com/repository/public&#39; } mavenCentral()}configurations { flinkShadowJar // dependencies which go into the shadowJar // always exclude these (also from transitive dependencies) since they are provided by Flink flinkShadowJar.exclude group: &#39;org.apache.flink&#39;, module: &#39;force-shading&#39; flinkShadowJar.exclude group: &#39;com.google.code.findbugs&#39;, module: &#39;jsr305&#39; flinkShadowJar.exclude group: &#39;org.slf4j&#39; flinkShadowJar.exclude group: &#39;org.apache.logging.log4j&#39;}dependencies { // -------------------------------------------------------------- // Compile-time dependencies that should NOT be part of the // shadow jar and are provided in the lib folder of Flink // -------------------------------------------------------------- implementation &quot;org.apache.flink:flink-streaming-java_${scalaBinaryVersion}:${flinkVersion}&quot; // -------------------------------------------------------------- // Dependencies that should be part of the shadow jar, e.g. // connectors. These must be in the flinkShadowJar configuration! // -------------------------------------------------------------- //flinkShadowJar &quot;org.apache.flink:flink-connector-kafka:${flinkVersion}&quot; implementation &quot;org.apache.logging.log4j:log4j-api:${log4jVersion}&quot; implementation &quot;org.apache.logging.log4j:log4j-core:${log4jVersion}&quot; implementation &quot;org.apache.logging.log4j:log4j-slf4j-impl:${log4jVersion}&quot; implementation &quot;org.slf4j:slf4j-log4j12:${slf4jVersion}&quot;}// make compileOnly dependencies available for tests:sourceSets { main.compileClasspath += configurations.flinkShadowJar main.runtimeClasspath += configurations.flinkShadowJar test.compileClasspath += configurations.flinkShadowJar test.runtimeClasspath += configurations.flinkShadowJar javadoc.classpath += configurations.flinkShadowJar}test { useJUnitPlatform()}shadowJar { configurations = [project.configurations.flinkShadowJar]}参考 Flink 官网构建文档 Shadow官网文档 阿里镜像仓库" }, { "title": "Java 并发 - synchronized关键字使用", "url": "/posts/Java-%E5%B9%B6%E5%8F%91-synchronized%E5%85%B3%E9%94%AE%E5%AD%97%E4%BD%BF%E7%94%A8/", "categories": "编程语言, java", "tags": "java", "date": "2021-12-31 00:00:00 +0800", "snippet": "1. synchronized的使用1.1 synchronized的作用范围 synchronized可以根据锁的对象，把锁分为两种，分别是类锁和对象锁1.1 对象锁 手动指定锁定对象，也可是是this,也可以是自定义的锁private final Object lock1 = new Object(); public void printStr1() { synchronized (lock1) { for (int i = 0; i &amp;lt; 5; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t i:&quot; + i); } }} synchronized修饰普通方法，锁对象默认为thispublic synchronized void printStr1() { for (int i = 0; i &amp;lt; 5; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t i:&quot; + i); }}1.2 类锁 synchronize修饰静态方法public synchronized static void printStr1() { for (int i = 0; i &amp;lt; 5; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t i:&quot; + i); }} synchronized指定锁对象为Class对象public void printStr1() { synchronized (SynchronizedClassLock.class){ for (int i = 0; i &amp;lt; 5; i++) { System.out.println(Thread.currentThread().getName() + &quot;\\t i:&quot; + i); } }}2. Synchronized加锁原理2.1 加锁和释放锁的原理 通过查看字节码来了解加锁代码：Object lock3 = new Object();public void printStr3() { synchronized (lock3){ } method1();}private static void method1(){}字节码方案信息：关注红色方框里的monitorenter和monitorexit即可。Monitorenter和Monitorexit指令，会让对象在执行，使其锁计数器加1或者减1。每一个对象在同一时间只与一个monitor(锁)相关联，而一个monitor在同一时间只能被一个线程获得，一个对象在尝试获得与这个对象相关联的Monitor锁的所有权的时候，monitorenter指令会发生如下3中情况之一： monitor计数器为0，意味着目前还没有被获得，那这个线程就会立刻获得然后把锁计数器+1，一旦+1，别的线程再想获取，就需要等待 如果这个monitor已经拿到了这个锁的所有权，又重入了这把锁，那锁计数器就会累加，变成2，并且随着重入的次数，会一直累加 这把锁已经被别的线程获取了，等待锁释放monitorexit指令：释放对于monitor的所有权，释放过程很简单，就是讲monitor的计数器减1，如果减完以后，计数器不是0，则代表刚才是重入进来的，当前线程还继续持有这把锁的所有权，如果计数器变成0，则代表当前线程不再拥有该monitor的所有权，即释放锁。下图表现了对象，对象监视器，同步队列以及执行线程状态之间的关系：该图可以看出，任意线程对Object的访问，首先要获得Object的监视器，如果获取失败，该线程就进入同步状态，线程状态变为BLOCKED，当Object的监视器占有者释放后，在同步队列中得线程就会有机会重新获取该监视器2.2 可重入原理：加锁次数计数器上面的demo中在执行完同步代码块之后紧接着再会去执行一个静态同步方法，而这个方法锁的对象依然就这个类对象，那么这个正在执行的线程还需要获取该锁吗? 答案是不必的，从上图中就可以看出来，执行静态同步方法的时候就只有一条monitorexit指令，并没有monitorenter获取锁的指令。这就是锁的重入性，即在同一锁程中，线程不需要再次获取同一把锁。Synchronized先天具有重入性。每个对象拥有一个计数器，当线程获取该对象锁后，计数器就会加一，释放锁后就会将计数器减一参考 关键字: synchronized详解 Synchronized方法锁、对象锁、类锁区别 synchronized到底锁住的是谁？" }, { "title": "MySQL 中执行计划分析 - Optimizer trace表", "url": "/posts/MySQL-%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E5%88%86%E6%9E%90-Optimizer-trace%E8%A1%A8/", "categories": "数据库, mysql", "tags": "mysql", "date": "2021-12-21 00:00:00 +0800", "snippet": "1. 概述​ 对于 MySQL 5.6 以及之前的版本来说，查询优化器就像是一个黑盒子一样，你只能通过 EXPLAIN 语句查看到最后优化器决定使用的执行计划，却无法知道它为什么做这个决策。​ 在 MySQL 5.6 以及之后的版本中，MySQL 提出了一个 optimizer trace 的功能，这个功能可以让我们方便的查看优化器生成执行计划的整个过程。2.Optimizer trace的使用 Optimizer trace 并不是自动就会默认开启的，开启 trace 多多少少都会有一些额外的工作要做，因此并不建议一直开着。但 trace 属于轻量级的工具，开启和关闭都非常简便，对系统的影响也微乎其微。而且支持在 session 中开启，不影响其它 session，对系统的影响降到了最低。2.1 查看optimizer trace状态&amp;gt; show variables like &#39;%optimizer_trace%&#39;;+----------------------------+--------------------------------------------------------------------------+|Variable_name |Value |+----------------------------+--------------------------------------------------------------------------+|optimizer_trace |enabled=off,one_line=off ||optimizer_trace_features |greedy_search=on,range_optimizer=on,dynamic_range=on,repeated_subselect=on||optimizer_trace_limit |1 ||optimizer_trace_max_mem_size|16384 ||optimizer_trace_offset |-1 |+----------------------------+--------------------------------------------------------------------------+ optimizer_trace：enabled状态；one_line的值是控制输出格式的，如果为on那么所有输出都将在一行中展示 optimizer_trace_limit：OPTIMIZER_TRACE表中保存条数 optimizer_trace_max_mem_size：由于优化过程可能会输出很多，如果超过某个限制时，多余的文本将不会被显示 optimizer_trace_offset：查询OPTIMIZER_TRACE表时的偏移量2.2 开启optimizer trace功能SET optimizer_trace=&quot;enabled=on&quot;;SET optimizer_trace_limit=10;SET optimizer_trace_offset=10;SET optimizer_trace_max_mem_size = 32768;注意：在这里设置了optimizer_trace_limit为10主要是因为在使用DataGrip时会自动插入多条数据影响查看2.3 查询上一个语句的优化过程SELECT * FROM information_schema.OPTIMIZER_TRACE; QUERY ：表示我们的查询语句。 TRACE ：表示优化过程的JSON格式文本。 MISSING_BYTES_BEYOND_MAX_MEM_SIZE ：由于优化过程可能会输出很多，如果超过某个限制时，多余的文本将不会被显示，这个字段展示了被忽略的文本字节数。 INSUFFICIENT_PRIVILEGES ：表示是否没有权限查看优化过程，默认值是0，只有某些特殊情况下才会是1 ，我们暂时不关心这个字段的值2.4 关闭optimizer trace 功能SET optimizer_trace=&quot;enabled=off&quot;;SET optimizer_trace_limit=1;SET optimizer_trace_offset=-1;SET optimizer_trace_max_mem_size = 16384;3. 具体分析TRACE文本分析：{ &quot;steps&quot;: [ { &quot;join_preparation&quot;: { # prepare阶段 &quot;select#&quot;: 1, &quot;steps&quot;: [ { &quot;expanded_query&quot;: &quot;/* select#1 */ select sql_no_cache `item_sale_summary`.`ent_id` AS `ent_id`,`item_sale_summary`.`region_code` AS `region_code`,ceiling((count(distinct `item_sale_summary`.`item_code`,`item_sale_summary`.`barcode`,date_format(`item_sale_summary`.`trans_date`,&#39;%Y-%m-%d&#39;)) / ((to_days(&#39;2021-12-05&#39;) - to_days(&#39;2021-11-05&#39;)) + 1))) AS `sku_item_sale` from `item_sale_summary` where ((`item_sale_summary`.`trans_date` between &#39;2021-11-01&#39; and &#39;2021-11-15&#39;) and (`item_sale_summary`.`ent_id` = 1747964630024192400)) group by `item_sale_summary`.`ent_id`,`item_sale_summary`.`region_code`&quot; } ] } }, { &quot;join_optimization&quot;: { # optimize阶段 &quot;select#&quot;: 1, &quot;steps&quot;: [ { &quot;condition_processing&quot;: { # 处理搜索条件 &quot;condition&quot;: &quot;WHERE&quot;, # 原始搜索条件 &quot;original_condition&quot;: &quot;((`item_sale_summary`.`trans_date` between &#39;2021-11-01&#39; and &#39;2021-11-15&#39;) and (`item_sale_summary`.`ent_id` = 1747964630024192400))&quot;, &quot;steps&quot;: [ { &quot;transformation&quot;: &quot;equality_propagation&quot;, # 等值传递转换 &quot;resulting_condition&quot;: &quot;((`item_sale_summary`.`trans_date` between &#39;2021-11-01&#39; and &#39;2021-11-15&#39;) and multiple equal(1747964630024192400, `item_sale_summary`.`ent_id`))&quot; }, { &quot;transformation&quot;: &quot;constant_propagation&quot;, # 常量传递转换 &quot;resulting_condition&quot;: &quot;((`item_sale_summary`.`trans_date` between &#39;2021-11-01&#39; and &#39;2021-11-15&#39;) and multiple equal(1747964630024192400, `item_sale_summary`.`ent_id`))&quot; }, { &quot;transformation&quot;: &quot;trivial_condition_removal&quot;, # 去除没用的条件 &quot;resulting_condition&quot;: &quot;((`item_sale_summary`.`trans_date` between &#39;2021-11-01&#39; and &#39;2021-11-15&#39;) and multiple equal(1747964630024192400, `item_sale_summary`.`ent_id`))&quot; } ] } }, { &quot;substitute_generated_columns&quot;: { # 替换虚拟生成列 } }, { &quot;table_dependencies&quot;: [ # 表的依赖信息 { &quot;table&quot;: &quot;`item_sale_summary`&quot;, &quot;row_may_be_null&quot;: false, &quot;map_bit&quot;: 0, &quot;depends_on_map_bits&quot;: [ ] } ] }, { &quot;ref_optimizer_key_uses&quot;: [ { &quot;table&quot;: &quot;`item_sale_summary`&quot;, &quot;field&quot;: &quot;ent_id&quot;, &quot;equals&quot;: &quot;1747964630024192400&quot;, &quot;null_rejecting&quot;: false }, { &quot;table&quot;: &quot;`item_sale_summary`&quot;, &quot;field&quot;: &quot;ent_id&quot;, &quot;equals&quot;: &quot;1747964630024192400&quot;, &quot;null_rejecting&quot;: false } ] }, { &quot;rows_estimation&quot;: [ # 预估不同单表访问方法的访问成本 { &quot;table&quot;: &quot;`item_sale_summary`&quot;, &quot;range_analysis&quot;: { &quot;table_scan&quot;: { # 全表扫描的行数以及成本 &quot;rows&quot;: 4245934, &quot;cost&quot;: 944293 }, &quot;potential_range_indexes&quot;: [ # 分析可能使用的索引 { &quot;index&quot;: &quot;unique_index&quot;, &quot;usable&quot;: true, # 可能被使用 &quot;key_parts&quot;: [ &quot;trans_date&quot;, &quot;ent_id&quot;, &quot;region_code&quot;, &quot;channel_keyword&quot;, &quot;item_code&quot;, &quot;barcode&quot; ] }, { &quot;index&quot;: &quot;idx_ent_date_region&quot;, &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;ent_id&quot;, &quot;trans_date&quot;, &quot;region_code&quot; ] }, { &quot;index&quot;: &quot;idx_saled_item&quot;, &quot;usable&quot;: true, &quot;key_parts&quot;: [ &quot;ent_id&quot;, &quot;region_code&quot;, &quot;item_code&quot;, &quot;barcode&quot;, &quot;trans_date&quot;, &quot;channel_keyword&quot; ] } ], &quot;best_covering_index_scan&quot;: { &quot;index&quot;: &quot;unique_index&quot;, &quot;cost&quot;: 1.11e6, &quot;chosen&quot;: false, &quot;cause&quot;: &quot;cost&quot; }, &quot;setup_range_conditions&quot;: [ ], &quot;group_index_range&quot;: { &quot;chosen&quot;: false, &quot;cause&quot;: &quot;not_applicable_aggregate_function&quot; }, &quot;analyzing_range_alternatives&quot;: { # 分析各种可能使用的索引的成本 &quot;range_scan_alternatives&quot;: [ { # 使用unique_index的成本分析 &quot;index&quot;: &quot;unique_index&quot;, &quot;ranges&quot;: [ &quot;0x61cb0f &amp;lt;= trans_date &amp;lt;= 0x6fcb0f&quot; ], &quot;index_dives_for_eq_ranges&quot;: true, # 是否使用index dive &quot;rowid_ordered&quot;: false, # 使用该索引获取的记录是否按照主键排序 &quot;using_mrr&quot;: false, # 是否使用mrr &quot;index_only&quot;: true, # 是否是索引覆盖访问 &quot;rows&quot;: 1, # 使用该索引获取的记录条数 &quot;cost&quot;: 1.21, # 使用该索引的成本 &quot;chosen&quot;: true # 是否选择该索引 }, { &quot;index&quot;: &quot;idx_ent_date_region&quot;, &quot;ranges&quot;: [ &quot;1747964630024192400 &amp;lt;= ent_id &amp;lt;= 1747964630024192400 AND 0x61cb0f &amp;lt;= trans_date &amp;lt;= 0x6fcb0f&quot; ], &quot;index_dives_for_eq_ranges&quot;: true, &quot;rowid_ordered&quot;: false, &quot;using_mrr&quot;: false, &quot;index_only&quot;: false, &quot;rows&quot;: 1, &quot;cost&quot;: 2.21, &quot;chosen&quot;: false, &quot;cause&quot;: &quot;cost&quot; # 因为成本太大所以不选择该索引 }, { &quot;index&quot;: &quot;idx_saled_item&quot;, &quot;ranges&quot;: [ &quot;1747964630024192400 &amp;lt;= ent_id &amp;lt;= 1747964630024192400&quot; ], &quot;index_dives_for_eq_ranges&quot;: true, &quot;rowid_ordered&quot;: false, &quot;using_mrr&quot;: false, &quot;index_only&quot;: true, &quot;rows&quot;: 753872, &quot;cost&quot;: 197892, &quot;chosen&quot;: false, &quot;cause&quot;: &quot;cost&quot; } ], &quot;analyzing_roworder_intersect&quot;: { # 分析使用索引合并的成本 &quot;usable&quot;: false, &quot;cause&quot;: &quot;too_few_roworder_scans&quot; } }, &quot;chosen_range_access_summary&quot;: { # 对于上述单表查询最优的访问方法 &quot;range_access_plan&quot;: { &quot;type&quot;: &quot;range_scan&quot;, &quot;index&quot;: &quot;unique_index&quot;, &quot;rows&quot;: 1, &quot;ranges&quot;: [ &quot;0x61cb0f &amp;lt;= trans_date &amp;lt;= 0x6fcb0f&quot; ] }, &quot;rows_for_plan&quot;: 1, &quot;cost_for_plan&quot;: 1.21, &quot;chosen&quot;: true } } } ] }, { # 分析各种可能的执行计划 &quot;considered_execution_plans&quot;: [ { &quot;plan_prefix&quot;: [ ], &quot;table&quot;: &quot;`item_sale_summary`&quot;, &quot;best_access_path&quot;: { &quot;considered_access_paths&quot;: [ { &quot;access_type&quot;: &quot;ref&quot;, &quot;index&quot;: &quot;idx_ent_date_region&quot;, &quot;rows&quot;: 710.55, &quot;cost&quot;: 852.66, &quot;chosen&quot;: true }, { &quot;access_type&quot;: &quot;ref&quot;, &quot;index&quot;: &quot;idx_saled_item&quot;, &quot;rows&quot;: 753872, &quot;cost&quot;: 197892, &quot;chosen&quot;: false }, { &quot;rows_to_scan&quot;: 1, &quot;access_type&quot;: &quot;range&quot;, &quot;range_details&quot;: { &quot;used_index&quot;: &quot;unique_index&quot; }, &quot;resulting_rows&quot;: 0.1776, &quot;cost&quot;: 1.41, &quot;chosen&quot;: true, &quot;use_tmp_table&quot;: true } ] }, &quot;condition_filtering_pct&quot;: 100, &quot;rows_for_plan&quot;: 0.1776, &quot;cost_for_plan&quot;: 1.41, &quot;sort_cost&quot;: 0.1776, &quot;new_cost_for_plan&quot;: 1.5876, &quot;chosen&quot;: true } ] }, { # 尝试给查询添加一些其他的查询条件 &quot;attaching_conditions_to_tables&quot;: { &quot;original_condition&quot;: &quot;((`item_sale_summary`.`ent_id` = 1747964630024192400) and (`item_sale_summary`.`trans_date` between &#39;2021-11-01&#39; and &#39;2021-11-15&#39;))&quot;, &quot;attached_conditions_computation&quot;: [ ], &quot;attached_conditions_summary&quot;: [ { &quot;table&quot;: &quot;`item_sale_summary`&quot;, &quot;attached&quot;: &quot;((`item_sale_summary`.`ent_id` = 1747964630024192400) and (`item_sale_summary`.`trans_date` between &#39;2021-11-01&#39; and &#39;2021-11-15&#39;))&quot; } ] } }, { &quot;clause_processing&quot;: { &quot;clause&quot;: &quot;GROUP BY&quot;, &quot;original_clause&quot;: &quot;`item_sale_summary`.`ent_id`,`item_sale_summary`.`region_code`&quot;, &quot;items&quot;: [ { &quot;item&quot;: &quot;`item_sale_summary`.`ent_id`&quot;, &quot;equals_constant_in_where&quot;: true }, { &quot;item&quot;: &quot;`item_sale_summary`.`region_code`&quot; } ], &quot;resulting_clause_is_simple&quot;: true, &quot;resulting_clause&quot;: &quot;`item_sale_summary`.`region_code`&quot; } }, { &quot;reconsidering_access_paths_for_index_ordering&quot;: { &quot;clause&quot;: &quot;GROUP BY&quot;, &quot;steps&quot;: [ ], &quot;index_order_summary&quot;: { &quot;table&quot;: &quot;`item_sale_summary`&quot;, &quot;index_provides_order&quot;: false, &quot;order_direction&quot;: &quot;undefined&quot;, &quot;index&quot;: &quot;unique_index&quot;, &quot;plan_changed&quot;: false } } }, { # 再稍稍的改进一下执行计划 &quot;refine_plan&quot;: [ { &quot;table&quot;: &quot;`item_sale_summary`&quot; } ] }, { &quot;creating_tmp_table&quot;: { &quot;tmp_table_info&quot;: { &quot;table&quot;: &quot;intermediate_tmp_table&quot;, &quot;row_length&quot;: 344, &quot;key_length&quot;: 349, &quot;unique_constraint&quot;: false, &quot;location&quot;: &quot;memory (heap)&quot;, &quot;row_limit_estimate&quot;: 48770 } } } ] } }, { # execute阶段 &quot;join_execution&quot;: { &quot;select#&quot;: 1, &quot;steps&quot;: [ { &quot;filesort_information&quot;: [ { &quot;direction&quot;: &quot;asc&quot;, &quot;table&quot;: &quot;`item_sale_summary`&quot;, &quot;field&quot;: &quot;region_code&quot; } ], &quot;filesort_priority_queue_optimization&quot;: { &quot;usable&quot;: false, &quot;cause&quot;: &quot;not applicable (no LIMIT)&quot; }, &quot;filesort_execution&quot;: [ ], &quot;filesort_summary&quot;: { &quot;rows&quot;: 0, &quot;examined_rows&quot;: 0, &quot;number_of_tmp_files&quot;: 0, &quot;sort_buffer_size&quot;: 261632, &quot;sort_mode&quot;: &quot;&amp;lt;sort_key, packed_additional_fields&amp;gt;&quot; } } ] } } ]}想要更加具体的了解其中的含义可参考Chapter 8 Tracing the Optimizer4. 总结以上为optimizer trace的简单使用，使用好该功能可以有效帮助我们了解MySQL的优化过程。整体优化过程虽然看起来杂乱，但主要分成了以下三个部分 prepare 阶段 optimize 阶段 execute 阶段的基于成本的优化主要集中在 optimize 阶段，对于单表查询来说，我们主要关注 optimize 阶段的 “rows_estimation” 这个过程，这个过程深入分析了对单表查询的各种执行方案的成本；对于多表连接查询来说，我们更多需要关注 “considered_execution_plans” 这个过程，这个过程里会写明各种不同的连接方式所对应的成本。反正优化器最终会选择成本最低的那种方案来作为最终的执行计划，也就是我们使用 EXPLAIN 语句所展现出的那种方案。5. 参考 《MySQL是怎样运行的》 MySQL · 最佳实践 · 性能分析的大杀器—Optimizer trace Chapter 8 Tracing the Optimizer" }, { "title": "Java 基础 - 注解机制详解", "url": "/posts/Java-%E5%9F%BA%E7%A1%80-%E6%B3%A8%E8%A7%A3%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3/", "categories": "编程语言, java", "tags": "java", "date": "2021-12-16 00:00:00 +0800", "snippet": "1. Java注解概述注解是JDK1.5版本开始引入的一个特性，用于对代码进行说明，可以对包、类、接口、字段、方法参数、局部变量等进行注解。它主要的作用有以下四方面： 生成文档，通过代码里标识的元数据生成javadoc文档。 编译检查，通过代码里标识的元数据让编译器在编译期间进行检查验证。 编译时动态处理，编译时通过代码里标识的元数据动态处理，例如动态生成代码。 运行时动态处理，运行时通过代码里标识的元数据动态处理，例如使用反射注入实例。常见注解分类： Java自带的标准注解，包括@Override、@Deprecated和@SuppressWarnings，分别用于标明重写某个方法、标明某个类或方法过时、标明要忽略的警告，用这些注解标明后编译器就会进行检查。 元注解，元注解是用于定义注解的注解，包括@Retention、@Target、@Inherited、@Documented，@Retention用于标明注解被保留的阶段，@Target用于标明注解使用的范围，@Inherited用于标明注解可继承，@Documented用于标明是否生成javadoc文档。 自定义注解，可以根据自己的需求定义注解，并可用元注解对自定义注解进行注解。 2. 内置注解&amp;amp;元注解2.1 内置注解2.1 .1 内置注解 - @Override代码@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)public @interface Override {}作用这个注解可以被用来修饰方法，并且它只在编译时有效，在编译后的class文件中便不再存在。这个注解的作用并不陌生，那就是告诉编译器被修饰的方法是重写的父类的中的相同签名的方法，编译器会对此做出检查，若发现父类中不存在这个方法或是存在的方法签名不同，则会报错2.1.2 内置注解 - @Override代码@Documented@Retention(RetentionPolicy.RUNTIME)@Target(value={CONSTRUCTOR, FIELD, LOCAL_VARIABLE, METHOD, PACKAGE, PARAMETER, TYPE})public @interface Deprecated {}作用从它的定义我们可以知道，它会被文档化，能够保留到运行时，能够修饰构造方法、属性、局部变量、方法、包、参数、类型。这个注解的作用是告诉编译器被修饰的程序元素已被“废弃”，不再建议用户使用2.1.3 内置注解 - @SuppressWarnings@Target({TYPE, FIELD, METHOD, PARAMETER, CONSTRUCTOR, LOCAL_VARIABLE})@Retention(RetentionPolicy.SOURCE)public @interface SuppressWarnings { String[] value();}它能够修饰的程序元素包括类型、属性、方法、参数、构造器、局部变量，只能存活在源码时，取值为String[]。它的作用是告诉编译器忽略指定的警告信息，它可以取的值如下所示 参数 作用 原描述 all 抑制所有警告 to suppress all warnings boxing 抑制装箱、拆箱操作时候的警告 to suppress warnings relative to boxing/unboxing operations cast 抑制映射相关的警告 to suppress warnings relative to cast operations dep-ann 抑制启用注释的警告 to suppress warnings relative to deprecated annotation deprecation 抑制过期方法警告 to suppress warnings relative to deprecation fallthrough 抑制确在switch中缺失breaks的警告 to suppress warnings relative to missing breaks in switch statements finally 抑制finally模块没有返回的警告 to suppress warnings relative to finally block that don’t return hiding 抑制与隐藏变数的区域变数相关的警告 to suppress warnings relative to locals that hide variable（） incomplete-switch 忽略没有完整的switch语句 to suppress warnings relative to missing entries in a switch statement (enum case) nls 忽略非nls格式的字符 to suppress warnings relative to non-nls string literals null 忽略对null的操作 to suppress warnings relative to null analysis rawtype 使用generics时忽略没有指定相应的类型 to suppress warnings relative to un-specific types when using restriction 抑制与使用不建议或禁止参照相关的警告 to suppress warnings relative to usage of discouraged or serial 忽略在serializable类中没有声明serialVersionUID变量 to suppress warnings relative to missing serialVersionUID field for a serializable class static-access 抑制不正确的静态访问方式警告 to suppress warnings relative to incorrect static access synthetic-access 抑制子类没有按最优方法访问内部类的警告 to suppress warnings relative to unoptimized access from inner classes unchecked 抑制没有进行类型检查操作的警告 to suppress warnings relative to unchecked operations unqualified-field-access 抑制没有权限访问的域的警告 to suppress warnings relative to field access unqualified unused 抑制没被使用过的代码的警告 to suppress warnings relative to unused code 2.2 元注解 在JDK 1.5中提供了4个标准的元注解：@Target，@Retention，@Documented，@Inherited, 在JDK 1.8中提供了两个元注解 @Repeatable和@Native2.2.1 元注解 - @TargetTarget注解的作用是：描述注解的使用范围（即：被修饰的注解可以用在什么地方） 。Target注解用来说明那些被它所注解的注解类可修饰的对象范围：注解可以用于修饰 packages、types（类、接口、枚举、注解类）、类成员（方法、构造方法、成员变量、枚举值）、方法参数和本地变量（如循环变量、catch参数），在定义注解类时使用了@Target 能够更加清晰的知道它能够被用来修饰哪些对象，它的取值范围定义在ElementType 枚举中。public enum ElementType { TYPE, // 类、接口、枚举类 FIELD, // 成员变量（包括：枚举常量） METHOD, // 成员方法 PARAMETER, // 方法参数 CONSTRUCTOR, // 构造方法 LOCAL_VARIABLE, // 局部变量 ANNOTATION_TYPE, // 注解类 PACKAGE, // 可用于修饰：包 TYPE_PARAMETER, // 类型参数，JDK 1.8 新增 TYPE_USE // 使用类型的任何地方，JDK 1.8 新增}2.2.2 元注解 - @Retention &amp;amp; @RetentionTargetReteniton注解的作用是：描述注解保留的时间范围（即：被描述的注解在它所修饰的类中可以被保留到何时） 。Reteniton注解用来限定那些被它所注解的注解类在注解到其他类上以后，可被保留到何时，一共有三种策略，定义在RetentionPolicy枚举中。public enum RetentionPolicy { SOURCE, // 源文件保留 CLASS, // 编译期保留，默认值 RUNTIME // 运行期保留，可通过反射去获取注解信息}2.2.3 元注解 - @DocumentedDocumented注解的作用是：描述在使用 javadoc 工具为类生成帮助文档时是否要保留其注解信息。2.2.4 元注解 - @InheritedInherited注解的作用：被它修饰的Annotation将具有继承性。如果某个类使用了被@Inherited修饰的Annotation，则其子类将自动具有该注解。这个注解在Spring中特别常见，例如启动类的@SpringBootApplication就可以看到2.2.5 元注解 - @Repeatable (Java8)重复注解的可读性更强JDK8之前public @interface Authority { String role();}public @interface Authorities { Authority[] value();}public class RepeatAnnotationUseOldVersion { @Authorities({@Authority(role=&quot;Admin&quot;),@Authority(role=&quot;Manager&quot;)}) public void doSomeThing(){ }}JDK8之后@Repeatable(Authorities.class)public @interface Authority { String role();}public @interface Authorities { Authority[] value();}public class RepeatAnnotationUseNewVersion { @Authority(role=&quot;Admin&quot;) @Authority(role=&quot;Manager&quot;) public void doSomeThing(){ }}不同的地方是，创建重复注解Authority时，加上@Repeatable,指向存储注解Authorities，在使用时候，直接可以重复使用Authority注解。3. 自定义注解3.1 注解与反射接口​ 在定义注解以后，在程序执行时得想办法拿到注解。反射包java.lang.reflect下的AnnotatedElement接口提供这些方法。这里注意：只有注解被定义为RUNTIME后，该注解才能是运行时可见，当class文件被装载时被保存在class文件中的Annotation才会被虚拟机读取​ AnnotatedElement 接口是反射相关类（Class、Method和Constructor）的父接口，所以程序通过反射获取了某个类的AnnotatedElement对象之后，程序就可以调用该对象的方法来访问Annotation信息。我们看下具体的先关接口 boolean isAnnotationPresent(Class&amp;lt;?extends Annotation&amp;gt; annotationClass)​判断该程序元素上是否包含指定类型的注解，存在则返回true，否则返回false。注意：此方法会忽略注解对应的注解容器。 &amp;lt;T extends Annotation&amp;gt; T getAnnotation(Class&amp;lt;T&amp;gt; annotationClass)​返回该程序元素上存在的、指定类型的注解，如果该类型注解不存在，则返回null。 Annotation[] getAnnotations()​返回该程序元素上存在的所有注解，若没有注解，返回长度为0的数组。 &amp;lt;T extends Annotation&amp;gt; T[] getAnnotationsByType(Class&amp;lt;T&amp;gt; annotationClass)​返回该程序元素上存在的、指定类型的注解数组。没有注解对应类型的注解时，返回长度为0的数组。该方法的调用者可以随意修改返回的数组，而不会对其他调用者返回的数组产生任何影响。getAnnotationsByType方法与 getAnnotation的区别在于，getAnnotationsByType会检测注解对应的重复注解容器。若程序元素为类，当前类上找不到注解，且该注解为可继承的，则会去父类上检测对应的注解。 &amp;lt;T extends Annotation&amp;gt; T getDeclaredAnnotation(Class&amp;lt;T&amp;gt; annotationClass)​返回直接存在于此元素上的所有注解。与此接口中的其他方法不同，该方法将忽略继承的注释。如果没有注释直接存在于此元素上，则返回null &amp;lt;T extends Annotation&amp;gt; T[] getDeclaredAnnotationsByType(Class&amp;lt;T&amp;gt; annotationClass)​返回直接存在于此元素上的所有注解。与此接口中的其他方法不同，该方法将忽略继承的注释 Annotation[] getDeclaredAnnotations()​返回直接存在于此元素上的所有注解及注解对应的重复注解容器。与此接口中的其他方法不同，该方法将忽略继承的注解。如果没有注释直接存在于此元素上，则返回长度为零的一个数组。该方法的调用者可以随意修改返回的数组，而不会对其他调用者返回的数组产生任何影响3.2 自定义注解的使用定义注解@Target(ElementType.METHOD) //使用范围：方法@Retention(RetentionPolicy.RUNTIME) // 生命周期：运行中@interface MyMethodAnnotation { public String title() default &quot;默认标题&quot;; public String description() default &quot;默认描述&quot;;}注解内容获取// 获取对应类的所有方法Method[] methods = AnnotationAndReflection.class.getMethods();for (Method method : methods) { // 判断是否含有指定注解 if (method.isAnnotationPresent(MyMethodAnnotation.class)){ // 获取所有注解 Annotation[] annotations = method.getAnnotations(); for (Annotation annotation : annotations) { System.out.println(annotation); } // 获取指定注解 MyMethodAnnotation annotation = method.getAnnotation(MyMethodAnnotation.class); }}4. 注解的使用场景Spring框架 配置话到注解化Junit3-&amp;gt;Junit4 从继承实现到注解实现被测试类public class HelloWorld { public void sayHello(){ System.out.println(&quot;hello....&quot;); throw new NumberFormatException(); } public void sayWorld(){ System.out.println(&quot;world....&quot;); } public String say(){ return &quot;hello world!&quot;; }}Junit3 实现通过继承 TestCase来实现，初始化是通过Override父类方法来进行，测试方式通过test的前缀方法获取。public class HelloWorldTest extends TestCase{ private HelloWorld hw; @Override protected void setUp() throws Exception { super.setUp(); hw=new HelloWorld(); } //1.测试没有返回值 public void testHello(){ try { hw.sayHello(); } catch (Exception e) { System.out.println(&quot;发生异常.....&quot;); } } public void testWorld(){ hw.sayWorld(); } //2.测试有返回值的方法 // 返回字符串 public void testSay(){ assertEquals(&quot;测试失败&quot;, hw.say(), &quot;hello world!&quot;); } //返回对象 public void testObj(){ assertNull(&quot;测试对象不为空&quot;, null); assertNotNull(&quot;测试对象为空&quot;,new String()); } @Override protected void tearDown() throws Exception { super.tearDown(); hw=null; } }Junit 4 实现UT通过定义@Before，@Test，@After等等注解来实现。public class HelloWorldTest { private HelloWorld hw; @Before public void setUp() { hw = new HelloWorld(); } @Test(expected=NumberFormatException.class) // 1.测试没有返回值,有别于junit3的使用，更加方便 public void testHello() { hw.sayHello(); } @Test public void testWorld() { hw.sayWorld(); } @Test // 2.测试有返回值的方法 // 返回字符串 public void testSay() { assertEquals(&quot;测试失败&quot;, hw.say(), &quot;hello world!&quot;); } @Test // 返回对象 public void testObj() { assertNull(&quot;测试对象不为空&quot;, null); assertNotNull(&quot;测试对象为空&quot;, new String()); } @After public void tearDown() throws Exception { hw = null; }}参考 Java 自定义注解及使用场景 Java 基础 - 注解机制详解 java注解的本质以及注解的底层实现原理 Java注解处理器 JUnit4源码分析运行原理" }, { "title": "MySQL - 事件调度器（Event Schedule）", "url": "/posts/MySQL-%E4%BA%8B%E4%BB%B6%E8%B0%83%E5%BA%A6%E5%99%A8/", "categories": "数据库, mysql", "tags": "mysql", "date": "2021-10-25 00:00:00 +0800", "snippet": "1.概述​ 事件调度器（Event Schedule）类似于Linux中的crontab（也就是定时任务），下面介绍事件调度器的基本使用方法2.使用2.1 查看事件调度器状态以及相关操作# 查看状态show variables like &#39;%event_scheduler%&#39;;+---------------+-----+|Variable_name |Value|+---------------+-----+|event_scheduler|ON |+---------------+-----+# 开启SET GLOBAL event_scheduler = 1;# 关闭SET GLOBAL event_scheduler = 0;注意：　　如果是设定事件计划为0 或OFF，即关闭事件计划进程的时候，不会有新的事件执行，但现有的正在运行的事件会执行到完毕。　　对于线上环境来说，使用even时，注意在主库上开启定时器，从库上关闭定时器，event触发所有操作均会记录binlog进行主从同步，从库上开启定时器很可能造成卡库。切换主库后之后记得将新主库上的定时器打开。2.2 事件的创建与查看创建语句CREATE [DEFINER = user] EVENT [IF NOT EXISTS] event_name ON SCHEDULE schedule [ON COMPLETION [NOT] PRESERVE] [ENABLE | DISABLE | DISABLE ON SLAVE] [COMMENT &#39;string&#39;] DO event_body;schedule: { AT timestamp [+ INTERVAL interval] ... | EVERY interval [STARTS timestamp [+ INTERVAL interval] ...] [ENDS timestamp [+ INTERVAL interval] ...]}interval: quantity {YEAR | QUARTER | MONTH | DAY | HOUR | MINUTE | WEEK | SECOND | YEAR_MONTH | DAY_HOUR | DAY_MINUTE | DAY_SECOND | HOUR_MINUTE | HOUR_SECOND | MINUTE_SECOND} DEFINER:指明该event的用户，服务器在执行该事件时，使用该用户来检查权限。 默认用户为当前用户，即definer = current_user 如果明确指明了definer，则必须遵循如下规则： 如果没有super权限，唯一允许的值就是自己当前用户，而不能设置为其他用户 如果具有super权限，则可以指定任意存在的用户；如果指定的用户不存在，则事件在执行时会报错 ON SCHEDULE:指定何时执行该事件，以及如何执行该事件 AT timestamp:用于创建单次执行的事件，timestamp执行事件执行的时间(如果指定的时间是过去的时间，则会产生一个warning),时间可以是具体的时间字符串或者是一个datetime类型的表达式(如current_timestamp) 如果要指定将来某个时间，直接使用at timestamp，例：at ‘2017-08-08 08:08:08’； 如果要指定将来某个时间间隔，可利用interval关键字(interval关键字可以进行组合) EVERY interval:用于创建重复执行的事件。例如，每分钟执行一次，则可以：EVERY 1 MINUTE。当然，every子句可以指定一个开始事件和结束时间，通过STARTS和ENDS关键字来表示，具体语法与前面类似 通常情况下，如果一个事件过期已过期，则会被立即删除。但是，create event定义中通过on completion preserve子句可以保留已过期的时间。默认：ON COMPLETION NOT PRESERVE，也就是不保存 默认情况下，enable on slave，事件一旦创建后就立即开始执行，可以通过disable关键字来禁用该事件 DO子句用于指示事件需要执行的操作，可以是一条SQL语句，也可以是被begin…end包括的语句块，也可以在语句块中调用存储过程示例，创建一个每10秒插入一条数据的任务CREATE EVENT sc1 ON SCHEDULE EVERY 10 SECOND DO BEGIN INSERT INTO scheduler_test1(time) VALUE (now()); END;事件查看# 查看所有事件SELECT * FROM information_schema.EVENTS;SHOW EVENTS ;2.3 事件的修改与删除修改ALTER [DEFINER = user] EVENT event_name [ON SCHEDULE schedule] [ON COMPLETION [NOT] PRESERVE] [RENAME TO new_event_name] [ENABLE | DISABLE | DISABLE ON SLAVE] [COMMENT &#39;string&#39;] [DO event_body]更多的详情可参考13.1.2 ALTER EVENT Statement删除DROP EVENT [IF EXISTS] event_name;3. 参考 MySQL事件调度器event的使用 Using the Event Scheduler" }, { "title": "MySQL - 执行计划", "url": "/posts/MySQL-%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92/", "categories": "数据库, mysql", "tags": "mysql", "date": "2021-10-18 00:00:00 +0800", "snippet": "1.概述​ 一条查询语句在经过MySQL查询优化器的各种基于成本和规则的优化会后生成一个所谓的执行计划，这个执行计划展示了接下来具体执行查询的方式，比如多表连接的顺序是什么，对于每个表采用什么访问方法来具体执行查询等等。​ 如果需要查询具体的执行计划，可通过在查询语句前追加EXPLAIN进行查看，例如：flink_data_qnh&amp;gt; EXPLAIN SELECT 1+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+--------------+|id|select_type|table|partitions|type|possible_keys|key |key_len|ref |rows|filtered|Extra |+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+--------------+|1 |SIMPLE |NULL |NULL |NULL|NULL |NULL|NULL |NULL|NULL|NULL |No tables used|+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+--------------+​ 除了以SELECT开头的查询语句，其余的DELETE、INSERT、REPLACE以及UPDATE语句前边都可以加上EXPLAIN这个词儿，用来查看这些语句的执行计划，不过这里只对SELECT进行解释 列名 描述 id 在一个大的查询语句中每个SELECT关键字都对应一个唯一的id select_type SELECT关键字对应的那个查询的类型 table 表名 partitions 匹配的分区信息 type 针对单表的访问方法 possible_keys 可能用到的索引 key 实际上使用的索引 key_len 实际使用到的索引长度 ref 当使用索引列等值查询时，与索引列进行等值匹配的对象信息 rows 预估的需要读取的记录条数 filtered 某个表经过搜索条件过滤后剩余记录条数的百分比 Extra 一些额外的信息 2. 执行计划输出中各列详解执行计划使用的表结构#两张一模一样表结构的表，s1,s2CREATE TABLE single_table( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part (key_part1, key_part2, key_part3)) Engine = InnoDB CHARSET = utf8;2.1 table不论我们的查询语句有多复杂，里边儿包含了多少个表，到最后也是需要对每个表进行单表访问的，所以规定EXPLAIN语句输出的每条记录都对应着某个单表的访问方法，该条记录的table列代表着该表的表名2.2 id每一个被查询优化后的查询对应一个id值，即有些查询语句存在子查询，但是被查询优化器转换成了连接查询，那么仍然视作一个查询# 连接查询&amp;gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2;+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+-----------------------------+|id|select_type|table|partitions|type|possible_keys|key |key_len|ref |rows|filtered|Extra |+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+-----------------------------+|1 |SIMPLE |s1 |NULL |ALL |NULL |NULL|NULL |NULL|9827|100 |NULL ||1 |SIMPLE |s2 |NULL |ALL |NULL |NULL|NULL |NULL|9827|100 |Using join buffer (hash join)|+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+-----------------------------+# 子查询&amp;gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = &#39;a&#39;;+--+-----------+-----+----------+-----+-------------+--------+-------+----+----+--------+-----------+|id|select_type|table|partitions|type |possible_keys|key |key_len|ref |rows|filtered|Extra |+--+-----------+-----+----------+-----+-------------+--------+-------+----+----+--------+-----------+|1 |PRIMARY |s1 |NULL |ALL |idx_key3 |NULL |NULL |NULL|9827|100 |Using where||2 |SUBQUERY |s2 |NULL |index|idx_key1 |idx_key1|303 |NULL|9827|100 |Using index|+--+-----------+-----+----------+-----+-------------+--------+-------+----+----+--------+-----------+2.3 select_type每条SQL语句可能包含多个查询，select_type就是定义每个查询的类型 SIMPLE 查询语句中不包含UNION或者子查询的查询都算作是SIMPLE类型，比方说下边这个单表查询的select_type的值就是SIMPLE PRIMARY 对于包含UNION、UNION ALL或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个查询的select_type值就是PRIMARY &amp;gt; EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2;+----+------------+----------+----------+----+-------------+----+-------+----+----+--------+---------------+|id |select_type |table |partitions|type|possible_keys|key |key_len|ref |rows|filtered|Extra |+----+------------+----------+----------+----+-------------+----+-------+----+----+--------+---------------+|1 |PRIMARY |s1 |NULL |ALL |NULL |NULL|NULL |NULL|9827|100 |NULL ||2 |UNION |s2 |NULL |ALL |NULL |NULL|NULL |NULL|9827|100 |NULL ||NULL|UNION RESULT|&amp;lt;union1,2&amp;gt;|NULL |ALL |NULL |NULL|NULL |NULL|NULL|NULL |Using temporary|+----+------------+----------+----------+----+-------------+----+-------+----+----+--------+---------------+ UNION 对于包含UNION或者UNION ALL的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的select_type值就是UNION，可以对比上一个例子的效果 UNION RESULT MySQL选择使用临时表来完成UNION查询的去重工作，针对该临时表的查询的select_type就是UNIONRESULT，例子上边有，就不过多赘述 SUBQUERY 如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是不相关子查询，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个SELECT关键字代表的那个查询的select_type就是SUBQUERY &amp;gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = &#39;a&#39;; +--+-----------+-----+----------+-----+-------------+--------+-------+----+----+--------+-----------+|id|select_type|table|partitions|type |possible_keys|key |key_len|ref |rows|filtered|Extra |+--+-----------+-----+----------+-----+-------------+--------+-------+----+----+--------+-----------+|1 |PRIMARY |s1 |NULL |ALL |idx_key3 |NULL |NULL |NULL|9827|100 |Using where||2 |SUBQUERY |s2 |NULL |index|idx_key1 |idx_key1|303 |NULL|9827|100 |Using index|+--+-----------+-----+----------+-----+-------------+--------+-------+----+----+--------+-----------+ DEPENDENT SUBQUERY 如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是相关子查询，则该子查询的第一个SELECT关键字代表的那个查询的select_type就是DEPENDENT SUBQUERY &amp;gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE s1.key2 = s2.key2) OR key3 = &#39;a&#39;;+--+------------------+-----+----------+------+-----------------+--------+-------+------------+----+--------+-----------+|id|select_type |table|partitions|type |possible_keys |key |key_len|ref |rows|filtered|Extra |+--+------------------+-----+----------+------+-----------------+--------+-------+------------+----+--------+-----------+|1 |PRIMARY |s1 |NULL |ALL |idx_key3 |NULL |NULL |NULL |9827|100 |Using where||2 |DEPENDENT SUBQUERY|s2 |NULL |eq_ref|idx_key2,idx_key1|idx_key2|5 |test.s1.key2|1 |10 |Using where|+--+------------------+-----+----------+------+-----------------+--------+-------+------------+----+--------+-----------+ select_type为DEPENDENT SUBQUERY的查询可能会被执行多次 DEPENDENT UNION 在包含UNION或者UNION ALL的大查询中，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小查询之外，其余的小查询的select_type的值就是DEPENDENT UNION &amp;gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE key1 = &#39;a&#39;UNION SELECT key1 FROM s1 WHERE key1 = &#39;b&#39;);+----+------------------+----------+----------+----+-------------+--------+-------+-----+----+--------+------------------------+|id |select_type |table |partitions|type|possible_keys|key |key_len|ref |rows|filtered|Extra |+----+------------------+----------+----------+----+-------------+--------+-------+-----+----+--------+------------------------+|1 |PRIMARY |s1 |NULL |ALL |NULL |NULL |NULL |NULL |9827|100 |Using where ||2 |DEPENDENT SUBQUERY|s2 |NULL |ref |idx_key1 |idx_key1|303 |const|1 |100 |Using where; Using index||3 |DEPENDENT UNION |s1 |NULL |ref |idx_key1 |idx_key1|303 |const|1 |100 |Using where; Using index||NULL|UNION RESULT |&amp;lt;union2,3&amp;gt;|NULL |ALL |NULL |NULL |NULL |NULL |NULL|NULL |Using temporary |+----+------------------+----------+----------+----+-------------+--------+-------+-----+----+--------+------------------------+ 从执行计划中可以看出来，SELECT key1 FROM s2 WHERE key1 = &#39;a&#39;这个小查询由于是子查询中第一个查询，所以它的select_type是DEPENDENT SUBQUERY，而SELECT key1 FROM s1 WHERE key1 = &#39;b&#39;这个查询的select_type就是DEPENDENT UNION。 DERIVED 对于采用物化的方式执行的包含派生表的查询，该派生表对应的子查询的select_type就是DERIVED &amp;gt; EXPLAIN SELECT * FROM (SELECT key1, count(*) as c FROM s1 GROUP BY key1) AS derived_s1 where c &amp;gt; 1;+--+-----------+----------+----------+-----+-------------+--------+-------+----+----+--------+-----------+|id|select_type|table |partitions|type |possible_keys|key |key_len|ref |rows|filtered|Extra |+--+-----------+----------+----------+-----+-------------+--------+-------+----+----+--------+-----------+|1 |PRIMARY |&amp;lt;derived2&amp;gt;|NULL |ALL |NULL |NULL |NULL |NULL|9827|100 |NULL ||2 |DERIVED |s1 |NULL |index|idx_key1 |idx_key1|303 |NULL|9827|100 |Using index|+--+-----------+----------+----------+-----+-------------+--------+-------+----+----+--------+-----------+ 从执行计划中可以看出，id为2的记录就代表子查询的执行方式，它的select_type是DERIVED，说明该子查询是以物化的方式执行的。id为1的记录代表外层查询，大家注意看它的table列显示的是，表示该查询是针对将派生表物化之后的表进行查询的。 MATERIALIZED 当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询对应的select_type属性就是MATERIALIZED &amp;gt; EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2);+--+------------+-----------+----------+------+-------------------+-------------------+-------+------------+----+--------+-----------+|id|select_type |table |partitions|type |possible_keys |key |key_len|ref |rows|filtered|Extra |+--+------------+-----------+----------+------+-------------------+-------------------+-------+------------+----+--------+-----------+|1 |SIMPLE |s1 |NULL |ALL |idx_key1 |NULL |NULL |NULL |9827|100 |Using where||1 |SIMPLE |&amp;lt;subquery2&amp;gt;|NULL |eq_ref|&amp;lt;auto_distinct_key&amp;gt;|&amp;lt;auto_distinct_key&amp;gt;|303 |test.s1.key1|1 |100 |NULL ||2 |MATERIALIZED|s2 |NULL |index |idx_key1 |idx_key1 |303 |NULL |9827|100 |Using index|+--+------------+-----------+----------+------+-------------------+-------------------+-------+------------+----+--------+-----------+ 执行计划的第三条记录的id值为2，说明该条记录对应的是一个单表查询，从它的select_type值为MATERIALIZED可以看出，查询优化器是要把子查询先转换成物化表。然后看执行计划的前两条记录的id值都为1，说明这两条记录对应的表进行连接查询，需要注意的是第二条记录的table列的值是，说明该表其实就是id为2对应的子查询执行之后产生的物化表，然后将s1和该物化表进行连接查询。 2.4 partitionsSQL语句执行会走了哪几个分区2.5 typetype列就表明了这个访问方法是个什么类型，在索引的访问方式那一章有做解释2.6 possible_keys和key对某个表执行单表查询时可能用到的索引有哪些，key列表示实际用到的索引有哪些&amp;gt;EXPLAIN SELECT * FROM s1 WHERE key1 &amp;gt; &#39;z&#39; AND key3 = &#39;a&#39;;+--+-----------+-----+----------+----+-----------------+--------+-------+-----+----+--------+-----------+|id|select_type|table|partitions|type|possible_keys |key |key_len|ref |rows|filtered|Extra |+--+-----------+-----+----------+----+-----------------+--------+-------+-----+----+--------+-----------+|1 |SIMPLE |s1 |NULL |ref |idx_key1,idx_key3|idx_key3|303 |const|1 |5 |Using where|+--+-----------+-----+----------+----+-----------------+--------+-------+-----+----+--------+-----------+上述执行计划的possible_keys列的值是idx_key1,idx_key3，表示该查询可能使用到idx_key1,idx_key3两个索引，然后key列的值是idx_key3，表示经过查询优化器计算使用不同索引的成本后，最后决定使用idx_key3来执行查询另外需要注意的一点是，possible_keys列中的值并不是越多越好，可能使用的索引越多，查询优化器计算查询成本时就得花费更长时间，所以如果可以的话，尽量删除那些用不到的索引2.7 key_lenkey_len列表示当优化器决定使用某个索引执行查询时，该索引记录的最大长度，它是由这三个部分构成的： 对于使用固定长度类型的索引列来说，它实际占用的存储空间的最大长度就是该固定值，对于指定字符集的变长类型的索引列来说，比如某个索引列的类型是VARCHAR(100)，使用的字符集是utf8，那么该列实际占用的最大存储空间就是100 × 3 = 300个字节。 如果该索引列可以存储NULL值，则key_len比不可以存储NULL值时多1个字节。 对于变长字段来说，都会有2个字节的空间来存储该变长列的实际长度。&amp;gt; EXPLAIN SELECT * FROM s1 WHERE id = 5;+--+-----------+-----+----------+-----+-------------+-------+-------+-----+----+--------+-----+|id|select_type|table|partitions|type |possible_keys|key |key_len|ref |rows|filtered|Extra|+--+-----------+-----+----------+-----+-------------+-------+-------+-----+----+--------+-----+|1 |SIMPLE |s1 |NULL |const|PRIMARY |PRIMARY|4 |const|1 |100 |NULL |+--+-----------+-----+----------+-----+-------------+-------+-------+-----+----+--------+-----+由于id列的类型是INT，并且不可以存储NULL值，所以在使用该列的索引时key_len大小就是42.8 ref当使用索引列等值匹配的条件去执行查询时，也就是在访问方法是const、eq_ref、ref、ref_or_null、unique_subquery、index_subquery其中之一时，ref列展示的就是与索引列作等值匹配具体的值&amp;gt; EXPLAIN SELECT * FROM s1 WHERE key1 = &#39;a&#39;;+--+-----------+-----+----------+----+-------------+--------+-------+-----+----+--------+-----+|id|select_type|table|partitions|type|possible_keys|key |key_len|ref |rows|filtered|Extra|+--+-----------+-----+----------+----+-------------+--------+-------+-----+----+--------+-----+|1 |SIMPLE |s1 |NULL |ref |idx_key1 |idx_key1|303 |const|1 |100 |NULL |+--+-----------+-----+----------+----+-------------+--------+-------+-----+----+--------+-----+const标识匹配的是一个常量&amp;gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.id = s2.id;+--+-----------+-----+----------+------+-------------+-------+-------+----------+----+--------+-----+|id|select_type|table|partitions|type |possible_keys|key |key_len|ref |rows|filtered|Extra|+--+-----------+-----+----------+------+-------------+-------+-------+----------+----+--------+-----+|1 |SIMPLE |s2 |NULL |ALL |PRIMARY |NULL |NULL |NULL |9827|100 |NULL ||1 |SIMPLE |s1 |NULL |eq_ref|PRIMARY |PRIMARY|4 |test.s2.id|1 |100 |NULL |+--+-----------+-----+----------+------+-------------+-------+-------+----------+----+--------+-----+可以看到对被驱动表s2的访问方法是eq_ref，而对应的ref列的值是xiaohaizi.s1.id，这说明在对被驱动表进行访问时会用到PRIMARY索引，也就是聚簇索引与一个列进行等值匹配的条件，于s2表的id作等值匹配的对象就是xiaohaizi.s1.id列2.9 rows如果查询优化器决定使用全表扫描的方式对某个表执行查询时，执行计划的rows列就代表预计需要扫描的行数，如果使用索引来执行查询时，执行计划的rows列就代表预计扫描的索引记录行数&amp;gt; EXPLAIN SELECT * FROM s1 WHERE key1 &amp;gt; &#39;blue&#39;+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+-----------+|id|select_type|table|partitions|type|possible_keys|key |key_len|ref |rows|filtered|Extra |+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+-----------+|1 |SIMPLE |s1 |NULL |ALL |idx_key1 |NULL|NULL |NULL|9827|49.99 |Using where|+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+-----------+我们看到执行计划的rows列的值是1，这意味着查询优化器在经过分析使用idx_key1进行查询的成本之后，觉得满足key1 &amp;gt; ‘z’这个条件的记录只有1条。2.10 filtered 如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要估计出满足搜索条件的记录到底有多少条。 如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要估计出满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。&amp;gt; EXPLAIN SELECT * FROM s1 WHERE key1 &amp;gt; &#39;blue&#39; AND common_field = &#39;a&#39;;+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+-----------+|id|select_type|table|partitions|type|possible_keys|key |key_len|ref |rows|filtered|Extra |+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+-----------+|1 |SIMPLE |s1 |NULL |ALL |idx_key1 |NULL|NULL |NULL|9827|5 |Using where|+--+-----------+-----+----------+----+-------------+----+-------+----+----+--------+-----------+从执行计划的key列中可以看出来，该查询使用idx_key1索引来执行查询，从rows列可以看出满足key1 &amp;gt;’blue’的记录有9827条。执行计划的filtered列就代表查询优化器预测在这9827条记录中，有多少条记录满足其余的搜索条件，也就是common_field = ‘a’这个条件的百分比。此处filtered列的值是5.00，说明查询优化器预测在9827条记录中有5%的记录满足common_field = ‘a’这个条件。对于单表查询来说，这个filtered列的值没什么意义，我们更关注在连接查询中驱动表对应的执行计划记录的filtered值，比方说下边这个查询：&amp;gt; EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.common_field =&#39;a&#39;;+--+-----------+-----+----------+----+-------------+--------+-------+------------+----+--------+-----------+|id|select_type|table|partitions|type|possible_keys|key |key_len|ref |rows|filtered|Extra |+--+-----------+-----+----------+----+-------------+--------+-------+------------+----+--------+-----------+|1 |SIMPLE |s1 |NULL |ALL |idx_key1 |NULL |NULL |NULL |9827|10 |Using where||1 |SIMPLE |s2 |NULL |ref |idx_key1 |idx_key1|303 |test.s1.key1|317 |100 |NULL |+--+-----------+-----+----------+----+-------------+--------+-------+------------+----+--------+-----------+从执行计划中可以看出来，查询优化器打算把s1当作驱动表，s2当作被驱动表。我们可以看到驱动表s1表的执行计划的rows列为9827，filtered列为10.00，这意味着驱动表s1的扇出值就是9827× 10.00% =982.7，这说明还要对被驱动表执行大约982次查询。2.11 Extra No tables used：当查询语句的没有FROM子句时将会提示该额外信息 Impossible WHERE：查询语句的WHERE子句永远为FALSE时将会提示该额外信息 No matching min/max row：当查询列表处有MIN或者MAX聚集函数，但是并没有符合WHERE子句中的搜索条件的记录时，将会提示该额外信息 Using index：当我们的查询列表以及搜索条件中只包含属于某个索引的列，也就是在可以使用索引覆盖的情况下，在Extra列将会提示该额外信息 Using index condition：有些搜索条件中虽然出现了索引列，但却不能使用到索引 Using where：当我们使用全表扫描来执行对某个表的查询，并且该语句的WHERE子句中有针对该表的搜索条件时，在Extra列中会提示上述额外信息 Using join buffer (Block Nested Loop)：在连接查询执行过程中，当被驱动表不能有效的利用索引加快访问速度，MySQL一般会为其分配一块名叫join buffer的内存块来加快查询速度，也就是我们所讲的基于块的嵌套循环算法 Not exists：当我们使用左（外）连接时，如果WHERE子句中包含要求被驱动表的某个列等于NULL值的搜索条件，而且那个列又是不允许存储NULL值的，那么在该表的执行计划的Extra列就会提示Not exists额外信息 Using intersect(…)、Using union(…)和Using sort_union(…)：如果执行计划的Extra列出现了Using intersect(…)提示，说明准备使用Intersect索引合并的方式执行查询，括号中的…表示需要进行索引合并的索引名称；如果出现了Using union(…)提示，说明准备使用Union索引合并的方式执行查询；出现了Using sort_union(…)提示，说明准备使用Sort-Union索引合并的方式执行查询 Zero limit：当我们的LIMIT子句的参数为0时，表示压根儿不打算从表中读出任何记录，将会提示该额外信息 Using filesort：有一些情况下对结果集中的记录进行排序是可以使用到索引的 Using temporary：在许多查询的执行过程中，MySQL可能会借助临时表来完成一些功能，比如去重、排序之类的，比如我们在执行许多包含DISTINCT、GROUP BY、UNION等子句的查询过程中，如果不能有效利用索引来完成查询，MySQL很有可能寻求通过建立内部的临时表来执行查询。如果查询中使用到了内部的临时表，在执行计划的Extra列将会显示Using temporary提示3. Json格式的执行计划上述的EXPLAIN语句输出中缺少了衡量执行计划好坏的重要执行成本属性，通过使用JSON可以查询到执行计划所花费的成本 在EXPLAIN单词和真正的查询语句中间加上FORMAT=JSON。4.总结​ 以上介绍了EXPLAIN中各个字段的含义，通过EXPLAIN可以查询出可以有效的帮助我们了解SQL脚本的执行情况。参考 《MySQL是怎么样运行的》" }, { "title": "MySQL索引与MongoDB索引的区别", "url": "/posts/MySQL%E7%B4%A2%E5%BC%95%E4%B8%8EMongoDB%E7%B4%A2%E5%BC%95%E7%9A%84%E5%8C%BA%E5%88%AB/", "categories": "数据库, mysql", "tags": "mysql, mongodb", "date": "2021-08-19 00:00:00 +0800", "snippet": "MySQL索引与MongoDB索引的区别1. 背景 最近学习了MySQL的索引的相关内容，而目前生产系统上使用的使MongoDB，遂对这两个不同数据库的索引进行了下对比。这里的MySQL值得使Innodb存储引擎。2. 两个数据库之间的区别MySQL中的Innodb采用的使B+Tree作为索引的结构，而MongoDB使用的使B-Tree作为索引结构，所以这两个数据库索引之间的区别也就是这两种数据结构之间的区别2.1 B 树和 B + 树B-数B 树的两个明显特点 树内的每个节点都存储数据 叶子节点之间无指针相邻B+数B + 树的两个明显特点 数据只出现在叶子节点 所有叶子节点增加了一个链指针针对上面的 B + 树和 B 树的特点，我们做一个总结 B 树的树内存储数据，因此查询单条数据的时候，B 树的查询效率不固定，最好的情况是 O(1)。我们可以认为在做单一数据查询的时候，使用 B 树平均性能更好。但是，由于 B 树中各节点之间没有指针相邻，因此 B 树不适合做一些数据遍历操作。 B + 树的数据只出现在叶子节点上，因此在查询单条数据的时候，查询速度非常稳定。因此，在做单一数据的查询上，其平均性能并不如 B 树。但是，B + 树的叶子节点上有指针进行相连，因此在做数据遍历的时候，只需要对叶子节点进行遍历即可，这个特性使得 B + 树非常适合做范围查询。 2.2 关系型 VS 非关系型 假设，我们此时有两个逻辑实体: 学生 (Student) 和班级(Class)，这两个逻辑实体之间是一对多的关系。毕竟一个班级有多个学生，一个学生只能属于一个班级。 关系型数据库我们在关系型数据库中，考虑的是用几张表来表示这二者之间的实体关系。常见的无外乎是，一对一关系，用一张表就行。一对多关系，用两张表。多对多关系，用三张表。那这里，我们需要用两张表表示二者之间逻辑关系，如下所示此时如果需要查询cname为1班的班级，有多少学生，MySQL怎么执行（cname这列建了索引）？SQL如下SELECT *FROM t_student t1, ( SELECT cid FROM t_class WHERE cname = &#39;1班&#39; ) t2WHERE t1.cid = t2.cid但凡做这种关联查询，你躲不开 join 操作的，既然涉及到了 join 操作，无外乎从一个表中取一个数据，去另一个表中逐行匹配，如果索引结构是 B + 树，叶子节点上是有指针的，能够极大的提高这种一行一行的匹配速度非关系型数据库在MongoDB中，虽然也可以和在MySQL一样通过两张表来表达学生和班级的关系，但是这并不符合非关系型数据库的设计初衷。在 MongoDB 中，根本不推荐这么设计。虽然，Mongodb 中有一个 $lookup操作，可以做join查询。但是理想情况下，这个$lookup 操作应该不会经常使用，如果你需要经常使用它，那么你就使用了错误的数据存储了（数据库）。如果你有相关联的数据，应该使用关系型数据库（SQL）。因此，正规的设计应该如下假设name这列，我们建了索引此时的执行语句db.class.find( { name: &#39;1班&#39; } )这样就能查询出自己想要的结果。而这，就是一种单一数据查询! 毕竟你不需要去逐行匹配，不涉及遍历操作, 幸运的情况下，有可能一次 IO 就能够得到你想要的结果。3. 总结可以看出由于关系型数据库和非关系型数据的设计方式上的不同。导致在关系型数据中，遍历操作比较常见，因此采用 B + 树作为索引，比较合适。而在非关系型数据库中，单一查询比较常见，因此采用 B 树作为索引，比较合适。参考 为什么Mongodb索引用B树，而Mysql用B+树?" }, { "title": "MySQL - MySQL中的索引", "url": "/posts/MySQL-MySQL%E4%B8%AD%E7%9A%84%E7%B4%A2%E5%BC%95/", "categories": "数据库, mysql", "tags": "mysql", "date": "2021-08-18 00:00:00 +0800", "snippet": "1. 概述定义：索引是存储引擎用于快速找到记录的一种数据结构。举例说明：如果查找一本书中的某个特定主题，一般会先看书的目录（类似索引），找到对应页面。在MySQL，存储引擎采用类似的方法使用索引，高效获取查找的数据。索引的分类 从存储结构上来划分 Btree 索引（B+tree，B-tree) 哈希索引 full-index 全文索引 从应用层次上来划分 普通索引：即一个索引只包含单个列，一个表可以有多个单列索引。 唯一索引：索引列的值必须唯一，但允许有空值。 复合索引：一个索引包含多个列。 从表记录的排列顺序和索引的排列顺序是否一致来划分 聚簇索引（主键）：表记录的排列顺序和索引的排列顺序一致。 非聚集索引：表记录的排列顺序和索引的排列顺序不一致。 2. InnoDB中的索引方案2.2 聚簇索引 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表。 B+树的叶子节点存储的是完整的用户记录。 所谓完整的用户记录，就是指这个记录中存储了所有列的值。 我们把具有这两种特性的B+树称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这种聚簇索引并不需要我们在MySQL语句中显式的使用INDEX语句去创建，InnoDB存储引擎会**自动的为我们创建聚簇索引**。另外有趣的一点是，在InnoDB存储引擎中，聚簇索引就是数据的存储方式（所有的用户记录都存储在了叶子节点），也就是所谓的索引即数据，数据即索引。 ps：聚簇索引默认情况下为准建，如果主键不存在，则使用自动生成隐藏的主键。 2.2 二级索引 上边介绍的聚簇索引只能在搜索条件是主键值时才能发挥作用，因为B+树中的数据都是按照主键进行排序的。那如果我们想以别的列作为搜索条件该咋办呢？这时就可以多建几棵B+树，不同的B+树中的数据采用不同的排序规则，这就是二级索引。比方说我们用c2列的大小作为数据页、页中记录的排序规则，再建一棵B+树，效果如下图所示：这个B+树与上边介绍的聚簇索引有几处不同： 使用记录c2列的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照c2列的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中记录的c2列大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的c2列大小顺序排成一个双向链表。 B+树的叶子节点存储的并不是完整的用户记录，而只是c2列+主键这两个列的值。 目录项记录中不再是主键+页号的搭配，而变成了c2列+页号的搭配。使用二级索引与聚簇索引时的区别：由于聚簇索引即数据，所以在使用时可以直接找到数据信息，而二级索引由于只包含索引值（上图的c2）和聚簇索引（主键）信息，所以根据二级索引查找到信息时，必须再根据主键值去聚簇索引中再查找一遍完整的用户记录，当然如果只需要返回索引包含的字段信息，是可以直接返回的（例如，select c2）。2.3 联合索引我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+树按照c2和c3列的大小进行排序，这个包含两层含义： 先把各个记录和页按照c2列进行排序。 在记录的c2列相同的情况下，采用c3列进行排序以c2和c3列的大小为排序规则建立的B+树称为联合索引，本质上也是一个二级索引。它的意思与分别为c2和c3列分别建立索引的表述是不同的，不同点如下： 建立联合索引只会建立一样的1棵B+树。 为c2和c3列分别建立索引会分别以c2和c3列的大小为排序规则建立2棵B+树。3. MyISAM中的索引方案 我们知道InnoDB中索引即数据，也就是聚簇索引的那棵B+树的叶子节点中已经把所有完整的用户记录都包含了，而MyISAM的索引方案虽然也使用树形结构，但是却将索引和数据分开存储： 将表中的记录按照记录的插入顺序单独存储在一个文件中，称之为数据文件。这个文件并不划分为若干个数据页，有多少记录就往这个文件中塞多少记录就成了。 使用MyISAM存储引擎的表会把索引信息另外存储到一个称为索引文件的另一个文件中。MyISAM会单独为表的主键创建一个索引，只不过在索引的叶子节点中存储的不是完整的用户记录，而是主键值 + 行号的组合。也就是先通过索引找到对应的行号，再通过行号去找对应的记录！ 这一点和InnoDB是完全不相同的，在InnoDB存储引擎中，我们只需要根据主键值对聚簇索引进行一次查找就能找到对应的记录，而在MyISAM中却需要进行一次回表操作，意味着MyISAM中建立的索引相当于全部都是二级索引！ 如果有需要的话，我们也可以对其它的列分别建立索引或者建立联合索引，原理和InnoDB中的索引差不多，不过在叶子节点处存储的是相应的列 + 行号。这些索引也全部都是二级索引。 4. MySql中的索引的使用条件 全值匹配：如果我们的搜索条件中的列和索引列一致的话，这种情况就称为全值匹配 匹配左边的列：在我们的搜索语句中也可以不用包含全部联合索引中的列，只包含左边的就行。例如索引字段为c1+c2+c3，我们使用c1或c1+c2都可以使用到索引，但是c2+c3不能使用到索引 匹配列前缀：索引c1，类型为字符串,当我们使用like &#39;a%&#39;时可以使用到索引，但是匹配的中间，或者后面则不能,例如like &#39;%a%&#39;、like %a 匹配范围值：所有记录都是按照索引列的值从小到大的顺序排好序的，所以这极大的方便我们查找索引列的值在某个范围内的记录。例如，where &#39;A&#39;&amp;lt;c1 and c1&amp;lt;&#39;C&#39;还有更多的使用情况就不一一列举，都大同小异5. 索引的访问方式在MySql中执行查询语句时，查询的执行方式大致分为两种： 使用全表扫描进行查询 这种执行方式很好理解，就是把表的每一行记录都扫一遍嘛，把符合搜索条件的记录加入到结果集就完了。不管是啥查询都可以使用这种方式执行。 使用索引进行查询 因为直接使用全表扫描的方式执行查询要遍历好多记录，所以代价可能太大了。如果查询语句中的搜索条件可以使用到某个索引，那直接使用索引来执行查询可能会加快查询执行的时间。 5.1 const有的时候我们可以通过主键列来定位一条记录，比方说这个查询：SELECT * FROM single_table WHERE id = 1438;类似的，我们根据唯一二级索引列来定位一条记录，比如下边这个查询：SELECT * FROM single_table WHERE key2 = 3841;对于唯一二级索引来说，查询该列为NULL值的情况比较特殊，比如这样：SELECT * FROM single_table WHERE key2 IS NULL;因为唯一二级索引列并不限制 NULL 值的数量，所以上述语句可能访问到多条记录，也就是说上边这个语句不可以使用const访问方法来执行5.2 ref有时候我们对某个普通的二级索引列与常数进行等值比较，比如这样：SELECT * FROM single_table WHERE key1 = &#39;abc&#39;;由于普通二级索引并不限制索引列值的唯一性，所以可能找到多条对应的记录，也就是说使用二级索引来执行查询的代价取决于等值匹配到的二级索引记录条数。如果匹配的记录较少，则回表的代价还是比较低的，所以MySQL可能选择使用索引而不是全表扫描的方式来执行查询。这种搜索条件为二级索引列与常数等值比较，采用二级索引来执行查询的访问方法称为：ref。特殊情况： 二级索引列值为NULL的情况 不论是普通的二级索引，还是唯一二级索引，它们的索引列对包含NULL值的数量并不限制，所以我们采用key IS NULL这种形式的搜索条件最多只能使用ref的访问方法，而不是const的访问方法。 如果最左边的连续索引列并不全部是等值比较的话，它的访问方法就不能称为ref了，比方说这样 SELECT * FROM single_table WHERE key_part1 = &#39;god like&#39; AND key_part2 &amp;gt; &#39;legendary&#39;; 5.3 ref_or_null不仅想找出某个二级索引列的值等于某个常数的记录，还想把该列的值为NULL的记录也找出来，就像下边这个查询：SELECT * FROM single_demo WHERE key1 = &#39;abc&#39; OR key1 IS NULL;当使用二级索引而不是全表扫描的方式执行该查询时，这种类型的查询使用的访问方法就称为ref_or_null5.4 range之前介绍的几种访问方法都是在对索引列与某一个常数进行等值比较的时候才可能使用到，但是有时候我们面对的搜索条件更复杂，比如下边这个查询：SELECT * FROM single_table WHERE key2 IN (1438, 6328) OR (key2 &amp;gt;= 38 AND key2 &amp;lt;= 79);这种利用索引进行范围匹配的访问方法称之为：range其实对于B+树索引来说，只要索引列和常数使用=、&amp;lt;=&amp;gt;、IN、NOT IN、IS NULL、IS NOT NULL、&amp;gt;、&amp;lt;、&amp;gt;=、&amp;lt;=、BETWEEN、!=（不等于也可以写成&amp;lt;&amp;gt;）或者LIKE操作符连接起来，就可以产生一个所谓的区间。5.5 indexSELECT key_part1, key_part2, key_part3 FROM single_table WHERE key_part2 = &#39;abc&#39;;该索引为 key_part1, key_part2, key_part3 由于key_part2并不是联合索引idx_key_part最左索引列，所以我们无法使用ref或者range访问方法来执行这个语句。但是这个查询符合下边这两个条件： 它的查询列表只有3个列：key_part1, key_part2, key_part3，而索引idx_key_part又包含这三个列。 搜索条件中只有key_part2列。这个列也包含在索引idx_key_part中。也就是说我们可以直接通过遍历idx_key_part索引的叶子节点的记录来比较key_part2 = ‘abc’这个条件是否成立，把匹配成功的二级索引记录的key_part1, key_part2, key_part3列的值直接加到结果集中就行了。由于二级索引记录比聚簇索记录小的多（聚簇索引记录要存储所有用户定义的列以及所谓的隐藏列，而二级索引记录只需要存放索引列和主键），而且这个过程也不用进行回表操作，所以直接遍历二级索引比直接遍历聚簇索引的成本要小很多，把这种采用遍历二级索引记录的执行方式称之为：index。5.6 all最直接的查询执行方式就是全表扫描，对于InnoDB表来说也就是直接扫描聚簇索引，把这种使用全表扫描执行查询的方式称之为：all。ps:以上所有访问方式速度大部分情况下是依次递减的6. 总结以上是最近学习MySql索引相关内容后的一个简单的总结参考 《MySql是怎么运行的》 MySQL：索引详解" }, { "title": "数据结构 - B树和B+树", "url": "/posts/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-B%E6%A0%91%E5%92%8CB+%E6%A0%91/", "categories": "算法", "tags": "数据结构", "date": "2021-08-02 03:03:43 +0800", "snippet": "1. 背景​ 最近在学习数据库相关的知识，了解到数据库很多是采用B-/+树作为索引，例如Mysql的InnoDB引擎使用的B+树、MongoDB默认采用B树作为索引。 在计算机科学中，B树（英语：B-tree）是一种自平衡的树，能够保持数据有序。这种数据结构能够让查找数据、顺序访问、插入数据及删除的动作，都在对数时间内完成。B树，概括来说是一个一般化的二叉查找树（binary search tree）一个节点可以拥有2个以上的子节点。与自平衡二叉查找树不同，B树适用于读写相对大的数据块的存储系统，例如磁盘。B树减少定位记录时所经历的中间过程，从而加快存取速度。B树这种数据结构可以用来描述外部存储。这种数据结构常被应用在数据库和文件系统的实现上。–wiki2. B树的定义一个 m 阶的B树是一个有以下属性的树： 每一个节点最多有 m 个子节点 每一个非叶子节点（除根节点）最少有 ⌈m/2⌉ 个子节点 如果根节点不是叶子节点，那么它至少有两个子节点 有 k 个子节点的非叶子节点拥有 k − 1 个键 所有的叶子节点都在同一层阶B 树中一个节点的子节点数目的最大值，用 m 表示，假如最大值为 10，则为 10 阶，如图所有节点中，节点【13,16,19】拥有的子节点数目最多，四个子节点（灰色节点），所以可以定义上面的图片为 4 阶 B 树根节点节点【10】即为根节点，特征：根节点拥有的子节点数量的上限和内部节点相同，如果根节点不是树中唯一节点的话，至少有俩个子节点（不然就变成单支了）。在 m 阶 B 树中（根节点非树中唯一节点），那么有关系式 2&amp;lt;= M &amp;lt;=m，M 为子节点数量；包含的元素数量 1&amp;lt;= K &amp;lt;=m-1,K 为元素数量叶子结点节点【1,2】、节点【11,12】等最后一层都为叶子节点，叶子节点对元素的数量有相同的限制，但是没有子节点，也没有指向子节点的指针。特征：在 m 阶 B 树中叶子节点的元素符合（m/2）-1&amp;lt;= K &amp;lt;=m-13. B数的相关操作3.1 查找B树的搜索和二叉搜索树类似。从根节点开始，从上到下递归的遍历树。在每一层上，搜索的范围被减小到包含了搜索值的子树中。子树值的范围被它的父节点的键确定。3.2 插入所有的插入都从根节点开始。要插入一个新的元素，首先搜索这棵树找到新元素应该被添加到的对应节点。将新元素插入到这一节点中的步骤如下： 如果节点拥有的元素数量小于最大值，那么有空间容纳新的元素。将新元素插入到这一节点，且保持节点中元素有序。 否则的话这一节点已经满了，将它平均地分裂成两个节点： 从该节点的原有元素和新的元素中选择出中位数 小于这一中位数的元素放入左边节点，大于这一中位数的元素放入右边节点，中位数作为分隔值。 分隔值被插入到父节点中，这可能会造成父节点分裂，分裂父节点时可能又会使它的父节点分裂，以此类推。如果没有父节点（这一节点是根节点），就创建一个新的根节点（增加了树的高度）。 如果分裂一直上升到根节点，那么一个新的根节点会被创建，它有一个分隔值和两个子节点。这就是根节点并不像内部节点一样有最少子节点数量限制的原因。以5阶B树举例说明其中5阶B数有一下特征 2&amp;lt;= 根节点子节点个数 &amp;lt;=5 3&amp;lt;= 内节点子节点个数 &amp;lt;=5 1&amp;lt;= 根节点元素个数 &amp;lt;=4 2&amp;lt;= 非根节点元素个数 &amp;lt;=4初始化数据插入元素【8】图（1）插入元素【8】后变为图（2），此时根节点元素个数为 5，不符合 1&amp;lt;= 根节点元素个数 &amp;lt;=4，进行分裂（真实情况是先分裂，然后插入元素，这里是为了直观而先插入元素，下面的操作都一样，不再赘述），取节点中间元素【7】，加入到父节点，左右分裂为 2 个节点，如图（3）接着插入元素【5】，【11】，【17】时，不需要任何分裂操作，如图（4）插入元素【13】节点元素超出最大数量，进行分裂，提取中间元素【13】，插入到父节点当中，如图（6）接着插入元素【6】，【12】，【20】，【23】时，不需要任何分裂操作，如图（7）插入【26】时，最右的叶子结点空间满了，需要进行分裂操作，中间元素【20】上移到父节点中，注意通过上移中间元素，树最终还是保持平衡，分裂结果的结点存在 2 个关键字元素。插入【4】时，导致最左边的叶子结点被分裂，【4】恰好也是中间元素，上移到父节点中，然后元素【16】,【18】,【24】,【25】陆续插入不需要任何分裂操作最后，当插入【19】时，含有【14】,【16】,【17】,【18】的结点需要分裂，把中间元素【17】上移到父节点中，但是情况来了，父节点中空间已经满了，所以也要进行分裂，将父节点中的中间元素【13】上移到新形成的根结点中，这样具体插入操作的完成。3.3 删除首先查找 B 树中需删除的元素, 如果该元素在 B 树中存在，则将该元素在其结点中进行删除；删除该元素后，首先判断该元素是否有左右孩子结点，如果有，则上移孩子结点中的某相近元素 (“左孩子最右边的节点” 或“右孩子最左边的节点”)到父节点中，然后是移动之后的情况；如果没有，直接删除。 某结点中元素数目小于（m/2）-1,(m/2) 向上取整，则需要看其某相邻兄弟结点是否丰满； 如果丰满（结点中元素个数大于 (m/2)-1），则向父节点借一个元素来满足条件； 如果其相邻兄弟都不丰满，即其结点数目等于 (m/2)-1，则该结点与其相邻的某一兄弟结点进行“合并” 成一个结点；以 5 阶 B 树为例，详细讲解删除的动作关键要领，元素个数小于 2（m/2 -1）就合并，大于 4（m-1）就分裂如图依次删除依次删除【8】,【20】,【18】,【5】首先删除元素【8】，当然首先查找【8】，【8】在一个叶子结点中，删除后该叶子结点元素个数为 2，符合 B 树规则，操作很简单，咱们只需要移动【11】至原来【8】的位置，移动【12】至【11】的位置（也就是结点中删除元素后面的元素向前移动）下一步，删除【20】, 因为【20】没有在叶子结点中，而是在中间结点中找到，咱们发现他的继承者【23】(字母升序的下个元素)，将【23】上移到【20】的位置，然后将孩子结点中的【23】进行删除，这里恰好删除后，该孩子结点中元素个数大于 2，无需进行合并操作。下一步删除【18】，【18】在叶子结点中, 但是该结点中元素数目为 2，删除导致只有 1 个元素，已经小于最小元素数目 2, 而由前面我们已经知道：如果其某个相邻兄弟结点中比较丰满（元素个数大于 ceil(5/2)-1=2），则可以向父结点借一个元素，然后将最丰满的相邻兄弟结点中上移最后或最前一个元素到父节点中，在这个实例中，右相邻兄弟结点中比较丰满（3 个元素大于 2），所以先向父节点借一个元素【23】下移到该叶子结点中，代替原来【19】的位置，【19】前移；然【24】在相邻右兄弟结点中上移到父结点中，最后在相邻右兄弟结点中删除【24】，后面元素前移。最后一步删除【5】， 删除后会导致很多问题，因为【5】所在的结点数目刚好达标，刚好满足最小元素个数（ceil(5/2)-1=2）, 而相邻的兄弟结点也是同样的情况，删除一个元素都不能满足条件，所以需要该节点与某相邻兄弟结点进行合并操作；首先移动父结点中的元素（该元素在两个需要合并的两个结点元素之间）下移到其子结点中，然后将这两个结点进行合并成一个结点。所以在该实例中，咱们首先将父节点中的元素【4】下移到已经删除【5】而只有【6】的结点中，然后将含有【4】和【6】的结点和含有【1】,【3】的相邻兄弟结点进行合并成一个结点。也许你认为这样删除操作已经结束了，其实不然，在看看上图，对于这种特殊情况，你立即会发现父节点只包含一个元素【7】，没达标（因为非根节点包括叶子结点的元素 K 必须满足于 2=&amp;lt;K&amp;lt;=4，而此处的 K=1），这是不能够接受的。如果这个问题结点的相邻兄弟比较丰满，则可以向父结点借一个元素。而此时兄弟节点元素刚好为 2，刚刚满足，只能进行合并，而根结点中的唯一元素【13】下移到子结点，这样，树的高度减少一层。4. B+树4.1 B+树的特征 有 m 个子树的中间节点包含有 m 个元素（B 树中是 k-1 个元素），每个元素不保存数据，只用来索引； 所有的叶子结点中包含了全部关键字的信息，及指向含有这些关键字记录的指针，且叶子结点本身依关键字的大小自小而大的顺序链接。 (而 B 树的叶子节点并没有包括全部需要查找的信息)； 所有的非终端结点可以看成是索引部分，结点中仅含有其子树根结点中最大（或最小）关键字。 (而 B 树的非终节点也包含需要查找的有效信息)；参考 B 树、B + 树详解 B树- 维基百科，自由的百科全书 " }, { "title": "Java 日志框架", "url": "/posts/Java-%E6%97%A5%E5%BF%97%E6%A1%86%E6%9E%B6/", "categories": "编程语言", "tags": "java", "date": "2021-06-26 02:00:19 +0800", "snippet": "1. 背景用了几年的Java日志框架，但却对里面的逻辑关系不是特别清楚，准备花时间理清一下其中的关系以及基本的使用说明1.1 常见Java日志框架Log4jLog4j 是 Apache 的一个 Java 的日志库，通过使用 Log4j，我们可以控制日志信息输送的目的地（控制台、文件、数据库等）；我们也可以控制每一条日志的输出格式；通过定义每一条日志信息的级别，我们能够更加细致地控制日志的生成过程。LogbackLogback，一个 “可靠、通用、快速而又灵活的 Java 日志框架”。logback 当前分成三个模块：logback-core，logback- classic 和 logback-access。logback-core 是其它两个模块的基础模块。logback-classic 是 log4j 的一个改良版本。此外 logback-classic 完整实现 SLF4J API 使你可以很方便地更换成其它日志系统，如 log4j 或 JDK14 Logging。logback-access 模块与 Servlet 容器（如 Tomcat 和 Jetty）集成，以提供 HTTP 访问日志功能。请注意，您可以在 logback-core 之上轻松构建自己的模块。Log4j 2Apache Log4j 2 是对 Log4j 的升级，它比其前身 Log4j 1.x 提供了重大改进，并提供了 Logback 中可用的许多改进，同时修复了 Logback 架构中的一些问题。现在最优秀的 Java 日志框架是 Log4j2，没有之一。根据官方的测试表明，在多线程环境下，Log4j2 的异步日志表现更加优秀。在异步日志中，Log4j2 使用独立的线程去执行 I/O 操作，可以极大地提升应用程序的性能。在官方的测试中，Log4j1/Logback/Log4j2 三个日志框架的异步日志性能比较如下图所示。其中，Loggers all async是基于 LMAX Disruptor 实现的。可见 Log4j2 的异步日志性能是最棒的。log4j，log4j2，logback 异步日志性能比较下图比较了 Log4j2 框架Sync、Async Appenders和Loggers all async三者的性能。其中Loggers all async表现最为出色，而且线程数越多，Loggers all async性能越好。log4j2 同步异步 Appender 比较1.2 日志通用接口​ 上述介绍的是一些日志框架的实现（Log4j、Logback、log4j2），他们都有各自的API可以调用，但是我们更多是使用通用的日志调用接口来解决系统与日志实现框架的耦合性。日志通用接口，它不是一个真正的日志实现，而是一个抽象层（ abstraction layer），它允许你在后台使用任意一个日志实现。常见的通用日志接口有commons logging、slf4j，由于前面一个基本没有使用过，所以不过多进行介绍。2. Apache Log4j 2 详解2.1 简介​ Apache Log4j 2 是对 Log4j 的升级，它比其前身 Log4j 1.x 提供了重大改进，并提供了 Logback 中可用的许多改进，同时修复了 Logback 架构中的一些问题。所以后面的例子环境为slf4j&amp;amp;log4j22.2 依赖引入// log4j核心包implementation group: &#39;org.apache.logging.log4j&#39;, name: &#39;log4j-api&#39;, version: &#39;2.14.1&#39;implementation group: &#39;org.apache.logging.log4j&#39;, name: &#39;log4j-core&#39;, version: &#39;2.14.1&#39;// 除了核心包意外，还需要将log4j2与slf4j建立连接implementation group: &#39;org.apache.logging.log4j&#39;, name: &#39;log4j-slf4j-impl&#39;, version: &#39;2.14.1&#39;// 最后引入Slf4j的APIimplementation group: &#39;org.slf4j&#39;, name: &#39;slf4j-api&#39;, version: &#39;1.7.31&#39;更多环境引入方式2.3 配置文件详解Configuration根节点，有以下两个属性 status 有 “trace”, “debug”, “info”, “warn”, “error” and “fatal”，用于控制 log4j2 日志框架本身的日志级别，如果将 stratus 设置为较低的级别就会看到很多关于 log4j2 本身的日志，如加载 log4j2 配置文件的路径等信息 monitorInterval 含义是每隔多少秒重新读取配置文件，可以不重启应用的情况下修改配置 Properties属性。使用来定义常量，以便在其他配置项中引用，该配置是可选的，例如定义日志的存放位置Appenders输出源，用于定义日志输出的地方。log4j2 支持的输出源有很多，有控制台 ConsoleAppender、文件 FileAppender、AsyncAppender、RandomAccessFileAppender、RollingFileAppender、RollingRandomAccessFile 等 ConsoleAppender 控制台输出源是将日志打印到控制台上，开发的时候一般都会配置，以便调试。 name：指定 Appender 的名字。 target：SYSTEM_OUT 或 SYSTEM_ERR, 一般只设置默认: SYSTEM_OUT。 PatternLayout：输出格式，不设置默认为:%m%n。 AsyncAppender 异步输出。AsyncAppender 接受对其他 Appender 的引用，并使 LogEvents 在单独的 Thread 上写入它们。 默认情况下，AsyncAppender 使用 java.util.concurrent.ArrayBlockingQueue ，它不需要任何外部库。请注意，多线程应用程序在使用此 appender 时应小心：阻塞队列容易受到锁争用的影响，并且我们的 测试表明， 当更多线程同时记录时性能可能会变差。考虑使用无锁异步记录器以获得最佳性能。 FileAppender 文件输出源，用于将日志写入到指定的文件，其底层是一个 OutputStreamAppender，需要配置输入到哪个位置（例如：D:/logs/mylog.log） name：指定 Appender 的名字。 fileName：指定输出日志的目的文件带全路径的文件名。 PatternLayout：输出格式，不设置默认为:%m%n。 RollingFileAppender RollingFileAppender 是一个 OutputStreamAppender，它写入 fileName 参数中指定的 File，并根据 TriggeringPolicy 和 RolloverPolicy 滚动文件。 RandomAccessFileAppender RandomAccessFileAppender 类似于标准的 FileAppender ，除了它总是被缓冲（这不能被关闭），并且在内部它使用 ByteBuffer + RandomAccessFile 而不是 BufferedOutputStream。与 FileAppender 相比，我们在测量中看到 “bufferedIO = true”，性能提升了 20-200％ 。 RollingRandomAccessFileAppender RollingRandomAccessFileAppender 类似于标准的 RollingFileAppender， 除了它总是被缓冲（这不能被关闭），并且在内部它使用ByteBuffer + RandomAccessFile 而不是BufferedOutputStream。与 RollingFileAppender 相比，我们在测量中看到 “bufferedIO = true”，性能提升了 20-200％。RollingRandomAccessFileAppender 写入 fileName 参数中指定的文件，并根据 TriggeringPolicy 和 RolloverPolicy 滚动文件。 name：指定 Appender 的名字。 fileName 指定当前日志文件的位置和文件名称 filePattern 指定当发生 Rolling 时，文件的转移和重命名规则 immediateFlush 设置为 true 时 - 默认值，每次写入后都会进行刷新。这将保证数据写入磁盘，但可能会影响性能。 bufferSize 缓冲区大小，默认为 262,144 字节（256 * 1024）。 Policies：指定滚动日志的策略，就是什么时候进行新建日志文件输出日志。 SizeBasedTriggeringPolicy 指定当文件大小大于 size 指定的值时，触发 Rolling TimeBasedTriggeringPolicy 这个配置需要和 filePattern 结合使用，日期格式精确到哪一位，interval 也精确到哪一个单位。注意 filePattern 中配置的文件重命名规则是 ${FILE_NAME}-%d{yyyy-MM-dd HH-mm-ss}-%i，最小的时间粒度是 ss，即秒钟。TimeBasedTriggeringPolicy 默认的 size 是 1，结合起来就是每 1 秒钟生成一个新文件。如果改成 %d{yyyy-MM-dd HH}，最小粒度为小时，则每一个小时生成一个文件 DefaultRolloverStrategy 指定最多保存的文件个数 FiltersFilters 决定日志事件能否被输出。过滤条件有三个值：ACCEPT(接受)，DENY(拒绝)，NEUTRAL(中立)。 ThresholdFilter 输出 warn 级别一下的日志 &amp;lt;Filters&amp;gt; &amp;lt;!--如果是error级别拒绝，设置 onMismatch=&quot;NEUTRAL&quot; 可以让日志经过后续的过滤器--&amp;gt; &amp;lt;ThresholdFilter level=&quot;error&quot; onMatch=&quot;DENY&quot; onMismatch=&quot;NEUTRAL&quot;/&amp;gt; &amp;lt;!--如果是debug\\info\\warn级别的日志输出--&amp;gt; &amp;lt;ThresholdFilter level=&quot;debug&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&amp;gt;&amp;lt;/Filters&amp;gt; 只输出 error 级别以上的日志 &amp;lt;Filters&amp;gt; &amp;lt;ThresholdFilter level=&quot;error&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&amp;gt;&amp;lt;/Filters&amp;gt; TimeFilter 时间过滤器可用于将过滤器限制为仅当天的某个部分。 &amp;lt;Filters&amp;gt; &amp;lt;!-- 只允许在每天的 8点~8点半 之间输出日志 --&amp;gt; &amp;lt;TimeFilter start=&quot;08:00:00&quot; end=&quot;08:30:00&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot; /&amp;gt;&amp;lt;/Filters&amp;gt; PatternLayout控制台或文件输出源（Console、File、RollingRandomAccessFile）都必须包含一个 PatternLayout 节点，用于指定输出文件的格式（如 日志输出的时间 文件 方法 行数 等格式）。简单示例：&amp;lt;PatternLayout pattern=&quot;%d{yyyy-MM-dd HH:mm:ss,SSS} [%t] %-5level %logger{0} - %msg%n&quot; charset=&quot;UTF-8&quot;/&amp;gt;详细配置请查看官网Policy &amp;amp; StrategyPolicy 触发策略 SizeBasedTriggeringPolicy 基于日志文件大小的触发策略。单位有：KB，MB，GB &amp;lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot;/&amp;gt; CronTriggeringPolicy 基于Cron表达式的触发策略，很灵活。 &amp;lt;CronTriggeringPolicy schedule=&quot;0/5 * * * * ?&quot; /&amp;gt; TimeBasedTriggeringPolicy基于时间的触发策略。该策略主要是完成周期性的 log 文件封存工作。有两个参数：interval，integer 型，指定两次封存动作之间的时间间隔。这个配置需要和filePattern结合使用，filePattern日期格式精确到哪一位，interval 也精确到哪一个单位。注意filePattern中配置的文件重命名规则是 %d{yyyy-MM-dd HH-mm-ss}-%i，最小的时间粒度是 ss，即秒钟。TimeBasedTriggeringPolicy 默认的 size 是 1，结合起来就是每 1 秒钟生成一个新文件。如果改成 %d{yyyy-MM-dd HH}，最小粒度为小时，则每一个小时生成一个文件modulate，boolean 型，说明是否对封存时间进行调制。若 modulate=true， 则封存时间将以 0 点为边界进行偏移计算。比如，modulate=true，interval=4hours， 那么假设上次封存日志的时间为 03:00，则下次封存日志的时间为 04:00， 之后的封存时间依次为 08:00，12:00，16:00简单示例：&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;Configuration&amp;gt; &amp;lt;Appenders&amp;gt; &amp;lt;RollingRandomAccessFile name=&quot;File&quot; fileName=&quot;./log/app.log&quot; filePattern=&quot;./log/app-%d{yyyy-MM-dd HH-mm}-%i.log&quot;&amp;gt; &amp;lt;PatternLayout pattern=&quot;%d{yyyy-MM-dd HH:mm:ss,SSS} [%t] %-5level %logger{0} - %msg%n&quot; charset=&quot;UTF-8&quot;/&amp;gt; &amp;lt;Policies&amp;gt; &amp;lt;!-- 每 5s 翻滚一次 --&amp;gt; &amp;lt;!--&amp;lt;CronTriggeringPolicy schedule=&quot;0/5 * * * * ?&quot;/&amp;gt;--&amp;gt; &amp;lt;!--根据当前filePattern配置，日志文件每3分钟滚动一次--&amp;gt; &amp;lt;TimeBasedTriggeringPolicy interval=&quot;3&quot;/&amp;gt; &amp;lt;!--日志文件大于10 KB滚动一次--&amp;gt; &amp;lt;SizeBasedTriggeringPolicy size=&quot;10 KB&quot;/&amp;gt; &amp;lt;/Policies&amp;gt; &amp;lt;!--保存日志文件个数--&amp;gt; &amp;lt;DefaultRolloverStrategy max=&quot;10&quot;/&amp;gt; &amp;lt;/RollingRandomAccessFile&amp;gt; &amp;lt;/Appenders&amp;gt; &amp;lt;Loggers&amp;gt; &amp;lt;Root level=&quot;info&quot;&amp;gt; &amp;lt;AppenderRef ref=&quot;File&quot;/&amp;gt; &amp;lt;/Root&amp;gt; &amp;lt;/Loggers&amp;gt;&amp;lt;/Configuration&amp;gt;Strategy 滚动策略 DefaultRolloverStrategy 默认滚动策略 常用参数：max，保存日志文件的最大个数，默认是 7，大于此值会删除旧的日志文件。 &amp;lt;!--保存日志文件个数--&amp;gt;&amp;lt;DefaultRolloverStrategy max=&quot;10&quot;/&amp;gt; DirectWriteRolloverStrategy 日志直接写入由文件模式表示的文件。 这两个Strategy都是控制如何进行日志滚动的，平时大部分用DefaultRolloverStrategy就可以了。LoggersLoggers 节点，常见的有两种：Root 和 Logger。Root节点用来指定项目的根日志，如果没有单独指定Logger，那么就会默认使用该 Root 日志输出Root每个配置都必须有一个根记录器 Root。如果未配置，则将使用默认根 LoggerConfig，其级别为 ERROR 且附加了 Console appender。根记录器和其他记录器之间的主要区别是：1. 根记录器没有 name 属性。2. 根记录器不支持 additivity 属性，因为它没有父级。 level：日志输出级别，共有 8 个级别，按照从低到高为：All &amp;lt; Trace &amp;lt; Debug &amp;lt; Info &amp;lt; Warn &amp;lt; Error &amp;lt; Fatal &amp;lt; OFF AppenderRef：Root 的子节点，用来指定该日志输出到哪个 Appender.LoggerLogger 节点用来单独指定日志的形式，比如要为指定包下的 class 指定不同的日志级别等。使用Logger元素必须有一个 name 属性，root logger 不用 name 元属性每个Logger可以使用 TRACE，DEBUG，INFO，WARN，ERROR，ALL 或 OFF 之一配置级别。如果未指定级别，则默认为 ERROR。可以为 additivity 属性分配值 true 或 false。如果省略该属性，则将使用默认值 true。Logger还可以配置一个或多个 AppenderRef 属性。引用的每个 appender 将与指定的Logger关联。如果在Logger上配置了多个 appender，则在处理日志记录事件时会调用每个 appender。 name：用来指定该 Logger 所适用的类或者类所在的包全路径，继承自 Root 节点。一般是项目包名或者框架的包名，比如：com.jourwon，org.springframework level：日志输出级别，共有 8 个级别，按照从低到高为：All &amp;lt; Trace &amp;lt; Debug &amp;lt; Info &amp;lt; Warn &amp;lt; Error &amp;lt; Fatal &amp;lt; OFF AppenderRef：Logger 的子节点，用来指定该日志输出到哪个 Appender，如果没有指定，就会默认继承自 Root。如果指定了，那么会在指定的这个 Appender 和 Root 的 Appender 中都会输出，此时我们可以设置 Logger 的 additivity=”false” 只在自定义的 Appender 中进行输出。更多配置文件参考官网2.4 添加配置文件默认情况下，Log4j2 在 classpath 下查找名为log4j2.xml的配置文件。你也可以使用 Java 启动命令指定配置文件的全路径。-Dlog4j.configurationFile=opt/demo/log4j2.xml，你还可以使用 Java 代码指定配置文件路径常用日志配置文件：&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;Configuration&amp;gt; &amp;lt;Properties&amp;gt; &amp;lt;!-- 日志输出级别 --&amp;gt; &amp;lt;Property name=&quot;LOG_INFO_LEVEL&quot; value=&quot;info&quot;/&amp;gt; &amp;lt;!-- error级别日志 --&amp;gt; &amp;lt;Property name=&quot;LOG_ERROR_LEVEL&quot; value=&quot;error&quot;/&amp;gt; &amp;lt;!-- 在当前目录下创建名为log目录做日志存放的目录 --&amp;gt; &amp;lt;Property name=&quot;LOG_HOME&quot; value=&quot;./log&quot;/&amp;gt; &amp;lt;!-- 档案日志存放目录 --&amp;gt; &amp;lt;Property name=&quot;LOG_ARCHIVE&quot; value=&quot;./log/archive&quot;/&amp;gt; &amp;lt;!-- 模块名称， 影响日志配置名，日志文件名，根据自己项目进行配置 --&amp;gt; &amp;lt;Property name=&quot;LOG_MODULE_NAME&quot; value=&quot;spring-boot&quot;/&amp;gt; &amp;lt;!-- 日志文件大小，超过这个大小将被压缩 --&amp;gt; &amp;lt;Property name=&quot;LOG_MAX_SIZE&quot; value=&quot;100 MB&quot;/&amp;gt; &amp;lt;!-- 保留多少天以内的日志 --&amp;gt; &amp;lt;Property name=&quot;LOG_DAYS&quot; value=&quot;15&quot;/&amp;gt; &amp;lt;!--输出日志的格式：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度， %msg：日志消息，%n是换行符 --&amp;gt; &amp;lt;Property name=&quot;LOG_PATTERN&quot; value=&quot;%d [%t] %-5level %logger{0} - %msg%n&quot;/&amp;gt; &amp;lt;!--interval属性用来指定多久滚动一次--&amp;gt; &amp;lt;Property name=&quot;TIME_BASED_INTERVAL&quot; value=&quot;1&quot;/&amp;gt; &amp;lt;/Properties&amp;gt; &amp;lt;Appenders&amp;gt; &amp;lt;!-- 控制台输出 --&amp;gt; &amp;lt;Console name=&quot;STDOUT&quot; target=&quot;SYSTEM_OUT&quot;&amp;gt; &amp;lt;!--输出日志的格式--&amp;gt; &amp;lt;PatternLayout pattern=&quot;${LOG_PATTERN}&quot;/&amp;gt; &amp;lt;!--控制台只输出level及其以上级别的信息（onMatch），其他的直接拒绝（onMismatch）--&amp;gt; &amp;lt;ThresholdFilter level=&quot;${LOG_INFO_LEVEL}&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&amp;gt; &amp;lt;/Console&amp;gt; &amp;lt;!-- 这个会打印出所有的info级别以上，error级别一下的日志，每次大小超过size或者满足TimeBasedTriggeringPolicy，则日志会自动存入按年月日建立的文件夹下面并进行压缩，作为存档--&amp;gt; &amp;lt;RollingRandomAccessFile name=&quot;RollingRandomAccessFileInfo&quot; fileName=&quot;${LOG_HOME}/${LOG_MODULE_NAME}-infoLog.log&quot; filePattern=&quot;${LOG_ARCHIVE}/${LOG_MODULE_NAME}-infoLog-%d{yyyy-MM-dd}-%i.log.gz&quot;&amp;gt; &amp;lt;Filters&amp;gt; &amp;lt;!--如果是error级别拒绝，设置 onMismatch=&quot;NEUTRAL&quot; 可以让日志经过后续的过滤器--&amp;gt; &amp;lt;ThresholdFilter level=&quot;${LOG_ERROR_LEVEL}&quot; onMatch=&quot;DENY&quot; onMismatch=&quot;NEUTRAL&quot;/&amp;gt; &amp;lt;!--如果是info\\warn输出--&amp;gt; &amp;lt;ThresholdFilter level=&quot;${LOG_INFO_LEVEL}&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&amp;gt; &amp;lt;/Filters&amp;gt; &amp;lt;PatternLayout pattern=&quot;${LOG_PATTERN}&quot;/&amp;gt; &amp;lt;Policies&amp;gt; &amp;lt;!--interval属性用来指定多久滚动一次，根据当前filePattern设置是1天滚动一次--&amp;gt; &amp;lt;TimeBasedTriggeringPolicy interval=&quot;${TIME_BASED_INTERVAL}&quot;/&amp;gt; &amp;lt;SizeBasedTriggeringPolicy size=&quot;${LOG_MAX_SIZE}&quot;/&amp;gt; &amp;lt;/Policies&amp;gt; &amp;lt;!-- DefaultRolloverStrategy属性如不设置，则默认同一文件夹下最多保存7个文件--&amp;gt; &amp;lt;DefaultRolloverStrategy max=&quot;${LOG_DAYS}&quot;/&amp;gt; &amp;lt;/RollingRandomAccessFile&amp;gt; &amp;lt;!--只记录error级别以上的日志，与info级别的日志分不同的文件保存--&amp;gt; &amp;lt;RollingRandomAccessFile name=&quot;RollingRandomAccessFileError&quot; fileName=&quot;${LOG_HOME}/${LOG_MODULE_NAME}-errorLog.log&quot; filePattern=&quot;${LOG_ARCHIVE}/${LOG_MODULE_NAME}-errorLog-%d{yyyy-MM-dd}-%i.log.gz&quot;&amp;gt; &amp;lt;Filters&amp;gt; &amp;lt;ThresholdFilter level=&quot;${LOG_ERROR_LEVEL}&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&amp;gt; &amp;lt;/Filters&amp;gt; &amp;lt;PatternLayout pattern=&quot;${LOG_PATTERN}&quot;/&amp;gt; &amp;lt;Policies&amp;gt; &amp;lt;TimeBasedTriggeringPolicy interval=&quot;${TIME_BASED_INTERVAL}&quot;/&amp;gt; &amp;lt;SizeBasedTriggeringPolicy size=&quot;${LOG_MAX_SIZE}&quot;/&amp;gt; &amp;lt;/Policies&amp;gt; &amp;lt;DefaultRolloverStrategy max=&quot;${LOG_DAYS}&quot;/&amp;gt; &amp;lt;/RollingRandomAccessFile&amp;gt; &amp;lt;/Appenders&amp;gt; &amp;lt;Loggers&amp;gt; &amp;lt;!-- 开发环境使用 --&amp;gt; &amp;lt;!--&amp;lt;Root level=&quot;${LOG_INFO_LEVEL}&quot;&amp;gt; &amp;lt;AppenderRef ref=&quot;STDOUT&quot;/&amp;gt; &amp;lt;/Root&amp;gt;--&amp;gt; &amp;lt;!-- 测试，生产环境使用 --&amp;gt; &amp;lt;Root level=&quot;${LOG_INFO_LEVEL}&quot;&amp;gt; &amp;lt;AppenderRef ref=&quot;RollingRandomAccessFileInfo&quot;/&amp;gt; &amp;lt;AppenderRef ref=&quot;RollingRandomAccessFileError&quot;/&amp;gt; &amp;lt;/Root&amp;gt; &amp;lt;/Loggers&amp;gt;&amp;lt;/Configuration&amp;gt;2.5 日志答应重复问题如果Root中的日志包含了Logger中的日志信息，并且AppenderRef是一样的配置，则日志会打印两次。这是 log4j2 继承机制问题，在 Log4j2 中，logger 是有继承关系的，root 是根节点，在 log4j2 中，有个 additivity 的属性，它是子 Logger 是否继承 父 Logger 的 输出源（appender） 的属性。具体说，默认情况下子 Logger 会继承父 Logger 的 appender，也就是说子 Logger 会在父 Logger 的 appender 里输出。若是 additivity 设为 false，则子 Logger 只会在自己的 appender 里输出，而不会在父 Logger 的 appender 里输出。要打破这种传递性，也非常简单，在 logger 中添加 additivity = “false”，如下所示：&amp;lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&amp;gt;&amp;lt;Configuration&amp;gt; &amp;lt;Appenders&amp;gt; &amp;lt;Console name=&quot;Console&quot;&amp;gt; &amp;lt;PatternLayout&amp;gt; &amp;lt;Pattern&amp;gt;%d{yyyy-MM-dd HH:mm:ss,SSS} [%t] %-5level %logger{0} - %msg%n&amp;lt;/Pattern&amp;gt; &amp;lt;/PatternLayout&amp;gt; &amp;lt;/Console&amp;gt; &amp;lt;/Appenders&amp;gt; &amp;lt;Loggers&amp;gt; &amp;lt;!-- name属性为项目包名或者类名 --&amp;gt; &amp;lt;Logger name=&quot;com.jourwon&quot; level=&quot;debug&quot; additivity=&quot;false&quot;&amp;gt; &amp;lt;AppenderRef ref=&quot;Console&quot;/&amp;gt; &amp;lt;/Logger&amp;gt; &amp;lt;Root level=&quot;error&quot;&amp;gt; &amp;lt;AppenderRef ref=&quot;Console&quot;/&amp;gt; &amp;lt;/Root&amp;gt; &amp;lt;/Loggers&amp;gt;&amp;lt;/Configuration&amp;gt;2.6 使用 Lombok 工具简化创建 Logger 类 lombok 就是一个注解工具 jar 包，能帮助我们省略一繁杂的代码。引入依赖// lombokcompileOnly &#39;org.projectlombok:lombok:1.18.20&#39;annotationProcessor &#39;org.projectlombok:lombok:1.18.20&#39;testCompileOnly &#39;org.projectlombok:lombok:1.18.20&#39;testAnnotationProcessor &#39;org.projectlombok:lombok:1.18.20&#39;使用 Lombok 后，@Slf4j 注解生成了 log 日志常量，无需去声明一个 log 就可以在类中使用 log 记录日志。@Slf4jpublic class Log4jTest { public static void main(String[] args) { log.error(&quot;Something else is wrong here&quot;); } }3. 参考 Java 日志框架与 Log4j2 详解 - 简书 Java日志框架：logback详解 " }, { "title": "Flink - Flink DataStream端到端的Exactly-Once保障", "url": "/posts/Flink-Flink-DataStream%E7%AB%AF%E5%88%B0%E7%AB%AF%E7%9A%84Exactly-Once%E4%BF%9D%E9%9A%9C/", "categories": "大数据框架", "tags": "flink", "date": "2021-05-08 00:00:00 +0800", "snippet": "1. Exactly-Once概述​ 一个一直运行的Flink Stream程序不出错那肯定时很好的，但是在现实世界中，系统难免会出现各种意外，一旦故障发生，Flink作业就会重启，读取最近Checkpoint的数据，恢复状态，并继续接着执行任务。​ Checkpoint时可以保证程序内部状态的一致性的，但是任会有数据重新消费的问题，举个简单的例子：​ 一个简单的计算总和的程序，从Kafka获取数字，并相加到一起，如图所示 程序正常Checkpoint，输出1，4，9 往后消费7，9后程序异常退出，此时程序输出1，4，9，16，25 程序从上次5的位置进行恢复往后消费 一直消费到11，此时程序由于是从5往后消费，所以会重新输出16,25在上述情况中，程序重启后部分数据被重新发送了一次，也就是一些数据在某些情况下不止被处理了一次，而是多次，即At-Least-Once。有时候我们期望一条数据只影响一次最终结果，也就是Exactly-Once2. 实现方式2.1 幂等写​ 幂等写（Idempotent Write）是指，任意多次笑一个系统写入相同数据，只对目标系统产生一次结果影响，例如重复向一个HashMap里面插入三组相同的二元组，只有第一次插入时，数组结果会发送变化，后面两次插入不会影响HashMap结果2.2 事务写​ 事务（Transaction）时数据库系统所要解决的核心问题。Flink借鉴了数据库中的事务技术，同时结合自身的Checkpoint机制来保证Sink只对外部输出产生一次影响。​ 简单来说，Flink事务写是指，Flink先将待输出的数据保存下来，暂时不提交到外部系统，等到CheckPoint结束，Flink上下游所有算子的数据一致时，再将之前保存的数据全部提交到外部系统，如图所示。在事务写的具体实现上，Flink目前提供了两种方式：预写日志（Write-Ahead-Log，WAL）和两段式提交（Two-Phase-Commit，2PC）。这两种方式也是很多数据库和分布式系统实现事务经常采用的方式，Flink根据自身的条件对两种方式做了适应性调整。2.2.1 Write-Ahead-Log协议原理​ Write-Ahead-Log核心思想是，再写入下游系统之前，先把数据以日志的形式缓存下来，等收到明确的确认提交信息后，再将Log中的数据提交到下游系统。由于数据都写到了Log里，即使出现故障恢复，也可以根据Log中的数据决定是否需要恢复、如何恢复。而在Fliink中，数据会被保存在State中。​ 但是Write-Ahead-Log仍然无法提供百分之百的Exactly-Once，例如，写入下游系统时可能中途崩溃，导致部分数据提交，部分数据未提交。​ Write-Ahead-Log的方式相对比较通用，目前Flink的Cassandra Sink使用这种方式提供Exactly-Once保障2.2.2 Two-Phase-Commit 协议的原理和实现​ Two-Phase-Commit 与Write-Ahead-Log相比，Flink中的Two-Phase-Commit协议不再将数据缓存在State中，而是直接将数据写入到外部系统，比如支持事务的Kafka。​ 在Flink写出数据到Kafka中时，Flink会先beginTransaction()开启事务，事务开启后再preCommit()预提交数据，待Flink Checkpoint完成后，Flink会commit()提交这些数据，此时一组数据就被写入到了Kafka。​ 值得注意的是，Kafka Consumer在默认情况下，是可以读取到preCommit()的数据，只有当isolation.level被设置为read_committed时，Kafka Consumer才会只读取commit()后的数据（ read_uncommitted - 是默认值）参考 Flink原理与实践 Apache Kafka 连接器 " }, { "title": "Flink — 状态(State)&amp;检查点(Checkpoint)&amp;保存点(Savepoint)原理", "url": "/posts/Flink-%E7%8A%B6%E6%80%81&%E6%A3%80%E6%9F%A5%E7%82%B9&%E4%BF%9D%E5%AD%98%E7%82%B9%E5%8E%9F%E7%90%86/", "categories": "大数据框架", "tags": "flink", "date": "2021-04-22 00:00:00 +0800", "snippet": "Flink DataStream—— 状态(State)&amp;amp;检查点(Checkpoint)&amp;amp;保存点(Savepoint)原理[TOC]1. 背景​ 最近一次项目当中需要将大量数据保存再Flink程序当中用作缓存数据一共后续数据使用，隧对最近使用到的状态、检查点、保存点等原理和使用进行一个总结2. 状态2.1 概述​ 首先，状态和算子是息息相关的，所以算子可以分为有状态算子和无状态算子，但是不同通俗的将某个算子定义为无状态算子或有状态算子。举个例子，以map为例，在默认使用map的情况下，map是属于一个无状态算子，因为他的结果输出是只观察当前输入的事件，并不依赖其他输入事件，所以此时他是一个无状态算子，但是在某些情况下，例如我们给他添加上状态，他就成了一个有状态算子。举个例子来说，现在存在输入的事件存在重复，我们需要对输入的事件进行去重，就需要对输入的事件进行一个保存，并过滤掉重复的事件，这个时候会需要给这个map算子添加一个状态(state)用来保存已经处理过的事件，用来确保后续事件不会重复进入。如上图所示，状态的概念和逻辑并不复杂，但是流式处理框架仍需要解决一下问题： 数据的产出要保证实时性，延迟不能太高。 需要保证数据不丢不重，恰好计算一次，尤其是当状态数据非常大或者应用出现故障需要恢复时，要保证状态不出任何错误。 一般流处理任务都是7*24小时运行的，程序的可靠性非常高。基于上述要求，我们不能将状态直接交由内存管理，因为内存的容量是有限制的，当状态数据稍微大一些时，就会出现内存不够的问题。假如我们使用一个持久化的备份系统，不断将内存中的状态备份起来，当流处理作业出现故障时，需要考虑如何从备份中恢复。而且，大数据应用一般是横向分布在多个节点上，流处理框架需要保证横向的伸缩扩展性。可见，状态的管理并不那么容易。作为一个计算框架，Flink提供了有状态的计算，封装了一些底层的实现，比如状态的高效存储、Checkpoint和Savepoint持久化备份机制、计算资源扩缩容等问题。因为Flink接管了这些问题，开发者只需调用Flink API，这样可以更加专注于业务逻辑。2.2 状态的几种类型Managed State和Raw StateFlink有两种基本类型的状态：托管状态（Managed State）和原生状态（Raw State）   Managed State Raw State 状态管理方式 Flink Runtime托管，自动存储、自动恢复、自动伸缩 用户自己管理 状态数据结构 Flink提供的常用数据结构，如ListState、MapState等 字节数组：byte[] 使用场景 绝大多数Flink算子 用户自定义算子 上表展示了两者的区别，主要包括： 从状态管理的方式上来说，Managed State由Flink Runtime托管，状态是自动存储、自动恢复的，Flink在存储管理和持久化上做了一些优化。当我们横向伸缩，或者说我们修改Flink应用的并行度时，状态也能自动重新分布到多个并行实例上。Raw State是用户自定义的状态。 从状态的数据结构上来说，Managed State支持了一系列常见的数据结构，如ValueState、ListState、MapState等。Raw State只支持字节，任何上层数据结构需要序列化为字节数组。使用时，需要用户自己序列化，以非常底层的字节数组形式存储，Flink并不知道存储的是什么样的数据结构。 从具体使用场景来说，绝大多数的算子都可以通过继承RichFunction函数类或其他提供好的接口类，在里面使用Managed State。Raw State是在已有算子和Managed State不够用时，用户自定义算子时使用。Keyed State和Operator State对Managed State继续细分，它又有两种类型：Keyed State和Operator StateKeyed State是KeyedStream上的状态。假如输入流按照id为Key进行了keyBy分组，形成一个KeyedStream，数据流中所有id为1的数据共享一个状态，可以访问和更新这个状态，以此类推，每个Key对应一个自己的状态。下图展示了Keyed State，因为一个算子子任务可以处理一到多个Key，算子子任务1处理了两种Key，两种Key分别对应自己的状态。Operator State可以用在所有算子上，每个算子子任务或者说每个算子实例共享一个状态，流入这个算子子任务的所有数据都可以访问和更新这个状态。下图展示了Operator State，算子子任务1上的所有数据可以共享第一个Operator State，以此类推，每个算子子任务上的数据共享自己的状态。无论是Keyed State还是Operator State，Flink的状态都是基于本地的，即每个算子子任务维护着自身的状态，不能访问其他算子子任务的状态。   Keyed State Operator State 适用算子类型 只适用于KeyedStream上的算子 可以用于所有算子 状态分配 每个Key对应一个状态 一个算子子任务对应一个状态 创建和访问方式 重写Rich Function，通过里面的RuntimeContext访问 实现CheckpointedFunction等接口 横向扩展 状态随着Key自动在多个算子子任务上迁移 有多种状态重新分配的方式 支持的数据结构 ValueState、ListState、MapState等 ListState、BroadcastState等 上表总结了Keyed State和Operator State的区别Keyed State的使用方法对于Keyed State，Flink提供了几种现成的数据结构供我们使用，包括ValueState、ListState等，他们的继承关系如下图所示。首先，State主要有三种实现，分别为ValueState、MapState和AppendingState，AppendingState又可以细分为ListState、ReducingState和AggregatingState。这几个状态的具体区别在于： ValueState&amp;lt;T&amp;gt;是单一变量的状态，T是某种具体的数据类型，比如Double、String，或我们自己定义的复杂数据结构。我们可以使用T value()方法获取状态，使用void update(T value)更新状态。 MapState&amp;lt;UK, UV&amp;gt;存储一个Key-Value Map，其功能与Java的Map几乎相同。UV get(UK key)可以获取某个Key下的Value值，void put(UK key, UV value)可以对某个Key设置Value，boolean contains(UK key)判断某个Key是否存在，void remove(UK key)删除某个Key以及对应的Value，Iterable&amp;lt;Map.Entry&amp;lt;UK, UV&amp;gt;&amp;gt; entries()返回MapState中所有的元素，Iterator&amp;lt;Map.Entry&amp;lt;UK, UV&amp;gt;&amp;gt; iterator()返回状态的迭代器。需要注意的是，MapState中的Key和Keyed State的Key不是同一个Key。 ListState&amp;lt;T&amp;gt;存储了一个由T类型数据组成的列表。我们可以使用void add(T value)或void addAll(List&amp;lt;T&amp;gt; values)向状态中添加元素，使用Iterable&amp;lt;T&amp;gt; get()获取整个列表，使用void update(List&amp;lt;T&amp;gt; values)来更新列表，新的列表将替换旧的列表。 ReducingState&amp;lt;T&amp;gt;和AggregatingState&amp;lt;IN, OUT&amp;gt;与ListState&amp;lt;T&amp;gt;同属于MergingState&amp;lt;IN, OUT&amp;gt;。与ListState&amp;lt;T&amp;gt;不同的是，ReducingState&amp;lt;T&amp;gt;只有一个元素，而不是一个列表。它的原理是：新元素通过void add(T value)加入后，与已有的状态元素使用ReduceFunction合并为一个元素，并更新到状态里。AggregatingState&amp;lt;IN, OUT&amp;gt;与ReducingState&amp;lt;T&amp;gt;类似，也只有一个元素，只不过AggregatingState&amp;lt;IN, OUT&amp;gt;的输入和输出类型可以不一样。ReducingState&amp;lt;T&amp;gt;和AggregatingState&amp;lt;IN, OUT&amp;gt;与窗口上进行ReduceFunction和AggregateFunction很像，都是将新元素与已有元素做聚合。以上就是关于状态的基本信息了。在日常分布式场景中，主要使用的还是Keyed State较多。3. 检查点​ 在上面介绍了Flink的算子都是基于本地的，而Flink又是一个部署在多节点的分布式系统，分布式系统经常出现进程被杀、节点宕机或网络中断等问题，那么本地的状态在遇到故障时如何保证不丢呢？Flink定期保存状态数据到存储上，故障发生后从之前的备份中恢复，这个过程被称为Checkpoint机制。3.1 Checkpoint大致流程 暂停处理新流入数据，将新数据缓存起来。 将算子子任务的本地状态数据拷贝到一个远程的持久化存储上。 继续处理新流入的数据，包括刚才缓存起来的数据。Flink是在Chandy–Lamport算法的基础上实现了一种分布式快照算法。在介绍Flink的快照详细流程前，我们先要了解一下检查点分界线（Checkpoint Barrier）的概念。如下图所示，Checkpoint Barrier被插入到数据流中，它将数据流切分成段。Flink的Checkpoint逻辑是，一段新数据流入导致状态发生了变化，Flink的算子接收到Checpoint Barrier后，对状态进行快照。每个Checkpoint Barrier有一个ID，表示该段数据属于哪次Checkpoint。如下图所示，当ID为n的Checkpoint Barrier到达每个算子后，表示要对n-1和n之间状态更新做快照。Checkpoint Barrier有点像Event Time中的Watermark，它被插入到数据流中，但并不影响数据流原有的处理顺序。接下来，我们构建一个并行数据流图，用这个并行数据流图来演示Flink的分布式快照机制。这个数据流图的并行度为2，数据流会在这些并行算子上从Source流动到Sink。首先，Flink的检查点协调器（Checkpoint Coordinator）触发一次Checkpoint（Trigger Checkpoint），这个请求会发送给Source的各个子任务。各Source算子子任务接收到这个Checkpoint请求之后，会将自己的状态写入到状态后端，生成一次快照，并且会向下游广播Checkpoint Barrier。Source算子做完快照后，还会给Checkpoint Coodinator发送一个确认，告知自己已经做完了相应的工作。这个确认中包括了一些元数据，其中就包括刚才备份到State Backend的状态句柄，或者说是指向状态的指针。至此，Source完成了一次Checkpoint。跟Watermark的传播一样，一个算子子任务要把Checkpoint Barrier发送给所连接的所有下游子任务。对于下游算子来说，可能有多个与之相连的上游输入，我们将算子之间的边称为通道。Source要将一个ID为n的Checkpoint Barrier向所有下游算子广播，这也意味着下游算子的多个输入通道里都会收到ID为n的Checkpoint Barrier，而且不同输入通道里Checkpoint Barrier的流入速度不同，ID为n的Checkpoint Barrier到达的时间不同。Checkpoint Barrier传播的过程需要进行对齐（Barrier Alignment），我们从数据流图中截取一小部分，以下图为例，来分析Checkpoint Barrier是如何在算子间传播和对齐的。如上图所示，对齐分为四步： 算子子任务在某个输入通道中收到第一个ID为n的Checkpoint Barrier，但是其他输入通道中ID为n的Checkpoint Barrier还未到达，该算子子任务开始准备进行对齐。 算子子任务将第一个输入通道的数据缓存下来，同时继续处理其他输入通道的数据，这个过程被称为对齐。 第二个输入通道ID为n的Checkpoint Barrier抵达该算子子任务，所有通道ID为n的Checkpoint Barrier都到达该算子子任务，该算子子任务执行快照，将状态写入State Backend，然后将ID为n的Checkpoint Barrier向下游所有输出通道广播。 对于这个算子子任务，快照执行结束，继续处理各个通道中新流入数据，包括刚才缓存起来的数据。数据流图中的每个算子子任务都要完成一遍上述的对齐、快照、确认的工作，当最后所有Sink算子确认完成快照之后，说明ID为n的Checkpoint执行结束，Checkpoint Coordinator向State Backend写入一些本次Checkpoint的元数据。之所以要进行对齐，主要是为了保证一个Flink作业所有算子的状态是一致的，也就是说，一个Flink作业前前后后所有算子写入State Backend的状态都是基于同样的数据。3.2 状态存储(State Backend)上面为Checkpint的大致流程，其中State Backend起到了持久化存储数据的重要功能。Flink将State Backend抽象成了一种插件，并提供了三种State Backend，每种State Backend对数据的保存和恢复方式略有不同。接下来我们开始详细了解一下Flink的State Backend。MemoryStateBackend从名字中可以看出，这种State Backend主要基于内存，它将数据存储在Java的堆区。当进行分布式快照时，所有算子子任务将自己内存上的状态同步到JobManager的堆上。因此，一个作业的所有状态要小于JobManager的内存大小。这种方式显然不能存储过大的状态数据，否则将抛出OutOfMemoryError异常。这种方式只适合调试或者实验，不建议在生产环境下使用。下面的代码告知一个Flink作业使用内存作为State Backend，并在参数中指定了状态的最大值，默认情况下，这个最大值是5MB。env.setStateBackend(new MemoryStateBackend(MAX_MEM_STATE_SIZE));如果不做任何配置，默认情况是使用内存作为State Backend。FStateBackend这种方式下，数据持久化到文件系统上，文件系统包括本地磁盘、HDFS以及包括Amazon、阿里云在内的云存储服务。使用时，我们要提供文件系统的地址，尤其要写明前缀，比如：file://、hdfs://或s3://。此外，这种方式支持Asynchronous Snapshot，默认情况下这个功能是开启的，可加快数据同步速度。// 使用HDFS作为State Backendenv.setStateBackend(new FsStateBackend(&quot;hdfs://namenode:port/flink-checkpoints/chk-17/&quot;));// 使用阿里云OSS作为State Backendenv.setStateBackend(new FsStateBackend(&quot;oss://&amp;lt;your-bucket&amp;gt;/&amp;lt;object-name&amp;gt;&quot;));// 使用Amazon作为State Backendenv.setStateBackend(new FsStateBackend(&quot;s3://&amp;lt;your-bucket&amp;gt;/&amp;lt;endpoint&amp;gt;&quot;));// 关闭Asynchronous Snapshotenv.setStateBackend(new FsStateBackend(checkpointPath, false));Flink的本地状态仍然在TaskManager的内存堆区上，直到执行快照时状态数据会写到所配置的文件系统上。因此，这种方式能够享受本地内存的快速读写访问，也能保证大容量状态作业的故障恢复能力。RocksDBStateBackend这种方式下，本地状态存储在本地的RocksDB上。RocksDB是一种嵌入式Key-Value数据库，数据实际保存在本地磁盘上。比起FsStateBackend的本地状态存储在内存中，RocksDB利用了磁盘空间，所以可存储的本地状态更大。然而，每次从RocksDB中读写数据都需要进行序列化和反序列化，因此读写本地状态的成本更高。快照执行时，Flink将存储于本地RocksDB的状态同步到远程的存储上，因此使用这种State Backend时，也要配置分布式存储的地址。Asynchronous Snapshot在默认情况也是开启的。此外，这种State Backend允许增量快照（Incremental Checkpoint），Incremental Checkpoint的核心思想是每次快照时只对发生变化的数据增量写到分布式存储上，而不是将所有的本地状态都拷贝过去。Incremental Checkpoint非常适合超大规模的状态，快照的耗时将明显降低，同时，它的代价是重启恢复的时间更长。默认情况下，Incremental Checkpoint没有开启，需要我们手动开启。// 开启Incremental Checkpointboolean enableIncrementalCheckpointing = true;env.setStateBackend(new RocksDBStateBackend(checkpointPath, enableIncrementalCheckpointing));相比FsStateBackend，RocksDBStateBackend能够支持的本地和远程状态都更大，Flink社区已经有TB级的案例。除了上述三种之外，开发者也可以自行开发State Backend的具体实现。4. 存储点目前，Checkpoint和Savepoint在代码层面使用的分布式快照逻辑基本相同，生成的数据也近乎一样，那这两个相似的名字到底有哪些功能性的区别呢？Checkpoint的目的是为了故障重启，使得作业中的状态数据与故障重启之前保持一致，是一种应对意外情况的有力保障。Savepoint的目的是手动备份数据，以便进行调试、迁移、迭代等，是一种协助开发者的支持功能。一方面，一个流处理作业不可能一次性就写好了，我们要在一个初版代码的基础上不断修复问题、增加功能、优化算法、甚至做一些机房迁移，一个程序是在迭代中更新的；另外一方面，流处理作业一般都是长时间运行的，作业内部的状态数据从零开始重新生成的成本很高，状态数据迁移成本高。综合这两方面的因素，Flink提供了Savepoint的机制，允许开发者调试开发有状态的作业。Flink的Checkpoint机制设计初衷为：第一，Checkpoint过程是轻量级的，尽量不影响正常数据处理；第二，故障恢复越快越好。开发者需要进行的操作并不多，少量的操作包括：设置多大的间隔来定期进行Checkpoint，使用何种State Backend。绝大多数工作是由Flink来处理的，比如Flink会定期执行快照，发生故障后，Flink自动从最近一次Checkpoint数据中恢复。随着作业的关停，Checkpoint数据一般会被Flink删除，除非开发者设置了保留Checkpoint数据。原则上，一个作业从Checkpoint数据中恢复，作业的代码和业务逻辑不能发生变化。相比而下，Savepoint机制主要考虑的是：第一，刻意备份，第二，支持修改状态数据或业务逻辑。Savepoint相关操作是有计划的、人为的。开发者要手动触发、管理和删除Savepoint。比如，将当前状态保存下来之后，我们可以更新并行度，修改业务逻辑代码，甚至在某份代码基础上生成一个对照组来验证一些实验猜想。可见，Savepoint的数据备份和恢复都有更高的时间和人力成本，Savepoint数据也必须有一定的可移植性，能够适应数据或逻辑上的改动。具体而言，Savepoint的潜在应用场景有： 我们可以给同一份作业设置不同的并行度，来找到最佳的并行度设置，每次可以从Savepoint中加载原来的状态数据。 我们想测试一个新功能或修复一个已知的bug，并用新的程序逻辑处理原来的数据。 进行一些A/B实验，使用相同的数据源测试程序的不同版本。 因为状态可以被持久化存储到分布式文件系统上，我们甚至可以将同样一份应用程序从一个集群迁移到另一个集群，只需保证不同的集群都可以访问这个文件系统。可见，Checkpoint和Savepoint是Flink提供的两个相似的功能，它们满足了不同的需求，以确保一致性、容错性，满足了作业升级、BUG 修复、迁移、A/B测试等不同场景。参考 状态、检查点和保存点 Flink 状态分类" }, { "title": "Flink — 时间概念与Watermark", "url": "/posts/Flink-%E6%97%B6%E9%97%B4%E6%A6%82%E5%BF%B5%E4%B8%8EWatermark/", "categories": "大数据框架", "tags": "flink", "date": "2021-03-16 00:00:00 +0800", "snippet": "1. Flink的三种时间概念类型对于流式处理，最大的特点是数据上具有时间的属性特征，Flink根据时间产生的不同位置分为三个时间概念： Event Time（事件时间）：每条数据或事件自带的时间属性。由于时间属性依附于数据本身，在高并发的情况下可能存在Event Time的到达为乱序的，即一个较早发生的数据延迟到达 Process Time（处理时间）：对于某个算子来说，Processing Time指算子使用当前机器的系统时钟时间 Ingestion Time（接入时间）：事件到达Flink Source的时间1.1 Flink程序时间语义设置// 最新Flink 1.12 版本默认使用Event Time// 另外两种时间语义，需要替换为：TimeCharacteristic.ProcessingTime和TimeCharacteristic.IngestionTime。env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);2. Event Time和WatermarkFlink的三种时间语义中，Processing Time和Ingestion Time都是基于Flink本身所产生的时间，可以不用设置时间字段和Watermark。如果要使用Event Time，以下两项配置缺一不可：第一，使用一个时间戳为数据流中每个事件的Event Time赋值；第二，生成Watermark。Event Time是每个事件的元数据，如果不设置，Flink并不知道每个事件的发生时间，我们必须要为每个事件的Event Time赋值一个时间戳。关于时间戳，包括Flink在内的绝大多数系统都使用Unix时间戳系统（Unix time或Unix epoch）。Unix时间戳系统以1970-01-01 00:00:00.000 为起始点，其他时间记为距离该起始时间的整数差值，一般是毫秒（millisecond）精度。有了Event Time时间戳，我们还必须生成Watermark。Watermark是Flink插入到数据流中的一种特殊的数据结构，它包含一个时间戳，并假设后续不会有小于该时间戳的数据，如果后续数据存在小于该时间戳的数据则视为延迟数据，需另外处理。下图展示了一个乱序数据流，其中方框是单个事件，方框中的数字是其对应的Event Time时间戳，圆圈为Watermark，圆圈中的数字为Watermark对应的时间戳。简单理解Watermark，当Flink处理到Watermark为10的数据时，则Flink就认为10以前的数据已经全部进入到了Flink。举个例子，9.30的考试，但是到9.45才会停止考生入场，9.45就相当于Watermark，当时间到9.45时，则会认为所有考生已经进场考试。Watermark的生成有以下几点需要注意： Watermark与事件的时间戳紧密相关。一个时间戳为t的Watermark会假设后续到达事件的时间戳都大于t。 假如Flink算子接收到一个违背上述规则的事件，该事件将被认定为迟到数据，如上图中时间戳为19的事件比Watermark(20)更晚到达。Flink提供了一些其他机制来处理迟到数据 Watermark时间戳必须单调递增，以保证时间不会倒流。 Watermark机制允许用户来控制准确度和延迟。Watermark设置得与事件时间戳相距紧凑，会产生不少迟到数据，影响计算结果的准确度，整个应用的延迟很低；Watermark设置得非常宽松，准确度能够得到提升，但应用的延迟较高，因为Flink必须等待更长的时间才进行计算。2.1 分布式环境下Watermark的传播在实际计算过程中，Flink的算子一般分布在多个并行的算子子任务（或者称为实例、分区）上，Flink需要将Watermark在并行环境下向前传播。如下图中第一步所示，Flink的每个并行算子子任务会维护针对该子任务的Event Time时钟，这个时钟记录了这个算子子任务Watermark处理进度，随着上游Watermark数据不断向下发送，算子子任务的Event Time时钟也要不断向前更新。由于上游各分区的处理速度不同，到达当前算子的Watermark也会有先后快慢之分，每个算子子任务会维护来自上游不同分区的Watermark信息，这是一个列表，列表内对应上游算子各分区的Watermark时间戳等信息。当上游某分区有Watermark进入该算子子任务后，Flink先判断新流入的Watermark时间戳是否大于Partition Watermark列表内记录的该分区的历史Watermark时间戳，如果新流入的更大，则更新该分区的Watermark。如上图中第二步所示，某个分区新流入的Watermark时间戳为4，算子子任务维护的该分区Watermark为1，那么Flink会更新Partition Watermark列表为最新的时间戳4。接着，Flink会遍历Partition Watermark列表中的所有时间戳，选择最小的一个作为该算子子任务的Event Time。同时，Flink会将更新的Event Time作为Watermark发送给下游所有算子子任务。算子子任务Event Time的更新意味着该子任务将时间推进到了这个时间，该时间之前的事件已经被处理并发送到下游。上图中第二步和第三步均执行了这个过程。Partition Watermark列表更新后，导致列表中最小时间戳发生了变化，算子子任务的Event Time时钟也相应进行了更新。整个过程可以理解为：数据流中的Watermark推动算子子任务的Watermark更新。Watermark像一个幕后推动者，不断将流处理系统的Event Time向前推进。我们可以将这种机制总结为： Flink某算子子任务根据各上游流入的Watermark来更新Partition Watermark列表。 选取Partition Watermark列表中最小的时间作为该算子子任务的Event Time，并将这个时间发送给下游算子。这样的设计机制满足了并行环境下Watermark在各算子中的传播问题，但是假如某个上游分区的Watermark一直不更新，Partition Watermark列表其他地方都在正常更新，唯独个别分区的Watermark停滞，这会导致算子的Event Time时钟不更新，相应的时间窗口计算也不会被触发，大量的数据积压在算子内部得不到处理，整个流处理处于空转状态。这种问题可能出现在数据流自带Watermark的场景，自带的Watermark在某些分区下没有及时更新。针对这种问题，一种解决办法是根据机器当前的时钟，周期性地生成Watermark。此外，在union()等多数据流处理时，Flink也使用上述Watermark更新机制，那就意味着，多个数据流的时间必须对齐，如果一方的Watermark时间较老，那整个应用的Event Time时钟也会使用这个较老的时间，其他数据流的数据会被积压。一旦发现某个数据流不再生成新的Watermark，我们要在SourceFunction中的SourceContext里调用markAsTemporarilyIdle()设置该数据流为空闲状态，避免空转。3. 时间戳设置与Watermark生成至此，已经了解了Flink的Event Time和Watermark机制的大致工作原理，接下来我们将展示如何在代码层面设置时间戳并生成Watermark。因为时间在后续处理中都会用到，时间的设置要在任何时间窗口操作之前。总之，时间越早设置越好。对时间和Watermark的设置只对Event Time时间语义起作用，如果一个作业基于Processing Time或Ingestion Time，那设置时间没有什么意义。Flink提供了新老两种方法设置时间戳和Watermark。无论哪种方法，我们都需要明白，Event Time时间戳和Watermark是捆绑在一起的，一旦涉及到Event Time，就必须抽取时间戳并生成Watermark。3.1 Source我们可以在 Source 阶段完成时间戳抽取和 Watermark 生成的工作。Flink 1.11 开始推出了新的 Source 接口，并计划逐步替代老的 Source 接口，我们将在第七章展示两种接口的具体工作方式，这里暂时以老的 Source 接口来展示时间戳抽取和 Watermark 生成的过程。在老的 Source 接口中，通过自定义SourceFunction或RichSourceFunction，在SourceContext里重写void collectWithTimestamp(T element, long timestamp)和void emitWatermark(Watermark mark)两个方法，其中，collectWithTimestamp()给数据流中的每个元素 T 赋值一个timestamp作为 Event Time，emitWatermark()生成 Watermark。下面的代码展示了调用这两个方法抽取时间戳并生成 Watermark。class MyType { public double data; public long eventTime; public boolean hasWatermark; public long watermarkTime; ...}class MySource extends RichSourceFunction[MyType] { @Override public void run(SourceContext&amp;lt;MyType&amp;gt; ctx) throws Exception { while (/* condition */) { MyType next = getNext(); ctx.collectWithTimestamp(next, next.eventTime); if (next.hasWatermarkTime()) { ctx.emitWatermark(new Watermark(next.watermarkTime)); } } }}3.2 Source 之后如果我们不想修改 Source，也可以在 Source 之后，通过assignTimestampsAndWatermarks()方法来设置。与 Source 接口一样，Flink 1.11 重构了assignTimestampsAndWatermarks()方法，重构后的assignTimestampsAndWatermarks()方法和新的 Source 接口结合更好、表达能力更强，这里介绍一下重构后的assignTimestampsAndWatermarks()方法。新的assignTimestampsAndWatermarks()方法主要依赖WatermarkStrategy，通过WatermarkStrategy我们可以为每个元素抽取时间戳并生成 Watermark。assignTimestampsAndWatermarks()方法结合WatermarkStrategy的大致使用方式为：DataStream&amp;lt;MyType&amp;gt; stream = ...DataStream&amp;lt;MyType&amp;gt; withTimestampsAndWatermarks = stream .assignTimestampsAndWatermarks( WatermarkStrategy .forGenerator(...) .withTimestampAssigner(...) );可以看到WatermarkStrategy.forGenerator(...).withTimestampAssigner(...)链式调用了两个方法，forGenerator()方法用来生成 Watermark，withTimestampAssigner()方法用来为数据流的每个元素设置时间戳。withTimestampAssigner()方法相对更好理解，它抽取数据流中的每个元素的时间戳，一般是告知 Flink 具体哪个字段为时间戳字段。例如，一个MyType数据流中eventTime字段为时间戳，数据流的每个元素为event，使用 Lambda 表达式来抽取时间戳，可以写成：.withTimestampAssigner((event, timestamp) -&amp;gt; event.eventTime)。这个 Lambda 表达式可以帮我们抽取数据流元素中的时间戳eventTime，我们暂且可以不用关注第二个参数timestamp。基于 Event Time 时间戳，我们还要设置 Watermark 生成策略，一种方法是自己实现一些 Watermark 策略类，并使用forGenerator()方法调用这些 Watermark 策略类。我们曾多次提到，Watermark 是一种插入到数据流中的特殊元素，Watermark 元素包含一个时间戳，当某个算子接收到一个 Watermark 元素时，算子会假设早于这条 Watermark 的数据流元素都已经到达。那么如何向数据流中插入 Watermark 呢？Flink 提供了两种方式，一种是周期性地（Periodic）生成 Watermark，一种是逐个式地（Punctuated）生成 Watermark。无论是 Periodic 方式还是 Punctuated 方式，都需要实现WatermarkGenerator接口类，如下所示，T为数据流元素类型。// Flink源码// 生成Watermark的接口类@Publicpublic interface WatermarkGenerator&amp;lt;T&amp;gt; { // 数据流中的每个元素流入后都会调用onEvent()方法 // Punctunated方式下，一般根据数据流中的元素是否有特殊标记来判断是否需要生成Watermark // Periodic方式下，一般用于记录各元素的Event Time时间戳 void onEvent(T event, long eventTimestamp, WatermarkOutput output); // 每隔固定周期调用onPeriodicEmit()方法 // 一般主要用于Periodic方式 // 固定周期用 ExecutionConfig#setAutoWatermarkInterval() 方法设置 void onPeriodicEmit(WatermarkOutput output);}Periodic假如我们想周期性地生成 Watermark，这个周期是可以设置的，默认情况下是每 200 毫秒生成一个 Watermark，或者说 Flink 每 200 毫秒调用一次生成 Watermark 的方法。我们可以在执行环境中设置这个周期：// 每5000毫秒生成一个Watermarkenv.getConfig.setAutoWatermarkInterval(5000L)下面的代码定期生成 Watermark，数据流元素是一个Tuple2，第二个字段Long是 Event Time 时间戳。// 定期生成Watermark// 数据流元素 Tuple2&amp;lt;String, Long&amp;gt; 共两个字段// 第一个字段为数据本身// 第二个字段是时间戳public static class MyPeriodicGenerator implements WatermarkGenerator&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;&amp;gt; { private final long maxOutOfOrderness = 60 * 1000; // 1分钟 private long currentMaxTimestamp; // 已抽取的Timestamp最大值 @Override public void onEvent(Tuple2&amp;lt;String, Long&amp;gt; event, long eventTimestamp, WatermarkOutput output) { // 更新currentMaxTimestamp为当前遇到的最大值 currentMaxTimestamp = Math.max(currentMaxTimestamp, eventTimestamp); } @Override public void onPeriodicEmit(WatermarkOutput output) { // Watermark比currentMaxTimestamp最大值慢1分钟 output.emitWatermark(new Watermark(currentMaxTimestamp - maxOutOfOrderness)); }}我们用变量currentMaxTimestamp记录已抽取的时间戳最大值，每个元素到达后都会调用onEvent()方法，更新currentMaxTimestamp时间戳最大值。当需要发射 Watermark 时，以时间戳最大值减 1 分钟作为 Watermark 发送出去。这种 Watermark 策略假设 Watermark 比已流入数据的最大时间戳慢 1 分钟，超过 1 分钟的将被视为迟到数据。实现好MyPeriodicGenerator后，我们要用forGenerator()方法调用这个类：// 第二个字段是时间戳DataStream&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;&amp;gt; watermark = input.assignTimestampsAndWatermarks( WatermarkStrategy .forGenerator((context -&amp;gt; new MyPeriodicGenerator())) .withTimestampAssigner((event, recordTimestamp) -&amp;gt; event.f1));考虑到这种基于时间戳最大值的场景比较普遍，Flink 已经帮我们封装好了这样的代码，名为BoundedOutOfOrdernessWatermarks，其内部实现与上面的代码几乎一致，我们只需要将最大的延迟时间作为参数传入：// 第二个字段是时间戳DataStream&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;&amp;gt; input = env .addSource(new MySource()) .assignTimestampsAndWatermarks( WatermarkStrategy .&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;&amp;gt;forBoundedOutOfOrderness(Duration.ofSeconds(5)) .withTimestampAssigner((event, timestamp) -&amp;gt; event.f1));除了BoundedOutOfOrdernessWatermarks，另外一种预置的 Watermark 策略为AscendingTimestampsWatermarks。AscendingTimestampsWatermarks其实是继承了BoundedOutOfOrdernessWatermarks，只不过AscendingTimestampsWatermarks会假设 Event Time 时间戳单调递增，从内部代码实现上来说，Watermark 的发射时间为时间戳最大值，不添加任何延迟。使用时，可以参照下面的方式：// 第二个字段是时间戳DataStream&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;&amp;gt; input = env .addSource(new MySource()) .assignTimestampsAndWatermarks( WatermarkStrategy .&amp;lt;Tuple2&amp;lt;String, Long&amp;gt;&amp;gt;forMonotonousTimestamps() .withTimestampAssigner((event, timestamp) -&amp;gt; event.f1));Punctuated假如数据流元素有一些特殊标记，标记了某些元素为 Watermark，我们可以逐个检查数据流各元素，根据是否有特殊标记判断是否要生成 Watermark。下面的代码以一个Tuple3&amp;lt;String, Long, Boolean&amp;gt;为例，其中第二个字段是时间戳，第三个字段标记了是否为 Watermark。我们只需要在onEvent()方法中根据第三个字段来决定是否生成一条新的 Watermark，由于这里不需要周期性的操作，因此onPeriodicEmit()方法里不需要做任何事情。// 逐个检查数据流中的元素，根据元素中的特殊字段，判断是否要生成Watermark// 数据流元素 Tuple3&amp;lt;String, Long, Boolean&amp;gt; 共三个字段// 第一个字段为数据本身// 第二个字段是时间戳// 第三个字段判断是否为Watermark的标记public static class MyPunctuatedGenerator implements WatermarkGenerator&amp;lt;Tuple3&amp;lt;String, Long, Boolean&amp;gt;&amp;gt; { @Override public void onEvent(Tuple3&amp;lt;String, Long, Boolean&amp;gt; event, long eventTimestamp, WatermarkOutput output) { if (event.f2) { output.emitWatermark(new Watermark(event.f1)); } } @Override public void onPeriodicEmit(WatermarkOutput output) { // 这里不需要做任何事情，因为我们在 onEvent() 方法中生成了Watermark }}假如每个元素都带有 Watermark 标记，Flink 是允许为每个元素都生成一个 Watermark 的，但这种策略非常激进，大量的 Watermark 会增大下游计算的延迟，拖累整个 Flink 作业的性能。4. 平衡延迟和准确性至此，我们已经了解了 Flink 的 Event Time 和 Watermark 生成方法，那么具体如何操作呢？实际上，这个问题可能并没有一个标准答案。批处理中，数据都已经准备好了，不需要考虑未来新流入的数据，而流处理中，我们无法完全预知有多少迟到数据，数据的流入依赖业务的场景、数据的输入、网络的传输、集群的性能等等。Watermark 是一种在延迟和准确性之间平衡的策略：Watermark 与事件的时间戳贴合较紧，一些重要数据有可能被当成迟到数据，影响计算结果的准确性；Watermark 设置得较松，整个应用的延迟增加，更多的数据会先缓存起来以等待计算，会增加内存的压力。对待具体的业务场景，我们可能需要反复尝试，不断迭代和调整时间戳和 Watermark 策略。参考 Flink的时间语义 时间属性深度解析 《Flink原理、实战、性能优化》" }, { "title": "Socks5代理工作原理", "url": "/posts/Socks5%E4%BB%A3%E7%90%86%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/", "categories": "计算机理论", "tags": "计算机网络", "date": "2021-01-26 04:47:19 +0800", "snippet": "Socks5代理工作原理[TOC]1. Socks5协议以下摘自维基百科 SOCKS是一种网络传输协议，主要用于客户端与外网服务器之间通讯的中间传递。SOCKS是”SOCKetS”的缩写[注 1]。 当防火墙后的客户端要访问外部的服务器时，就跟SOCKS代理服务器连接。这个代理服务器控制客户端访问外网的资格，允许的话，就将客户端的请求发往外部的服务器。 这个协议最初由David Koblas开发，而后由NEC的Ying-Da Lee将其扩展到SOCKS4。最新协议是SOCKS5，与前一版本相比，增加支持UDP、验证，以及IPv6。 根据OSI模型，SOCKS是会话层的协议，位于表示层与传输层之间。 SOCKS协议不提供加密。SOCKS 协议第 4 版本为基于 TCP 协议的 C/S 应用，包括 TELNET, FTP 和 使用广泛的信息发现协议如 HTTP 、 WAIS 提供了不保证安全性的防火墙穿透服务。SOCKS 5 扩展了第 4 版本，加入了 UDP 协议支持，在框架上加入了强认证功能，并且地址信息也加入了域名和 IPV6 的支持。2. Socks5 协议交互过程 除非特别说明，包结构图里面的十进制数字代表该字段的长度（字节数）。给定的字段，必定要有确定的值，语法 X’hh’代表该单字节字段的值。’Variable’ 代表该字段为可变长度，其长度要么由对应的关联字段标识（通常为一到两个字节），要么由数据类型确定。当 TCP 客户端想要建立必须透过防火墙（取决于具体的情况）的连接时，客户端必须与合适的 SOCKS 服务建立连接。SOCKS 服务默认监听 1080 端口，如果连接成功，客户端需要与服务端协商认证方式并完成认证，之后便可以发送中继请求。SOCKS 服务端会执行请求，要么建立起合适的连接，要么拒绝请求。2.1 认证第一步，客户端向代理服务器发送代理请求，其中包含了代理的版本和认证方式： +----+----------+----------+ |VER | NMETHODS | METHODS | +----+----------+----------+ | 1 | 1 | 1 to 255 | +----+----------+----------+ VER：版本号 X’04’：Socks4协议 X’05’：Socks5协议 NMETHODS：方法数目，该字段包含了METHODS中锁包含了方法识别码的个数 METHOD ：方法列表第二步，代理服务器从给定的方法列表中选择一个方法并返回选择报文 +----+--------+ |VER | METHOD | +----+--------+ | 1 | 1 | +----+--------+如果 METHOD （方法）字段为 X’FF‘， 表示方法列表中的所有方法均不可用，客户端收到此信息必须关闭连接。目前已定义方法如下： X’00‘　　无需认证 X’01‘　　GSSAPI X’02‘　　用户名/密码 X’03‘　到　X’7F’　　IANA 指定 X’80‘　到　X’FE’　　为私有方法保留 X’FF‘　　无可接受方法随后，客户端与服务端开始协商该方法对应的后续认证，后续认证方法因方法而异，在此不进行展开。2.2请求第三步，一旦认证方法对应的协商完成，客户端就可以发送请求细节了。如果认证方法为了完整性或者可信性的校验，需要对后续请求报文进行封装，则后续请求报文都要按照对应规定进行封装。SOCKS请求格式如下 +----+-----+-------+------+----------+----------+ |VER | CMD | RSV | ATYP | DST.ADDR | DST.PORT | +----+-----+-------+------+----------+----------+ | 1 | 1 | X&#39;00&#39; | 1 | Variable | 2 | +----+-----+-------+------+----------+----------+字段含义： VER：协议版本： X‘05’ CMD：命令 CONNECT　连接， X‘01’ BIND　监听X‘02’ UDP ASSOCIATE　UDP关联 X‘03’ RSV：保留字段 ATYP：地址类型 X‘01’： 表明地址字段为一个 IPV4 地址，长度为 4 个字节 X‘03’ ：表明地址字段为一个（合法的）域名，且第一个字节为域名长度标识，（显然）其不以 NULL 作为结束标识 X‘04’ ：表明地址字段为一个 IPV6 地址，长度为 16 个字节 DST.ADDR：目标地址 DST.PORT目标端口 （网络字节序）SOCKS 服务端会根据请求类型和源、目标地址，执行对应操作，并且返回对应的一个或多个报文信息。第四步，回复报文，客户端与服务端建立连接并完成认证之后就会发送请求信息，服务端执行对应请求并返回如下格式的报文： +----+-----+-------+------+----------+----------+ |VER | REP | RSV | ATYP | BND.ADDR | BND.PORT | +----+-----+-------+------+----------+----------+ | 1 | 1 | X&#39;00&#39; | 1 | Variable | 2 | +----+-----+-------+------+----------+----------+ VER协议版本：　X‘05’ REP 回复字段（回复类型）： X‘00’　成功 X‘01’　常规 SOCKS 服务故障 X‘02’　规则不允许的连接 X‘03’　网络不可达 X‘04’　主机无法访问 X‘05’　拒绝连接 X‘06’　连接超时 X‘07’　不支持的命令 X‘08’　不支持的地址类型 X‘09’　到　X’FF’　未定义 RSV　保留字段 ATYP　地址类型 IPV4　X‘01’ 域名　X‘03’ IPV6　X‘04’ BND.ADDR　服务端绑定地址 BND.PORT　服务端绑定端口 （网络字节序）其中，标记为保留字段（ RSV ）的值必须设定为 X‘00’ 。如果协商的方法为了完整性、可信性的校验需要封装数据包，则返回的数据包也会进行对应的封装。2.3 通信当连接建立后，客户端就可以和正常一样访问服务端通信了，此时通信的数据除了目的地址是发往代理程序以外，所有内容都是和普通连接一模一样。对代理程序而言，后面所有收到的来自客户端的数据都会原样转发到服务读端。2.4 总结通信流程总结：3. 抓包验证最近学习了Netty，正好可以使用Netty写一个客户端和服务端用于进行验证程序。服务端使用的是Netty提供例子，客户端是自己开发的简单的Demo。不过不知道是什么原因，自己在使用Wireshark 进行抓包时，并没有自动识别为Socks协议，可能和我代理的内容有点关系，我请求的代理服务器 ORZ首先看前面三个数据包，很典型的 TCP 握手连接，服务端为本地 1098端口，源端口为本地 59992 端口。3.1 认证建立连接完毕之后，第 4 个数据包 就是Socks数据包了与文档一致，版本号 X‘05’ ，可选方法数目 X‘01’ （一种），方法列表此时显然只有一个字节，其值也对应为 X‘00’ 。根据文档显然其对应为 SOCKS 5 版本，客户端只有一种协商方法 —— 无需认证。紧随其后的第 5 个数据包为 TCP 的 ACK 包，跳过，第 6 个数据包如下：这一步也与文档一致，为协商认证方式的回复数据包，版本号 X‘05’ ，选择的认证方式为 X‘00’ 。可见无需认证，后续直接进行指令交互就可以了。3.2 请求第 7 个数据包为应用发送到 SOCKS 服务端的 TCP ACK 包，第 8 个数据包正如协议描述，为指令请求，抓包样例为 CONNECT 指令，具体如下图：只需要注意一下新碰到的字段： CMD 字段为 X‘01’ 表示 CONNECT 指令，保留字段确实为 X‘00’ ，地址类型为 X‘04’ 也就是IPV4，端口也是1098，其实在这里为了Demo方便，我在这里请求的我自己。由于我请求的我自己，所以中间又有几条自己请求自己的数据包最后第十一条是返回数据，其实这几个字段对照前面给的报文模板都能找到相应的解释，在这就不过多赘述4.总结​ 以上就是对Socks5协议解析的全部流程了，总的来说不算太复杂参考 SOCKS Protocol Version 5 RFC 1928 - SOCKS 5 协议中文文档 socks5代理工作流程和原理 " }, { "title": "Clickhouse - 数据迁移", "url": "/posts/Clickhouse-%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/", "categories": "数据库", "tags": "clickhouse", "date": "2021-01-20 00:29:39 +0800", "snippet": "背景​ 数据报表即将上线，需准备一个Clickhouse测试库用作后续开发方案调研迁移集群实际上就是要把所有数据库（system 除外）的表结构和数据完整的复制一遍。ClickHouse 官方和社区有一些现成的解决方案，也可以自己实现。拷贝数据目录先观察一下 ClickHouse 在文件系统上的目录结构（配置文件 /ect/clickhouse-server/config.xml 里面配置的 &amp;lt;path&amp;gt;），为了便于查看，只保留了 data 和 metadata 目录。.├── data│   ├── default│   ├── system│   │   ├── asynchronous_metric_log│   │   ├── metric_log│   │   ├── query_log│   │   ├── query_thread_log│   │   └── trace_log├── metadata│   ├── default│   │   └── v_table_size.sql│   ├── default.sql│   ├── system│   │   ├── asynchronous_metric_log.sql│   │   ├── metric_log.sql│   │   ├── query_log.sql│   │   ├── query_thread_log.sql│   │   └── trace_log.sql data 目录里保存的是数据，每个数据库一个目录，内部每个表一个子目录。 metadata 目录里保存的是元数据，即数据库和表结构。其中 &amp;lt;database&amp;gt;.sql 是 创建数据库的 DDL（ATTACH DATABASE default ENGINE = Ordinary） &amp;lt;database&amp;gt;/&amp;lt;table&amp;gt;.sql 是建表的 DDL (ATTACH TABLE ...). 这里的 DDL 使用的是 ATTACH 语句，进入文档 查看 ATTACH 的作用及跟 CREATE 的区别基于这个信息，直接把 data 和 metadata 目录（要排除 system）复制到新集群，即可实现数据迁移。用一个小表做测试，验证可行。操作流程 在源集群的硬盘上打包好对应数据库或表的 data 和 metadata 数据 拷贝到目标集群对应的目录 重启 clickhouse-server使用 remote 表函数ClickHouse 除了查询常规的表，还能使用表函数来构建一些特殊的「表」，其中 remote 函数 可用于查询另一个 ClickHouse 的表。使用方式很简单:SELECT * FROM remote(&#39;addresses_expr&#39;, db, table, &#39;user&#39;, &#39;password&#39;) LIMIT 10;因此，可以借助这个功能实现数据迁移：INSERT INTO &amp;lt;local_database&amp;gt;.&amp;lt;local_table&amp;gt;SELECT * FROM remote(&#39;remote_clickhouse_addr&#39;, &amp;lt;remote_database&amp;gt;, &amp;lt;remote_table&amp;gt;, &#39;&amp;lt;remote_user&amp;gt;&#39;, &#39;&amp;lt;remote_password&amp;gt;&#39;)操作流程 在源集群的 system.tables 表查询出数据库、表、DDL、分区、表引擎等信息 在目标集群上，运行 DDL 创建表，然后运行上述迁移语句复制数据 遍历所有表，执行 2使用 clickhouse-copierClickhouse-copier 是 ClickHouse 官方提供的一款数据迁移工具，可用于把表从一个集群迁移到另一个（也可以是同一个）集群。Clickhouse-copier 使用 Zookeeper 来管理同步任务，可以同时运行多个 clickhouse-copier 实例。使用方式:clickhouse-copier --daemon --config zookeeper.xml --task-path /task/path --base-dir /path/to/dir其中 --config zookeeper.xml 是 Zookeeper 的连接信息，--task-path /task/path 是 Zookeeper 里任务配置的节点路径。在使用时，需要先定义一个 XML 格式的任务配置文件，上传到 /task/path/description 里。同步任务是表级别的，可以配置的内容还比较多。Clickhouse-copier 可以监听 /task/path/description 的变化，动态加载新的配置而不需要重启。操作流程 创建 zookeeper.xml 创建任务配置文件，格式见官方文档，每个表都要配置（可使用代码自动生成） 把配置文件内容上传到 Zookeeper 启动 clickhouse-copier 进程理论上 clickhouse-copier 运行在源集群或目标集群的环境都可以，官方文档推进在源集群，这样可以节省带宽。使用 clickhouse-backupclickhouse-backup 是社区开源的一个 ClickHouse 备份工具，可用于实现数据迁移。其原理是先创建一个备份，然后从备份导入数据，类似 MySQL 的 mysqldump + SOURCE。这个工具可以作为常规的异地冷备方案，不过有个局限是只支持 MergeTree 系列的表。操作流程 在源集群使用 clickhouse-backup create 创建备份 把备份文件压缩拷贝到目标集群 在目标集群使用 clickhouse-backup restore 恢复对比拷贝数据目录使用 remote 表函数使用 clickhouse-copier使用 clickhouse-backup操作复杂度较麻烦，需要在两台服务器上操作文件系统并拷贝文件，不方便自动化一般，需要写程序自动化看起来比使用 remote 更复杂一些，主要是生成配置文件比较麻烦类似拷贝数据目录，会更简单一些全量同步支持支持支持支持增量同步不支持支持应该支持不支持迁移视图不支持支持不确定，理论上应该支持不支持性能较好较好不确定，应该比 remote 快不确定局限性不支持集群，很多人工操作不适合大表？应该需要相同的拓扑结构不确定，可能没有只支持 MergeTree 系列从官方和社区的一些资料综合来看 clickhouse-copier 功能最强大，不过考虑到数据量较少，而且对 clickhouse-copier 有些地方也不是很清楚，最终决定使用 remote 函数来做数据迁移。关于别的数据迁移方案、更多的 clickhouse-copier 使用案例，可参考 Altinity 的博客 Clickhouse-copier in practice.使用 remote 函数做数据迁移使用 remote 函数还能实现更多特性： 对于分区表，可逐个分区进行同步，这样实际上同步的最小单位是分区，可以实现增量同步 可方便集成数据完整性（行数对比）检查，自动重新同步更新过的表代码代码如下，需要先安装 clickhouse-driverimport collectionsimport datetimeimport functoolsimport loggingimport timefrom clickhouse_driver import Clientsource_conn = Client(host=&#39;source-host&#39;, user=&#39;user&#39;, password=&#39;password&#39;)target_conn = Client(host=&#39;target-host&#39;, user=&#39;user&#39;, password=&#39;password&#39;)def format_partition_expr(p): if isinstance(p, int): return p return f&quot;&#39;{p}&#39;&quot;def execute_queries(conn, queries): if isinstance(queries, str): queries = queries.split(&#39;;&#39;) for q in queries: conn.execute(q.strip())class Table(object): def __init__(self, database, name, ddl, partition_key, is_view): self.database = database self.name = name self.ddl = ddl.replace(&#39;CREATE TABLE&#39;, &#39;CREATE TABLE IF NOT EXISTS&#39;) self.partition_key = partition_key self.is_view = is_view def exists(self, conn): q = f&quot;SELECT name FROM system.tables WHERE database = &#39;{self.database}&#39; AND name = &#39;{self.name}&#39;&quot; return len(conn.execute(q)) &amp;gt; 0 def get_partitions(self, conn): partitions = [] q = f&#39;SELECT {self.partition_key}, count() FROM {self.identity} GROUP BY {self.partition_key} ORDER BY {self.partition_key}&#39; partitions = collections.OrderedDict(conn.execute(q)) return partitions def get_total_count(self, conn): q = f&#39;SELECT COUNT() FROM {self.identity}&#39; return conn.execute(q)[0][0] def check_consistency(self): if not self.exists(target_conn): return False, None source_ttl_count = self.get_total_count(source_conn) target_ttl_count = self.get_total_count(target_conn) if source_ttl_count == target_ttl_count: return True, None if not self.partition_key: return False, None source_partitions = self.get_partitions(source_conn) target_partitions = self.get_partitions(target_conn) bug_partitions = [] for p, c in source_partitions.items(): if p not in target_partitions or c != target_partitions[p]: bug_partitions.append(p) return False, bug_partitions def create(self, replace=False): target_conn.execute(f&#39;CREATE DATABASE IF NOT EXISTS {self.database}&#39;) if self.is_view: replace = True if replace: target_conn.execute(f&#39;DROP TABLE IF EXISTS {self.identity}&#39;) target_conn.execute(self.ddl) def copy_data_from_remote(self, by_partition=True): self.create() if self.is_view: logging.info(&#39;ignore view %s&#39;, self.identity) return is_identical, bug_partitions = self.check_consistency() if is_identical: logging.info(&#39;table %s has the same number of rows, skip&#39;, self.identity) return if self.partition_key and by_partition: for p in bug_partitions: logging.info(&#39;copy partition %s=%s&#39;, self.partition_key, p) self._copy_partition_from_remote(p) else: self._copy_table_from_remote() def _copy_table_from_remote(self): queries = f&#39;&#39;&#39; DROP TABLE {self.identity}; {self.ddl}; INSERT INTO {self.identity} SELECT * FROM remote(&#39;{source_conn.host}&#39;, {self.identity}, &#39;{source_conn.user}&#39;, &#39;{source_conn.password}&#39;) &#39;&#39;&#39; execute_queries(target_conn, queries) def _copy_partition_from_remote(self, partition): partition = format_partition_expr(partition) queries = f&#39;&#39;&#39; ALTER TABLE {self.identity} DROP PARTITION {partition}; INSERT INTO {self.identity} SELECT * FROM remote(&#39;{source_conn.host}&#39;, {self.identity}, &#39;{source_conn.user}&#39;, &#39;{source_conn.password}&#39;) WHERE {self.partition_key} = {partition} &#39;&#39;&#39; execute_queries(target_conn, queries) def copy_to_another_table(self, database, name=None): if not name: name = self.name assert not (self.database == database and self.name == name) if self.partition_key: partitions = self.get_partitions(target_conn) queries = [f&#39;CREATE TABLE IF NOT EXISTS {database}.{name} AS {self.identity}&#39;] for p in partitions.keys(): expr = format_partition_expr(p) queries.append(f&#39;ALTER TABLE {database}.{name} DROP PARTITION {expr}&#39;) queries.append(f&#39;ALTER TABLE {database}.{name} ATTACH PARTITION {expr} FROM {self.identity}&#39;) execute_queries(target_conn, queries) else: queries = f&#39;&#39;&#39; DROP TABLE IF EXISTS {database}.{name}; CREATE TABLE {database}.{name} AS {self.identity}; INSERT INTO {database}.{name} SELECT * FROM {self.identity}; &#39;&#39;&#39; execute_queries(target_conn, queries) @property def identity(self): return f&#39;{self.database}.{self.name}&#39; def __str__(self): return self.identity __repr__ = __str__def get_all_tables() -&amp;gt; [Table]: q = &#39;&#39;&#39; SELECT database, name, create_table_query, partition_key, engine = &#39;View&#39; AS is_view FROM system.tables WHERE database NOT IN (&#39;system&#39;) ORDER BY if(engine = &#39;View&#39;, 999, 0), database, name &#39;&#39;&#39; rows = source_conn.execute(q) tables = [Table(*values) for values in rows] return tablesdef copy_remote_tables(tables): for idx, t in enumerate(tables): start_time = datetime.datetime.now() logging.info(&#39;&amp;gt;&amp;gt;&amp;gt;&amp;gt; start to migrate table %s, progress %s/%s&#39;, t.identity, idx+1, len(tables)) t.copy_data_from_remote() logging.info(&#39;&amp;lt;&amp;lt;&amp;lt;&amp;lt; migrated table %s in %s&#39;, t.identity, datetime.datetime.now() - start_time)def with_retry(max_attempts=5, backoff=120): def decorator(f): @functools.wraps(f) def inner(*args, **kwargs): attempts = 0 while True: attempts += 1 logging.info(&#39;start attempt #%s&#39;, attempts) try: f(*args, **kwargs) except Exception as e: if attempts &amp;gt;= max_attempts: raise e logging.exception(&#39;caught exception&#39;) time.sleep(backoff) else: break return inner return decorator@with_retry(max_attempts=10, backoff=60)def main(): tables = get_all_tables() logging.info(&#39;got %d tables: %s&#39;, len(tables), tables) copy_remote_tables(tables)if __name__ == &#39;__main__&#39;: main()使用方式：直接运行即可，挂了重跑，不会有副作用。局限性​ 仅通过对比行数来判断数据同步完整，没有比较内部数据的一致性，因此如果上游表行数不变，更新了部分字段，将无法自动识别，需要先从目标库里把表删掉重新同步。​ 必须为两个相同的库，例如从阿里云Clickhouse迁移数据到本地基本就不可行。" }, { "title": "Clickhouse 常用命令", "url": "/posts/Clickhouse-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/", "categories": "数据库, clickhouse", "tags": "clickhouse", "date": "2020-12-21 00:00:00 +0800", "snippet": "数据表基本操作-- 追加新字段ALTER TABLE tb_name ADD COLUMN [IF NOT EXISTS] name [type] [default_expr] [AFTER name_after]ALTER TABLE testtable ADD COLUMN colum1 String DEFAULT &#39;defaultvalue&#39;;-- 修改字段类型ALTER TABLE tb_name MODIFY COLUMN [IF EXISTS] name [type] [default_expr];ALTER TABLE testtable MODIFY COLUMN age Int32;-- 修改备注ALTER TABLE tb_name COMMENT COLUMN [IF EXISTS] name &#39;some comment&#39;;ALTER TABLE testtable COMMENT COLUMN key &#39;主键ID&#39;;-- 删除已有字段ALTER TABLE tb_name DROP COLUMN [IF EXISTS] name;ALTER TABLE tb_name DROP COLUMN key;-- 修改数据表名称RENAME TABLE default.testcol_v1 TO db_test.testcol_v2分区操作-- 查询分区信息SELECT partition_id,name,table,database FROM system.parts WHERE table = &#39;partition_v2&#39;-- 删除指定分区ALTER TABLE tb_name DROP PARTITION partition_exprALTER TABLE testtable DROP PARTITION 201907查询数据库和表容量-- 查看数据库容量select sum(rows) as &quot;总行数&quot;, formatReadableSize(sum(data_uncompressed_bytes)) as &quot;原始大小&quot;, formatReadableSize(sum(data_compressed_bytes)) as &quot;压缩大小&quot;, round(sum(data_compressed_bytes) / sum(data_uncompressed_bytes) * 100, 0) &quot;压缩率&quot;from system.parts;-- 查询test表，2019年10月份的数据容量select table as &quot;表名&quot;, sum(rows) as &quot;总行数&quot;, formatReadableSize(sum(data_uncompressed_bytes)) as &quot;原始大小&quot;, formatReadableSize(sum(data_compressed_bytes)) as &quot;压缩大小&quot;, round(sum(data_compressed_bytes) / sum(data_uncompressed_bytes) * 100, 0) &quot;压缩率&quot;from system.parts -- 根据实际情况加查询条件 where table in(&#39;test&#39;) and partition like &#39;2019-10-%&#39; group by table; 查看和删除任务-- 这个命令和mysql是一样的show processlist；-- 如果进程太多，也可用通过查询系统表 processes，select * from system.processes;-- 指定主要关心字段select user,query_id,query,elapsed,memory_usagefrom system.processes;-- 通过上面指令获取到进程相关信息后，可以用query_id条件kill进程KILL QUERY WHERE query_id=&#39;2-857d-4a57-9ee0-327da5d60a90&#39; -- 杀死default用户下的所有进程KILL QUERY WHERE user=&#39;default&#39;– 参考 运维查看数据库及表容量 Clickhouse原理解析与应用全实践分析" }, { "title": "MongoDB 在集群模式下Count也真实数据量不一致", "url": "/posts/MongoDB-%E5%9C%A8%E9%9B%86%E7%BE%A4%E6%A8%A1%E5%BC%8F%E4%B8%8BCount%E4%B9%9F%E7%9C%9F%E5%AE%9E%E6%95%B0%E6%8D%AE%E9%87%8F%E4%B8%8D%E4%B8%80%E8%87%B4/", "categories": "数据库, MongoDB", "tags": "mongodb", "date": "2020-11-22 00:00:00 +0800", "snippet": "1. 背景在同步Clickhouse数据时，发现MongoDB数据量与Clickhouse数据量不一致，经同事提醒，可能是分片MongoDB集群Count不一致导致吗，于是Google查询相关资料2.相关信息通过查看官网发现中有解释这种现象的解释 On a sharded cluster, db.collection.count() can result in an inaccurate count if orphaned documentsexist or if a chunk migration is in progress. To avoid these situations, on a sharded cluster, use the $group stage of the db.collection.aggregate() method to $sum the documents. For example, the following operation counts the documents in a collection 官方文档解释了这种现象的原因以及解决方法：不准确的原因： 操作的是分片的集合（前提）； shard 分片正在做块迁移，导致有重复数据出现 存在孤立文档（因为不正常关机、块迁移失败等原因导致）解决方法使用聚合 aggregate 的方式查询 count 数量，shell 命令如下：db.collection.aggregate( [ { $group: { _id: null, count: { $sum: 1 } } } ])java 代码 所以在 Java 中也可以采用聚合的方式获取 count 结果，使用聚合 aggregate 的方式可以准确的获取 sharding 后的集合的 count 结果。DBObject groupFields = new BasicDBObject(&quot;\\_id&quot;, null);groupFields.put(&quot;count&quot;, new BasicDBObject(&quot;$sum&quot;, 1));BasicDBObject group = new BasicDBObject(&quot;$group&quot;, groupFields);List&amp;lt;BasicDBObject&amp;gt; aggreList = new ArrayList&amp;lt;BasicDBObject&amp;gt;();aggreList.add(group);AggregateIterable&amp;lt;Document&amp;gt; output = collection.aggregate(aggreList);for (Document dbObject : output){ System.out.println(&quot;Aggregates count = &quot;+ dbObject);}参考 MongoDB：count 结果不准确的原因与解决方法 官方文档 " }, { "title": "Java - Nio 基本概念&amp;操作", "url": "/posts/Java-Nio-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5&%E6%93%8D%E4%BD%9C/", "categories": "编程语言", "tags": "java, io", "date": "2020-09-25 19:41:17 +0800", "snippet": "1.概述从JDK1.4开始，Java提供了一系列改进的输入/输出处理的新特性，被统称为NIO(即New I/O)。新增了许多用于处理输入输出的类，这些类都被放在java.nio包及子包下，并且对原java.io包中的很多类进行改写，新增了满足NIO的功能。NIO采用内存映射文件的方式来处理输入输出，NIO将文件或文件的一段区域映射到内存中，这样就可以像访问内存一样访问文件了。Java NIO（New IO） 是从Java 1.4版本开始引入的一个新的IO API，可以替代标准的Java IO API。NIO与原来的IO有同样的作用和目的，但是使用的方式完全不同， NIO支持面向缓冲区的、基于通道的IO操作。 NIO将以更加高效的方式进行文件的读写操作。2.组件 Buffer （缓冲区） Channel（通道） Selector（选择器）2.1Buffer （缓冲区）2.1.1 Buffer 的基本概述缓冲区实际上是一个容器对象，更直接的说，其实就是一个数组，在NIO库中，所有数据都是用缓冲区处理的。在读取数据时，它是直接读到缓冲区中的； 在写入数据时，它也是写入到缓冲区中的；任何时候访问 NIO 中的数据，都是将它放到缓冲区中。而在面向流I/O系统中，所有数据都是直接写入或者直接将数据读取到Stream对象中。，如下图所示上面的图描述了从一个客户端向服务端发送数据，然后服务端接收数据的过程。客户端发送数据时，必须先将数据存入Buffer中，然后将Buffer中的内容写入通道。服务端这边接收数据必须通过Channel将数据读入到Buffer中，然后再从Buffer中取出数据来处理。在NIO中，所有的缓冲区类型都继承于抽象类Buffer，最常用的就是ByteBuffer，对于Java中的基本类型，基本都有一个具体Buffer类型与之相对应，它们之间的继承关系如下图所示：PS:可以看到出了Boolean 类型外，其它都有对应的Buffer.2.1.2 Buffer用法使用Buffer读写数据一般遵循以下四个步骤： 写入数据到Buffer 调用flip()方法 从Buffer中读取数据 调用clear()方法或者compact()方法当向buffer写入数据时，buffer会记录下写了多少数据。一旦要读取数据，需要通过flip()方法将Buffer从写模式切换到读模式。在读模式下，可以读取之前写入到buffer的所有数据。一旦读完了所有的数据，就需要清空缓冲区，让它可以再次被写入。有两种方式能清空缓冲区：调用clear()或compact()方法。clear()方法会清空整个缓冲区。compact()方法只会清除已经读过的数据。任何未读的数据都被移到缓冲区的起始处，新写入的数据将放到缓冲区未读数据的后面。2.1.3 Buffer中的capacity,position和limitflip()/clear()/mark()/reset()等方法主要是对Buffer内capacity,position和limit的这三个属性进行操作capacity作为一个内存块，Buffer有一个固定的大小值，也叫“capacity”.你只能往里写capacity个byte、long，char等类型。一旦Buffer满了，需要将其清空（通过读数据或者清除数据）才能继续写数据往里写数据。position当你写数据到Buffer中时，position表示当前的位置。初始的position值为0.当一个byte、long等数据写到Buffer后， position会向前移动到下一个可插入数据的Buffer单元。position最大可为capacity – 1.当读取数据时，也是从某个特定位置读。当将Buffer从写模式切换到读模式，position会被重置为0. 当从Buffer的position处读取数据时，position向前移动到下一个可读的位置。limit在写模式下，Buffer的limit表示你最多能往Buffer里写多少数据。 写模式下，limit等于Buffer的capacity。当切换Buffer到读模式时， limit表示你最多能读到多少数据。因此，当切换Buffer到读模式时，limit会被设置成写模式下的position值。换句话说，你能读到之前写入的所有数据（limit被设置成已写数据的数量，这个值在写模式下就是position）以一次读写为例初始时position 0，limit、capacity相等写入数据，每写一个数据，position都会+1此时需要读取数据，首先调用flip()方法flip方法会将limit赋值为position，position置零此时进行读取数据时，数据每读取一个，position都会+1，知道limit的位置2.1.4 Buffer操作写入到Bufferint bytesRead = inChannel.read(buf); //read into buffer.byte aByte = buf.get();flip()方法flip方法将Buffer从写模式切换到读模式。调用flip()方法会将position设回0，并将limit设置成之前position的值。换句话说，position现在用于标记读的位置，limit表示之前写进了多少个byte、char等 —— 现在能读取多少个byte、char等。从Buffer中读取数据从Buffer中读取数据有两种方式： 从Buffer读取数据到Channel。 使用get()方法从Buffer中读取数据。//read from buffer into channel.int bytesWritten = inChannel.write(buf);// directly getbyte aByte = buf.get();rewind()方法Buffer.rewind()将position设回0，所以你可以重读Buffer中的所有数据。limit保持不变，仍然表示能从Buffer中读取多少个元素（byte、char等）。clear()与compact()方法一旦读完Buffer中的数据，需要让Buffer准备好再次被写入。可以通过clear()或compact()方法来完成。如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。如果Buffer中有一些未读的数据，调用clear()方法，数据将“被遗忘”，意味着不再有任何标记会告诉你哪些数据被读过，哪些还没有。如果Buffer中仍有未读的数据，且后续还需要这些数据，但是此时想要先先写些数据，那么使用compact()方法。compact()方法将所有未读的数据拷贝到Buffer起始处。然后将position设到最后一个未读元素正后面。limit属性依然像clear()方法一样，设置成capacity。现在Buffer准备好写数据了，但是不会覆盖未读的数据。mark()与reset()方法通过调用Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用Buffer.reset()方法恢复到这个position。例如：buffer.mark();//call buffer.get() a couple of times, e.g. during parsing.buffer.reset(); //set position back to mark.Scatter/Gather 分散（scatter）从Channel中读取是指在读操作时将读取的数据写入多个buffer中。因此，Channel将从Channel中读取的数据“分散（scatter）”到多个Buffer中。 聚集（gather）写入Channel是指在写操作时将多个buffer的数据写入同一个Channel，因此，Channel 将多个Buffer中的数据“聚集（gather）”后发送到Channel。ByteBuffer header = ByteBuffer.allocate(128);ByteBuffer body = ByteBuffer.allocate(1024);//write data into buffersByteBuffer[] bufferArray = { header, body };channel.write(bufferArray);2.2 Channel （通道）Channel和传统IO中的Stream很相似。虽然很相似，但是有很大的区别，主要区别为：通道是双向的，通过一个Channel既可以进行读，也可以进行写；而Stream只能进行单向操作，通过一个Stream只能进行读或者写，比如InputStream只能进行读取操作，OutputStream只能进行写操作；通道是一个对象，通过它可以读取和写入数据，当然了所有数据都通过Buffer对象来处理。我们永远不会将字节直接写入通道中，相反是将数据写入包含一个或者多个字节的缓冲区。同样不会直接从通道中读取字节，而是将数据从通道读入缓冲区，再从缓冲区获取这个字节。在Java NIO中几个重要的Channel实现： FileChannel：FileChannel 从文件中读写数据。 DatagramChannel：DatagramChannel 能通过UDP读写网络中的数据。 SocketChannel：SocketChannel 能通过TCP读写网络中的数据。 ServerSocketChannel：ServerSocketChannel可以监听新进来的TCP连接，像Web服务器那样。对每一个新进来的连接都会创建一个SocketChannel。DemoRandomAccessFile aFile = new RandomAccessFile(&quot;data/nio-data.txt&quot;, &quot;rw&quot;);FileChannel inChannel = aFile.getChannel();ByteBuffer buf = ByteBuffer.allocate(48);int bytesRead = inChannel.read(buf);while (bytesRead != -1) {System.out.println(&quot;Read &quot; + bytesRead);buf.flip();while(buf.hasRemaining()){System.out.print((char) buf.get());}buf.clear();bytesRead = inChannel.read(buf);}aFile.close();2.3 Selector （选择器）Selector类是NIO的核心类，Selector能够检测多个注册的通道上是否有事件发生，如果有事件发生，便获取事件然后针对每个事件进行相应的响应处理。这样一来，只是用一个单线程就可以管理多个通道，也就是管理多个连接。这样使得只有在连接真正有读写事件发生时，才会调用函数来进行读写，就大大地减少了系统开销，并且不必为每个连接都创建一个线程，不用去维护多个线程，并且避免了多线程之间的上下文切换导致的开销。3. 总结这里只是介绍了NIO的一个基本概念和一些基础操作，更多的使用可以参考Java NIO系列教程参考 Java NIO系列教程 JAVA NIO学习一：NIO简介、NIO&amp;amp;IO的主要区别 " }, { "title": "字符编码与字符串表达式", "url": "/posts/%E5%AD%97%E7%AC%A6%E7%BC%96%E7%A0%81%E4%B8%8E%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%A1%A8%E8%BE%BE%E5%BC%8F/", "categories": "编程语言", "tags": "编码", "date": "2020-08-14 06:32:26 +0800", "snippet": "字符编码与字符串表达式背景 最近在看基本类型时，发现char类型是2个字节，也就是16bit，最多只能表达2^16的字符，显然字符是不止这么多的，也就意味着在Java中使用char可能存在精度丢失，且String中底层同样是用char[]进行来进行维护的，会不会同样存在丢失的问题呢？字符编码简单来说，字符编码的本质是建立整数和字符的映射。从而使得字符可以在计算机内以整数的形式表示，方便传输。比如，我们可以定义 ‘a’ = 1，’b’ = 2，’c’ = 3，就是在进行字符编码。当然，我们自己定的标准没人会遵守。国际上对字符编码的标准主要有两个，分别是 ASCII 和 Unicode，由于 Unicode 是 ASCII 的超集，所以 Unicode 是事实上的字符编码国际标准。所以这意味着什么呢？意味着，每个整数都可能代表一个字符，所以对于字符来说，整数本身就是一种资源，开发完就没有了。ASCIIASCII（American Standard Code for Information Interchange，美国信息交换标准代码）是最早的通行标准，规定了 0-127 的对应的字符。这里节选了一部分。UnicodeUnicode 中文翻译也叫统一码、万国码、单一码。Unicode 首先承认了 ASCII 占用 0-127 整数资源的合法性，之后又一次占用了 128-65535 的整数资源，有了这么多的整数资源，我们就可以把世界各种文字的每一种字符分配一个整数来表示了。比如‘中’这个字符，Unicode 就把整数 20013（十六进制表示为 4E2D） 分配给他了。之后，Unicode 联盟发现 65536 个整数也不够分配的，于是就索性一次性又把之后的 16 个 65536 的数字即 65536-1114111 的整数资源给占了，然后把多占的 16 个 65536 的段分别命名为 16 个平面，加上原来的 0-65535 平面，Unicode 总共有 17 个平面。比如第 1 平面就是 65536-131072。当然，到目前为止，还只分配了 7 个平面出去。65535 之后分配的字符大多数是 emoji 表情，比如 😺 是 127850（1F36A）所以，重点是什么呢？重点就是 Unicode 没有所谓的占用多少字节一说。因为 Unicode 本质上是整数，问你 Unicode 占用多少个字节，就等于问你存整数占用多少个字节。我们要用多少字节表示整数，完全取决于整数本身是多大。比如现在 int 是 32 位，可以存 0 - 2^31 这么多整数。而 long 则可以存 0-2^63 这么多。理论上，对于 17 个平面的 Unicode 要完整一次性表达，我们需要 20 位就可以了，如果只要表达一个 Unicode 平面，则只要 16 位。如果只要表达前 128 位，则只需要 8 位空间。字符串表达我们前面知道了字符编码是字符对数字的映射，那么，我们要怎么表达一个字符串呢？char[]在内存中，一般通过 char 数组 来保存字符串的每个字符。每个 char 就是对应一个 Unicode 整数，然而，不同语言对于 char 的长度规定却不一样，比如 Java 定义 char 只有 16 位，所以只能表达 Unicode 0-65535 之间的字符，后面的字符就无法表示了。Java 的 String 也是基于 char[] ，那么是不是意味着 Java 的 String 不能含有 65535 之后的 Unicode 字符呢？不是的。Java 在处理字符串 String 时，并不是完全按照原始的 char[] 来保存每个字符，对于 65535 之后的字符会启用两个 char 对应一个字符。所以，正确遍历 Java String 的方法是用 String#codePoints() ，Java 把所有字符串转换成了一个 IntStream，所以 String 的底层虽然是 char[]，但是实际上，你可以把它理解为 int[] 。所以，往 String 里面存取 65535 之后的字符是没有问题的。但是你如果直接用 String#toCharArray 就有大问题，因为有的字符实际上用了两个 char 来表示。StringBuffer sb = new StringBuffer();sb.append(Character.toChars(127850));System.out.println(sb); // 输出 😺定长组合分割数组的方式一般只能在内存中使用，我们要传输或保存一个字符串，则需要转成字节流的格式。我们假设定义一个编码标准 ‘a’ = 0，’b’ = 1，’c’ = 10，那我要表达 abc ，最无误的方法是用数组[0,1,10] 。要转成字节流，一种自然的方法是直接拼成 0110，但是到时候再想变回数组的时候，就无法正确分割了。对于这个问题，有定长和不定长两种思路。定长的思路就是先规定我一次截取多少个字节作为一个字符，比如对于上面的例子，我规定这个分割长度为 2，那么，在组合时，应该拼成 000110 ，就可以直接把原来的 [0,1,10] 读取出来。UTF-16UTF-16 直接规定这个分割长度为每字符16 位，所以，这意味着只能表示 0-65535 的 Unicode 字符，之后的就不能表达了。比如 “中国”的 Unicode 分别是 20013 和 22269我们用 UTF-16 就是把上面的十进制转成 16 位的二进制，直接拼接在一起，读取的时候一次读 16 位01001110 00101101 01010110 11111101GBKGBK 全称汉字内码扩展规范，和 UTF-16 很像，也是以 16 位为单位进行合并和切割，但是，除了 0-127 继承了 ASCII 外，具体的 128-65535 的数字分配和 Unicode 则完全不一致（毕竟要有中国特色）。所以，只用分配中日韩文字的话，那就随便我们怎么玩都行，只要不超过 65536 个，都没有问题。比如“中国”，GBK 码分别是 54992 47610 ，转换二进制后和 UTF-16 格式一致。不定长 UTF-8定长组合分割优点是简单，缺点是需要定义一个单位长度，在表示 ASCII 的时候会补很多个 0 浪费空间。而且无法应对将来数字长度扩容时候错误分割的问题。这就是 UTF-8 为什么诞生的原因。UTF-8 直接用开头几位告诉你这个整数的位数，再把整数自身告诉你，这样就可以应对 Unicode 扩容的问题。同时，又可以减少占位符 0 的使用。UTF-8 已经事实上成为字符串表达的通用标准。因为他可以适应 Unicode 的变化。提供可伸缩的表达方法。具体的规则如下：1）对于单字节的符号，字节的第一位设为 0，后面 7 位为这个符号的 unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。2）对于 n 字节的符号（n&amp;gt;1），第一个字节的前 n 位都设为 1，第 n+1 位设为 0，后面字节的前两位一律设为 10。剩下的没有提及的二进制位，全部为这个符号的 unicode 码。这还跟 TCP 传输字节流的分片原理有点像。具体的规则如下： Unicode 符号范围 UTF-8 编码方式 （二进制） 0-127 0xxxxxxx 128-2047 110xxxxx 10xxxxxx 2048-65535 1110xxxx 10xxxxxx 10xxxxxx 65536-1114111 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx xxx 则是该字符的整数二进制表示。如：严的 Unicode 是 20005 （4E25）（100111000100101），根据上表，可以发现20005处在第三行的范围内，因此严的 UTF-8 编码需要三个字节，即格式是1110xxxx 10xxxxxx 10xxxxxx。然后，从严的最后一个二进制位开始，依次从后向前填入格式中的x，多出的位补0。这样就得到了，严的 UTF-8 编码是11100100 10111000 10100101。对单字符进行转换之后，字符串传输的时候直接拼接即可，切割的时候则先读取第一位的 1 的数量，来判断后面多少字节都是同一个字的，再进行切割。这样，如果中间有漏字符，也可以发现。比如 “中国”的 UTF-8 表示为：11100100 10111000 10101101 11100101 10011011 10111101其实你可以发现，因为 UTF-8 加入了位数提示，所以会占用更多的长度来表达字符串。比如中文通常是 2048-65535 之间，所以一个中文在 UTF-8 会占用 3 个 8 位（3 字节）。而更加节约的 UTF-16 只用占用 2 个字节。但是 UTF-8 可以无误的表达 65535 之后的字符，这是 UTF-16 和 GBK 无法做到的。在过去的标准里，UTF-8 最多可以用 6 个 8 位（6 字节）表示表示一个字符，然而 Unicode 也只能表示到 1114111，所以 UTF-8 也只需 4 位就足够了。另外，因为用到 65536 之后的机会并不多。一些数据库 ，比如 Mysql，默认储存 UTF-8 时，就只给每个字符留了最多 3 位的空间。后面 Emoji 兴起后，Mysql 为了兼容之前的版本，不得不新增了一个数据类型 utf8mb4 来支持 4 位的 UTF8，这个功能在 Mysql 5.5.3 中加入。我们应该优先设定 Mysql 数据类型为 utf8mb4参考 谈谈字符编码：Unicode、UTF-8 和 char[]" }, { "title": "Java - 浮点数机制及所存在的问题", "url": "/posts/Java-%E6%B5%AE%E7%82%B9%E6%95%B0%E6%9C%BA%E5%88%B6%E5%8F%8A%E6%89%80%E5%AD%98%E5%9C%A8%E7%9A%84%E9%97%AE%E9%A2%98/", "categories": "编程语言", "tags": "java", "date": "2020-08-14 06:32:26 +0800", "snippet": "0. 背景 总所周知，即使是小朋友也知道0.1+0.2 = 0.3肯定是正确的，但是在Java中，如果输入 0.1+0.2 == 0.3，返回的却是false 在Java中，如果你动手尝试输入 0.1+0.2，可以看到返回的值是0.30000000000000004，至于为什么会发生这样的事情，这便是后面要探讨的了——Java浮点数机制。1. Java浮点数机制 通过查阅资料可以发现，现在很多主流的语言对浮点数的实现都是采用的IEEE 754，其中这些语言中也包含Java，要了解Java的浮点数机制，也就得了解IEEE 754是如何定义浮点数的 IEEE 浮点数标准是从逻辑上用三元组{S，E，M}来表示一个数 V 的，即 V=（-1）S×M×2^E 上图分别表示了不同精度的浮点数 其中：符号位 s（Sign）决定数是正数（s＝0）还是负数（s＝1），而对于数值 0 的符号位解释则作为特殊情况处理。 有效数字位 M（Significand）是二进制小数，它的取值范围为 1~2-ε，或者为 0~1-ε。它也被称为尾数位（Mantissa）、系数位（Coefficient），甚至还被称作小数。 指数位 E（Exponent）是 2 的幂（可能是负数），它的作用是对浮点数加权。 类型（type） 符号位（sign） 指数位（biased exponent） 有效数位（normalised mantisa） 偏值（bias） 单精度（Float） 1（31st bit） 8（30-23） 23（22-0） 127 双精度（Double 1（63st bit） 11（62-52） 52（51-0） 1023 下面用几个例子来做示范// 原始值85.125// 转换成二进制形式85 = 10101010.125 = 00185.125 = 1010101.001 =1.010101001 x 2^6 // 正数sign = 0 // 在单精度中的表现形式// 指数位，因为需要用8位指数来表示正负两种情况，所以这里需要用6+偏值biased exponent = 127+6 = 133133 = 10000101Normalised mantisa = 010101001 //后面将会自动补0到23位长度// 所以在IEEE 754中该数的单精度的表示0 10000101 01010100100000000000000// 转换为十六进制 42AA4000// 在双精度中的表现形式biased exponent = 1023+6=10291029 = 10000000101Normalised mantisa = 010101001 //后面将会自动补0到52位长度// 所以在IEEE 754中该数的双精度的表示0 10000000101 0101010010000000000000000000000000000000000000000000// 转换为十六进制 40554800000000003. 为什么0.1+0.2 ！= 0.3 知道了在Java中的浮点数运行机制后，再来解决这个问题就很好办了// 第一步求出0.1的二进制形式0.1 x 2 = 0.2 00.2 x 2 = 0.4 00.4 x 2 = 0.8 00.8 x 2 = 1.6 10.6 x 2 = 1.2 10.2 x 2 = 0.4 0.....// 所以最后计算出来0.1的二进制表现形式为一个无限循环小数0.1 = 0.000110011001100.... x 2^0// 使用IEEE754 来表示1.10011 ... x 2^（-4）0 1.1001100110011001100110 01111011// 0.2 的最终表现形，指数位+1即可0 1.1001100110011001100110 01111100// 所以最后的0.30 1.0011001100110011001100 01111101及0 01111101 00110011001100110011001// 再将次数转成二进制是就成了0.30000000000000004所以0.1+0.2在Java中并不等于0.3" }, { "title": "RabitMQ&amp;Java简单使用教程", "url": "/posts/RabitMQ&Java%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/", "categories": "框架, rabbitmq", "tags": "rabbitmq, java", "date": "2020-06-30 00:00:00 +0800", "snippet": "RabbitMQ&amp;amp;Java使用说明RabbitMQ简介RabbitMQ是实现了高级消息队列协议（AMQP）的开源消息代理软件（亦称面向消息的中间件）。RabbitMQ服务器是用Erlang语言编写的，而群集和故障转移是构建在开放电信平台框架上的。所有主要的编程语言均有与代理接口通讯的客户端库。RabbitMQ安装docker一键安装# 拉去镜像（后缀为management表示为带图形化管理界面的版本）docker pull docker.io/rabbitmq:3.8-management# 启动镜像docker run -d --name rabbitmq3.7.7 -p 5672:5672 -p 15672:15672 -v `pwd`/data:/var/lib/rabbitmq --hostname myRabbit -e RABBITMQ_DEFAULT_VHOST=my_vhost -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin [ent_id]# -d 后台运行容器；# --name 指定容器名；# -p 指定服务运行的端口（5672：应用访问端口；15672：控制台Web端口号）；# -v 映射目录或文件；# --hostname 主机名（RabbitMQ的一个重要注意事项是它根据所谓的 “节点名称” 存储数据，默认为主机名）；# -e 指定环境变量；（RABBITMQ_DEFAULT_VHOST：默认虚拟机名；RABBITMQ_DEFAULT_USER：默认的用户名；RABBITMQ_DEFAULT_PASS：默认用户名的密码）RabbitMQ中的五种队列 Simplest Queue Work Queue Publish/Subscibe Routing Topics导入依赖&amp;lt;dependency&amp;gt; &amp;lt;groupId&amp;gt;com.rabbitmq&amp;lt;/groupId&amp;gt; &amp;lt;artifactId&amp;gt;amqp-client&amp;lt;/artifactId&amp;gt; &amp;lt;version&amp;gt;3.4.1&amp;lt;/version&amp;gt;&amp;lt;/dependency&amp;gt;SimplestQueue（简单队列） p(producing):生产者，用于生产消息并推送到队列中 红色：消息队列，用于缓存生产者推送的消息，消费者可以从中取出消息 c(Consuming):消费者，读取队列中的消息代码工具方法码//用于返回一个连接public static Connection getConnection() throws Exception { //定义连接工厂 ConnectionFactory factory = new ConnectionFactory(); //设置服务地址 factory.setHost(&quot;localhost&quot;); //端口 factory.setPort(5672); //设置账号信息，用户名、密码、vhost factory.setVirtualHost(&quot;testhost&quot;); factory.setUsername(&quot;admin&quot;); factory.setPassword(&quot;admin&quot;); // 通过工程获取连接 Connection connection = factory.newConnection(); return connection; }生产者// 获取到连接以及mq通道Connection connection = ConnectionUtil.getConnection();// 从连接中创建通道Channel channel = connection.createChannel();// 声明（创建）队列channel.queueDeclare(QUEUE_NAME, false, false, false, null);// 消息内容String message = &quot;Hello World!&quot;;channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes());System.out.println(&quot; [x] Sent &#39;&quot; + message + &quot;&#39;&quot;);//关闭通道和连接channel.close();connection.close();消费者// 获取到连接以及mq通道Connection connection = ConnectionUtil.getConnection();// 从连接中创建通道Channel channel = connection.createChannel();// 声明队列channel.queueDeclare(QUEUE_NAME, false, false, false, null);// 定义队列的消费者QueueingConsumer consumer = new QueueingConsumer(channel);// 监听队列channel.basicConsume(QUEUE_NAME, true, consumer);// 获取消息while (true) { QueueingConsumer.Delivery delivery = consumer.nextDelivery(); String message = new String(delivery.getBody()); System.out.println(&quot; [x] Received &#39;&quot; + message + &quot;&#39;&quot;);}Work Queue 一个生产者、两个消费者 一条消息只能被一个消费者读取生产者String QUEUE_NAME = &quot;test_queue_work&quot;; // 获取到连接以及mq通道Connection connection = ConnectionUtil.getConnection();Channel channel = connection.createChannel();// 声明队列channel.queueDeclare(QUEUE_NAME, false, false, false, null);for (int i = 0; i &amp;lt; 100; i++) { // 消息内容 String message = &quot;&quot; + i; channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes()); System.out.println(&quot; [x] Sent &#39;&quot; + message + &quot;&#39;&quot;); Thread.sleep(i * 10);}channel.close();connection.close();消费者X2String QUEUE_NAME = &quot;test_queue_work&quot;;// 获取到连接以及mq通道Connection connection = ConnectionUtil.getConnection();Channel channel = connection.createChannel();// 声明队列channel.queueDeclare(QUEUE_NAME, false, false, false, null);// 同一时刻服务器只会发一条消息给消费者//channel.basicQos(1);// 定义队列的消费者QueueingConsumer consumer = new QueueingConsumer(channel);// 监听队列，false表示手动返回完成状态，true表示自动channel.basicConsume(QUEUE_NAME, true, consumer);// 获取消息while (true) { QueueingConsumer.Delivery delivery = consumer.nextDelivery(); String message = new String(delivery.getBody()); System.out.println(&quot; [y] Received &#39;&quot; + message + &quot;&#39;&quot;); // 返回确认状态，注释掉表示使用自动确认模式 //channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false);}两种分发模式 轮询分发 ：使用任务队列的优点之一就是可以轻易的并行工作。如果我们积压了好多工作，我们可以通过增加工作者（消费者）来解决这一问题，使得系统的伸缩性更加容易。在默认情况下，RabbitMQ将逐个发送消息到在序列中的下一个消费者(而不考虑每个任务的时长等等，且是提前一次性分配，并非一个一个分配)。平均每个消费者获得相同数量的消息。这种方式分发消息机制称为Round-Robin（轮询）。 公平分发 ：虽然上面的分配法方式也还行，但是有个问题就是：比如：现在有2个消费者，所有的奇数的消息都是繁忙的，而偶数则是轻松的。按照轮询的方式，奇数的任务交给了第一个消费者，所以一直在忙个不停。偶数的任务交给另一个消费者，则立即完成任务，然后闲得不行。而RabbitMQ则是不了解这些的。这是因为当消息进入队列，RabbitMQ就会分派消息。它不看消费者为应答的数目，只是盲目的将消息发给轮询指定的消费者。 默认情况下是使用的轮询分发模式。将上述代码注释移除，并将channel.basicConsume(QUEUE_NAME, false, consumer);设置为false，则会采用公平分发Publish/Subscibe（订阅模式） 一个生产者，多个消费者 每个消费者都有自己的队列 生产者没有将消息直接发送到队列，而是发送到了交换机 每个队列都要绑定到交换机 生产者发送的消息，经过交换机到达队列，实现一个消息被多个消费者获取的目的PS：一个消费者队列可以有多个消费者实例，只有其中一个消费者实例会消费生产者// 交换机名称String EXCHANGE_NAME = &quot;test_exchange_fanout&quot;;// 获取到连接以及mq通道Connection connection = ConnectionUtil.getConnection();Channel channel = connection.createChannel();// 声明exchangechannel.exchangeDeclare(EXCHANGE_NAME, &quot;fanout&quot;);// 消息内容String message = &quot;Hello World!&quot;;channel.basicPublish(EXCHANGE_NAME, &quot;&quot;, null, message.getBytes());System.out.println(&quot; [x] Sent &#39;&quot; + message + &quot;&#39;&quot;);channel.close();connection.close();PS:消息发送到没有队列绑定的交换机时，消息将丢失，因为，交换机没有存储消息的能力，消息只能存在在队列中。消费者//另一个消费则需要将队列名称换成另外一个例如test_queue_work2，其他代码相同String QUEUE_NAME = &quot;test_queue_work1&quot;;String EXCHANGE_NAME = &quot;test_exchange_fanout&quot;;// 获取到连接以及mq通道Connection connection = ConnectionUtil.getConnection();Channel channel = connection.createChannel();// 声明队列channel.queueDeclare(QUEUE_NAME, false, false, false, null);// 绑定队列到交换机channel.queueBind(QUEUE_NAME, EXCHANGE_NAME, &quot;&quot;);// 同一时刻服务器只会发一条消息给消费者channel.basicQos(1);// 定义队列的消费者QueueingConsumer consumer = new QueueingConsumer(channel);// 监听队列，手动返回完成channel.basicConsume(QUEUE_NAME, false, consumer);// 获取消息while (true) { QueueingConsumer.Delivery delivery = consumer.nextDelivery(); String message = new String(delivery.getBody()); System.out.println(&quot; [Recv] Received &#39;&quot; + message + &quot;&#39;&quot;); Thread.sleep(10); channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false);}Routing（路由模式） 在Publish/Subscibe模式中，所有的消息均会发送到所有的消费者，但是目前有这样一个场景，所有的日志记录必须发送到消费者A，用于记录消息，但是只有错误的日志需要发送到消费者B，这是就需Exchange有过滤功能 在Routing模式下，就可以实现这个功能生产者Connection connection = ConnectUtils.getConnection();Channel channel = connection.createChannel();//声明Exchangechannel.exchangeDeclare(EXCHANGE_NAME,&quot;direct&quot;);//分别发送两条消息到&quot;delete&quot;、&quot;insert&quot;渠道channel.basicPublish(EXCHANGE_NAME,&quot;delete&quot;,null,&quot;删除商品&quot;.getBytes());channel.basicPublish(EXCHANGE_NAME,&quot;insert&quot;,null,&quot;插入商品&quot;.getBytes());channel.close();connection.close();消费者AConnection connection = ConnectUtils.getConnection();Channel channel = connection.createChannel();channel.queueDeclare(QUEUE_NAME1,false,false,false,null);//绑定到交换机，接受&quot;insert&quot;、&quot;delete&quot;两个渠道的消息，也就是最终结果会受到两条消息channel.queueBind(QUEUE_NAME1,EXCHANGE_NAME,&quot;insert&quot;);channel.queueBind(QUEUE_NAME1,EXCHANGE_NAME,&quot;delete&quot;);channel.basicQos(1);QueueingConsumer consumer = new QueueingConsumer(channel);// 监听队列，手动返回完成channel.basicConsume(QUEUE_NAME1, false, consumer);// 获取消息while (true) { QueueingConsumer.Delivery delivery = consumer.nextDelivery(); String message = new String(delivery.getBody()); System.out.println(&quot; [Recv1] Received &#39;&quot; + message + &quot;&#39;&quot;); Thread.sleep(10); channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false);}消费者BConnection connection = ConnectUtils.getConnection();Channel channel = connection.createChannel();channel.queueDeclare(QUEUE_NAME2,false,false,false,null);//绑定到交换机，只绑定了&quot;delete&quot;渠道，也就是只会受到一条消息channel.queueBind(QUEUE_NAME2,EXCHANGE_NAME,&quot;delete&quot;);channel.basicQos(1);QueueingConsumer consumer = new QueueingConsumer(channel);// 监听队列，手动返回完成channel.basicConsume(QUEUE_NAME2, false, consumer);// 获取消息while (true) { QueueingConsumer.Delivery delivery = consumer.nextDelivery(); String message = new String(delivery.getBody()); System.out.println(&quot; [Recv2] Received &#39;&quot; + message + &quot;&#39;&quot;); Thread.sleep(10); channel.basicAck(delivery.getEnvelope().getDeliveryTag(), false);}Topics（主题模式） 主题模式是路由模式的一个升级，在过滤条件上更加灵活 主题模式是将路由键和某个模式进行匹配。此时队列需要绑定一个模式上。#匹配一个或多个词，*匹配不多不少一个词。因此audit.#能够匹配到audit.irs.corporate，但是audit.*只会匹配到audit.irs生产者Connection connection = ConnectUtils.getConnection();Channel channel = connection.createChannel();channel.exchangeDeclare(EXCHANGE_NAME, &quot;topic&quot;);//发送两条消息channel.basicPublish(EXCHANGE_NAME, &quot;routkey.1&quot;, null, &quot;routkey消息&quot;.getBytes());channel.basicPublish(EXCHANGE_NAME, &quot;common.1&quot;, null, &quot;common消息&quot;.getBytes());channel.close();connection.close();消费者AConnection connection = ConnectUtils.getConnection();Channel channel = connection.createChannel();channel.queueDeclare(QUEUE_NAME1, false, false, false, null);//绑定到交换机channel.queueBind(QUEUE_NAME1, EXCHANGE_NAME, &quot;routkey.#&quot;);QueueingConsumer consumer = new QueueingConsumer(channel);// 监听队列channel.basicConsume(QUEUE_NAME1, true, consumer);// 获取消息while (true) { QueueingConsumer.Delivery delivery = consumer.nextDelivery(); String message = new String(delivery.getBody()); System.out.println(&quot; [Recv1] Received &#39;&quot; + message + &quot;&#39;&quot;);}消费者BConnection connection = ConnectUtils.getConnection();Channel channel = connection.createChannel();channel.queueDeclare(QUEUE_NAME2, false, false, false, null);//绑定到交换机channel.queueBind(QUEUE_NAME2, EXCHANGE_NAME, &quot;#.#&quot;);QueueingConsumer consumer = new QueueingConsumer(channel);channel.basicConsume(QUEUE_NAME2, true, consumer);// 获取消息while (true) { QueueingConsumer.Delivery delivery = consumer.nextDelivery(); String message = new String(delivery.getBody()); System.out.println(&quot; [Recv2] Received &#39;&quot; + message + &quot;&#39;&quot;);}" }, { "title": "JVM虚拟机 - 垃圾回收与内存分配", "url": "/posts/JVM%E8%99%9A%E6%8B%9F%E6%9C%BA-%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%E4%B8%8E%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/", "categories": "编程语言, JVM", "tags": "java, jvm", "date": "2020-05-15 00:00:00 +0800", "snippet": "1. 概述 判断对象状态：介绍JVM是如何判断判断对象的状态（即一个对象是否应该被回收） 垃圾回收算法：回收对象的几种算法 垃圾回收器：JVM现有的几种垃圾回收器2. 判断对象状态2.1 引用计数法​ 很多教科书判断对象是否存活的算法是这样的：在对象中添加一个引用计数器，每当有一个地方引用它时，计数器值就加一；当引用失效时，计数器值就减一；任何时刻计数器为零的对象就是不可能再被使用的。笔者面试过很多应届生和一些有多年工作经验的开发人员，他们对于这个问题给予的都是这个答案。​ 客观地说，引用计数算法（Reference Counting）虽然占用了一些额外的内存空间来进行计数，但它的原理简单，判定效率也很高，在大多数情况下它都是一个不错的算法。但是，在Java领域，至少主流的Java虚拟机里面都没有选用引用计数算法来管理内存，主要原因是，这个看似简单的算法有很多例外情况要考虑，必须要配合大量额外处理才能保证正确地工作，譬如单纯的引用计数就很难解决对象之间相互循环引用的问题。2.2 可达性分析算法​ 当前主流的商用程序语言（Java、C#，上溯至前面提到的古老的Lisp）的内存管理子系统，都是通过可达性分析（Reachability Analysis）算法来判定对象是否存活的。这个算法的基本思路就是通过一系列称为“GC Roots”的根对象作为起始节点集，从这些节点开始，根据引用关系向下搜索，搜索过程所走过的路径称为“引用链”（Reference Chain），如果某个对象到GC Roots间没有任何引用链相连，或者用图论的话来说就是从GC Roots到这个对象不可达时，则证明此对象是不可能再被使用的。​ 如图所示，对象object 5、object 6、object 7虽然互有关联，但是它们到GC Roots是不可达的，因此它们将会被判定为可回收的对象。在Java技术体系里面，固定可作为GC Roots的对象包括以下几种： 在虚拟机栈（栈帧中的本地变量表）中引用的对象，譬如各个线程被调用的方法堆栈中使用到的参数、局部变量、临时变量等。 在方法区中类静态属性引用的对象，譬如Java类的引用类型静态变量。 在方法区中常量引用的对象，譬如字符串常量池（String Table）里的引用。 在本地方法栈中JNI（即通常所说的Native方法）引用的对象。 Java虚拟机内部的引用，如基本数据类型对应的Class对象，一些常驻的异常对象（比如NullPointExcepiton、OutOfMemoryError）等，还有系统类加载器。 所有被同步锁（synchronized关键字）持有的对象。 反映Java虚拟机内部情况的JMXBean、JVMTI中注册的回调、本地代码缓存等。 ​ 除了这些固定的GC Roots集合以外，根据用户所选用的垃圾收集器以及当前回收的内存区域不同，还可以有其他对象“临时性”地加入，共同构成完整GC Roots集合。2.3 引用​ 无论是通过引用计数算法判断对象的引用数量，还是通过可达性分析算法判断对象是否引用链可达，判定对象是否存活都和“引用”离不开关系。在JDK 1.2版之前，Java里面的引用是很传统的定义：如果reference类型的数据中存储的数值代表的是另外一块内存的起始地址，就称该reference数据是代表某块内存、某个对象的引用。这种定义并没有什么不对，只是现在看来有些过于狭隘了，一个对象在这种定义下只有“被引用”或者“未被引用”两种状态，对于描述一些“食之无味，弃之可惜”的对象就显得无能为力。譬如我们希望能描述一类对象：当内存空间还足够时，能保留在内存之中，如果内存空间在进行垃圾收集后仍然非常紧张，那就可以抛弃这些对象——很多系统的缓存功能都符合这样的应用场景。​ 在JDK 1.2版之后，Java对引用的概念进行了扩充，将引用分为强引用（Strongly Re-ference）、软引用（Soft Reference）、弱引用（Weak Reference）和虚引用（Phantom Reference）4种，这4种引用强度依次逐渐减弱。 强引用是最传统的“引用”的定义，是指在程序代码之中普遍存在的引用赋值，即类似“Object obj=new Object()”这种引用关系。无论任何情况下，只要强引用关系还存在，垃圾收集器就永远不会回收掉被引用的对象。 软引用是用来描述一些还有用，但非必须的对象。只被软引用关联着的对象，在系统将要发生内存溢出异常前，会把这些对象列进回收范围之中进行第二次回收，如果这次回收还没有足够的内存，才会抛出内存溢出异常。在JDK 1.2版之后提供了SoftReference类来实现软引用。 弱引用也是用来描述那些非必须对象，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生为止。当垃圾收集器开始工作，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK 1.2版之后提供了WeakReference类来实现弱引用。 虚引用也称为“幽灵引用”或者“幻影引用”，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的只是为了能在这个对象被收集器回收时收到一个系统通知。在JDK 1.2版之后提供了PhantomReference类来实现虚引用。2.4 关于方法区回收​ 有些人认为方法区（如HotSpot虚拟机中的元空间或者永久代）是没有垃圾收集行为的，《Java虚拟机规范》中提到过可以不要求虚拟机在方法区中实现垃圾收集，事实上也确实有未实现或未能完整实现方法区类型卸载的收集器存在（如JDK 11时期的ZGC收集器就不支持类卸载），方法区垃圾收集的“性价比”通常也是比较低的：在Java堆中，尤其是在新生代中，对常规应用进行一次垃圾收集通常可以回收70%至99%的内存空间，相比之下，方法区回收囿于苛刻的判定条件，其区域垃圾收集的回收成果往往远低于此。​ 方法区的垃圾收集主要回收两部分内容：废弃的常量和不再使用的类型。回收废弃常量与回收Java堆中的对象非常类似。举个常量池中字面量回收的例子，假如一个字符串“java”曾经进入常量池中，但是当前系统又没有任何一个字符串对象的值是“java”，换句话说，已经没有任何字符串对象引用常量池中的“java”常量，且虚拟机中也没有其他地方引用这个字面量。如果在这时发生内存回收，而且垃圾收集器判断确有必要的话，这个“java”常量就将会被系统清理出常量池。常量池中其他类（接口）、方法、字段的符号引用也与此类似。​ 判定一个常量是否“废弃”还是相对简单，而要判定一个类型是否属于“不再被使用的类”的条件就比较苛刻了。需要同时满足下面三个条件： 该类所有的实例都已经被回收，也就是Java堆中不存在该类及其任何派生子类的实例 加载该类的类加载器已经被回收，这个条件除非是经过精心设计的可替换类加载器的场景，如OSGi、JSP的重加载等，否则通常是很难达成的 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法​ Java虚拟机被允许对满足上述三个条件的无用类进行回收，这里说的仅仅是“被允许”，而并不是和对象一样，没有引用了就必然会回收。​ 关于是否要对类型进行回收，HotSpot虚拟机提供了-Xnoclassgc参数进行控制，还可以使用-verbose：class以及-XX：+TraceClass-Loading、-XX：+TraceClassUnLoading查看类加载和卸载信息，其中-verbose：class和-XX：+TraceClassLoading可以在Product版的虚拟机中使用，-XX：+TraceClassUnLoading参数需要FastDebug版 的虚拟机支持。​ 在大量使用反射、动态代理、CGLib等字节码框架（例如Spring框架），动态生成JSP以及OSGi这类频繁自定义类加载器的场景中，通常都需要Java虚拟机具备类型卸载的能力，以保证不会对方法区造成过大的内存压力3. 垃圾回收算法3.1 分代收集理论​ 分代收集名为理论，实质是一套符合大多数程序运行实际情况的经验法则，它建立在两个分代假说之上： 弱分代假说（Weak Generational Hypothesis）：绝大多数对象都是朝生夕灭的。 强分代假说（Strong Generational Hypothesis）：熬过越多次垃圾收集过程的对象就越难以消亡。​ 在Java堆划分出不同的区域之后，垃圾收集器才可以每次只回收其中某一个或者某些部分的区域——因而才有了“Minor GC”“Major GC”“Full GC”这样的回收类型的划分；也才能够针对不同的区域安排与里面存储对象存亡特征相匹配的垃圾收集算法——因而发展出了“标记-复制算法”“标记-清除算法”“标记-整理算法”等针对性的垃圾收集算法。​ 把分代收集理论具体放到现在的商用Java虚拟机里，设计者一般至少会把Java堆划分为新生代（Young Generation）和老年代（Old Generation）两个区域 。顾名思义，在新生代中，每次垃圾收集时都发现有大批对象死去，而每次回收后存活的少量对象，将会逐步晋升到老年代中存放。​ 假如要现在进行一次只局限于新生代区域内的收集（Minor GC），但新生代中的对象是完全有可能被老年代所引用的，为了找出该区域中的存活对象，不得不在固定的GC Roots之外，再额外遍历整个老年代中所有对象来确保可达性分析结果的正确性，反过来也是一样 。遍历整个老年代所有对象的方案虽然理论上可行，但无疑会为内存回收带来很大的性能负担。为了解决这个问题，就需要对分代收集理论添加第三条经验法则： 跨代引用假说（Intergenerational Reference Hypothesis）：跨代引用相对于同代引用来说仅占极少数。​ 这其实是可根据前两条假说逻辑推理得出的隐含推论：存在互相引用关系的两个对象，是应该倾向于同时生存或者同时消亡的。举个例子，如果某个新生代对象存在跨代引用，由于老年代对象难以消亡，该引用会使得新生代对象在收集时同样得以存活，进而在年龄增长之后晋升到老年代中，这时跨代引用也随即被消除了。​ 依据这条假说，我们就不应再为了少量的跨代引用去扫描整个老年代，也不必浪费空间专门记录每一个对象是否存在及存在哪些跨代引用，只需在新生代上建立一个全局的数据结构（该结构被称为“记忆集”，Remembered Set），这个结构把老年代划分成若干小块，标识出老年代的哪一块内存会存在跨代引用。此后当发生Minor GC时，只有包含了跨代引用的小块内存里的对象才会被加入到GC Roots进行扫描。虽然这种方法需要在对象改变引用关系（如将自己或者某个属性赋值）时维护记录数据的正确性，会增加一些运行时的开销，但比起收集时扫描整个老年代来说仍然是划算的。GC回收关键字： 新生代收集（Minor GC/Young GC）：指目标只是新生代的垃圾收集。 老年代收集（Major GC/Old GC）：指目标只是老年代的垃圾收集。目前只有CMS收集器会有单独收集老年代的行为。 混合收集（Mixed GC）：指目标是收集整个新生代以及部分老年代的垃圾收集。目前只有G1收集器会有这种行为 整堆收集（Full GC）：收集整个Java堆和方法区的垃圾收集。3.2 标记-清除算法​ 如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后，统一回收掉所有被标记的对象，也可以反过来，标记存活的对象，统一回收所有未被标记的对象。标记过程就是对象是否属于垃圾的判定过程​ 主要缺点有两个：第一个是执行效率不稳定，如果Java堆中包含大量对象，而且其中大部分是需要被回收的，这时必须进行大量标记和清除的动作，导致标记和清除两个过程的执行效率都随对象数量增长而降低；第二个是内存空间的碎片化问题，标记、清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致当以后在程序运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。​ 算法执行过程如图所示3.3 标记-复制算法​ 为了解决标记-清除算法面对大量可回收对象时执行效率低的问题，1969年Fenichel提出了一种称为“半区复制”（Semispace Copying）的垃圾收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。如果内存中多数对象都是存活的，这种算法将会产生大量的内存间复制的开销，但对于多数对象都是可回收的情况，算法需要复制的就是占少数的存活对象，而且每次都是针对整个半区进行内存回收，分配内存时也就不用考虑有空间碎片的复杂情况，只要移动堆顶指针，按顺序分配即可。这样实现简单，运行高效，不过其缺陷也显而易见，这种复制回收算法的代价是将可用内存缩小为了原来的一半，空间浪费未免太多了一点。​ 算法执行过程如图所示​ 现在的商用Java虚拟机大多都优先采用了这种收集算法去回收新生代，但是由于大部分对象都活不过第一轮收集，因此并不需要按照1∶1的比例来划分新生代的内存空间。例如，HotSpot虚拟机的Serial、ParNew等新生代收集器均采用了这种策略来设计新生代的内存布局。Appel式回收的具体做法是把新生代分为一块较大的Eden空间和两块较小的Survivor空间，每次分配内存只使用Eden和其中一块Survivor。发生垃圾搜集时，将Eden和Survivor中仍然存活的对象一次性复制到另外一块Survivor空间上，然后直接清理掉Eden和已用过的那块Survivor空间。HotSpot虚拟机默认Eden和Survivor的大小比例是8∶1，也即每次新生代中可用内存空间为整个新生代容量的90%（Eden的80%加上一个Survivor的10%），只有一个Survivor空间，即10%的新生代是会被“浪费”的。3.4 标记-整理算法​ 标记-复制算法在对象存活率较高时就要进行较多的复制操作，效率将会降低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。​ 其中的标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向内存空间一端移动，然后直接清理掉边界以外的内存​ 算法执行过程如图所示​ 如果移动存活对象，尤其是在老年代这种每次回收都有大量对象存活区域，移动存活对象并更新所有引用这些对象的地方将会是一种极为负重的操作，而且这种对象移动操作必须全程暂停用户应用程序才能进行 ，这就更加让使用者不得不小心翼翼地权衡其弊端了，像这样的停顿被最初的虚拟机设计者形象地描述为“Stop The World”4. 垃圾回收器​ 下图展示了七种作用于不同分代的收集器，如果两个收集器之间存在连线，就说明它们可以搭配使用，图中收集器所处的区域，则表示它是属于新生代收集器抑或是老年代收集器。​ 在介绍这些收集器各自的特性之前，让我们先来明确一个观点：虽然我们会对各个收集器进行比较，但并非为了挑选一个最好的收集器出来，虽然垃圾收集器的技术在不断进步，但直到现在还没有最好的收集器出现，更加不存在“万能”的收集器，所以我们选择的只是对具体应用最合适的收集器。这点不需要多加论述就能证明：如果有一种放之四海皆准、任何场景下都适用的完美收集器存在，HotSpot虚拟机完全没必要实现那么多种不同的收集器了。4.1 Serial收集器​ Serial收集器是最基础、历史最悠久的收集器，曾经（在JDK 1.3.1之前）是HotSpot虚拟机新生代收集器的唯一选择。只看名字就能够猜到，这个收集器是一个单线程工作的收集器，但它的“单线程”的意义并不仅仅是说明它只会使用一个处理器或一条收集线程去完成垃圾收集工作，更重要的是强调在它进行垃圾收集时，必须暂停其他所有工作线程，直到它收集结束。图示意了Serial/Serial Old收集器的运行过程。但是该收集器也有自己的优点：那就是简单而高效（与其他收集器的单线程相比），对于内存资源受限的环境，它是所有收集器里额外内存消耗（Memory Footprint）最小的；对于单核处理器或处理器核心数较少的环境来说，Serial收集器由于没有线程交互的开销，专心做垃圾收集自然可以获得最高的单线程收集效率。4.2 ParNew收集器​ ParNew收集器实质上是Serial收集器的多线程并行版本，除了同时使用多条线程进行垃圾收集之外，其余的行为包括Serial收集器可用的所有控制参数（例如：-XX：SurvivorRatio、-XX：PretenureSizeThreshold、-XX：HandlePromotionFailure等）、收集算法、Stop The World、对象分配规则、回收策略等都与Serial收集器完全一致，在实现上这两种收集器也共用了相当多的代码。ParNew/Serial Old收集器的工作过程如图所示。 ParNew收集器除了支持多线程并行收集之外，其他与Serial收集器相比并没有太多创新之处，但它却是不少运行在服务端模式下的HotSpot虚拟机，尤其是JDK 7之前的遗留系统中首选的新生代收集器，其中有一个与功能、性能无关但其实很重要的原因是：除了Serial收集器外，目前只有它能与CMS收集器配合工作。值得注意的是ParNew收集器在单核心处理器的环境中绝对不会有比Serial收集器更好的效果，甚至由于存在线程交互的开销，该收集器在通过超线程（Hyper-Threading）技术实现的伪双核处理器环境中都不能百分之百保证超越Serial收集器。4.3 Parallel Scavenge收集器​ Parallel Scavenge收集器也是一款新生代收集器，它同样是基于标记-复制算法实现的收集器，也是能够并行收集的多线程收集器，但是与上面的收集器相比，Parallel Scavenge收集器的特点是它的关注点与其他收集器不同，CMS等收集器的关注点是尽可能地缩短垃圾收集时用户线程的停顿时间，而Parallel Scavenge收集器的目标则是达到一个可控制的吞吐量（Throughput）。所谓吞吐量就是处理器用于运行用户代码的时间与处理器总消耗时间的比值（吞吐量=运行代码时间/(运行用户代码时间+运行垃圾收集时间)）。如果虚拟机完成某个任务，用户代码加上垃圾收集总共耗费了100分钟，其中垃圾收集花掉1分钟，那吞吐量就是99%。​ Parallel Scavenge收集器提供了两个参数用于精确控制吞吐量，分别是控制最大垃圾收集停顿时间的-XX：MaxGCPauseMillis参数以及直接设置吞吐量大小的-XX：GCTimeRatio参数。 -XX：MaxGCPauseMillis参数允许的值是一个大于0的毫秒数，收集器将尽力保证内存回收花费的时间不超过用户设定值。不过不要异想天开地认为如果把这个参数的值设置得更小一点就能使得系统的垃圾收集速度变得更快，垃圾收集停顿时间缩短是以牺牲吞吐量和新生代空间为代价换取的：系统把新生代调得小一些，收集300MB新生代肯定比收集500MB快，但这也直接导致垃圾收集发生得更频繁，原来10秒收集一次、每次停顿100毫秒，现在变成5秒收集一次、每次停顿70毫秒。停顿时间的确在下降，但吞吐量也降下来了。 -XX：GCTimeRatio参数的值则应当是一个大于0小于100的整数，也就是垃圾收集时间占总时间的比率，相当于吞吐量的倒数。譬如把此参数设置为19，那允许的最大垃圾收集时间就占总时间的5%（即1/(1+19)），默认值为99，即允许最大1%（即1/(1+99)）的垃圾收集时间。 ​ 由于与吞吐量关系密切，Parallel Scavenge收集器也经常被称作“吞吐量优先收集器”。除上述两个参数之外，Parallel Scavenge收集器还有一个参数-XX：+UseAdaptiveSizePolicy值得我们关注。这是一个开关参数，当这个参数被激活之后，就不需要人工指定新生代的大小（-Xmn）、Eden与Survivor区的比例（-XX：SurvivorRatio）、晋升老年代对象大小（-XX：PretenureSizeThreshold）等细节参数了，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或者最大的吞吐量。这种调节方式称为垃圾收集的自适应的调节策略（GC Ergonomics）。如果对于收集器运作不太了解，手工优化存在困难的话，使用Parallel Scavenge收集器配合自适应调节策略，把内存管理的调优任务交给虚拟机去完成也许是一个很不错的选择。只需要把基本的内存数据设置好（如-Xmx设置最大堆），然后使用-XX：MaxGCPauseMillis参数（更关注最大停顿时间）或-XX：GCTimeRatio（更关注吞吐量）参数给虚拟机设立一个优化目标，那具体细节参数的调节工作就由虚拟机完成了。自适应调节策略也是Parallel Scavenge收集器区别于ParNew收集器的一个重要特性。ps：这也是java version &quot;1.8.0_241&quot;默认的新生代垃圾收集器4.4 Serial Old收集器​ Serial Old是Serial收集器的老年代版本，它同样是一个单线程收集器，使用标记-整理算法。这个收集器的主要意义也是供客户端模式下的HotSpot虚拟机使用。如果在服务端模式下，它也可能有两种用途：一种是在JDK 5以及之前的版本中与Parallel Scavenge收集器搭配使用 ，另外一种就是作为CMS收集器发生失败时的后备预案，在并发收集发生Concurrent Mode Failure时使用。Serial Old收集器的工作过程如图所示。4.5 Parallel Old收集器​ Parallel Old是Parallel Scavenge收集器的老年代版本，支持多线程并发收集，基于标记-整理算法实现。这个收集器是直到JDK 6时才开始提供的，在此之前，新生代的Parallel Scavenge收集器一直处于相当尴尬的状态，原因是如果新生代选择了Parallel Scavenge收集器，老年代除了Serial Old（PS MarkSweep）收集器以外别无选择，其他表现良好的老年代收集器，如CMS无法与它配合工作。由于老年代Serial Old收集器在服务端应用性能上的“拖累”，使用Parallel Scavenge收集器也未必能在整体上获得吞吐量最大化的效果。同样，由于单线程的老年代收集中无法充分利用服务器多处理器的并行处理能力，在老年代内存空间很大而且硬件规格比较高级的运行环境中，这种组合的总吞吐量甚至不一定比ParNew加CMS的组合来得优秀。​ 直到Parallel Old收集器出现后，“吞吐量优先”收集器终于有了比较名副其实的搭配组合，在注重吞吐量或者处理器资源较为稀缺的场合，都可以优先考虑Parallel Scavenge加Parallel Old收集器这个组合（目前jdk1.8就是采用这两个收集器）。Parallel Old收集器的工作过程如图所示。4.6 CMS收集器​ CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用集中在互联网网站或者基于浏览器的B/S系统的服务端上，这类应用通常都会较为关注服务的响应速度，希望系统停顿时间尽可能短，以给用户带来良好的交互体验。CMS收集器就非常符合这类应用的需求。​ 从名字（包含“Mark Sweep”）上就可以看出CMS收集器是基于标记-清除算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为四个步骤，包括： 初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep）​ 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快；并发标记阶段就是从GC Roots的直接关联对象开始遍历整个对象图的过程，这个过程耗时较长但是不需要停顿用户线程，可以与垃圾收集线程一起并发运行；而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间通常会比初始标记阶段稍长一些，但也远比并发标记阶段的时间短；最后是并发清除阶段，清理删除掉标记阶段判断的已经死亡的对象，由于不需要移动存活对象，所以这个阶段也是可以与用户线程同时并发的。​ 由于在整个过程中耗时最长的并发标记和并发清除阶段中，垃圾收集器线程都可以与用户线程一起工作，所以从总体上来说，CMS收集器的内存回收过程是与用户线程一起并发执行的。通过下图可以比较清楚地看到CMS收集器的运作步骤中并发和需要停顿的阶段。​ CMS是一款优秀的收集器，它最主要的优点在名字上已经体现出来：并发收集、低停顿，一些官方公开文档里面也称之为“并发低停顿收集器”（Concurrent Low Pause Collector）。CMS收集器是HotSpot虚拟机追求低停顿的第一次成功尝试，但是它还远达不到完美的程度，至少有以下三个明显的缺点： 首先，CMS收集器对处理器资源非常敏感。事实上，面向并发设计的程序都对处理器资源比较敏感。在并发阶段，它虽然不会导致用户线程停顿，但却会因为占用了一部分线程（或者说处理器的计算能力）而导致应用程序变慢，降低总吞吐量。CMS默认启动的回收线程数是（处理器核心数量+3）/4，也就是说，如果处理器核心数在四个或以上，并发回收时垃圾收集线程只占用不超过25%的处理器运算资源，并且会随着处理器核心数量的增加而下降。但是当处理器核心数量不足四个时，CMS对用户程序的影响就可能变得很大。如果应用本来的处理器负载就很高，还要分出一半的运算能力去执行收集器线程，就可能导致用户程序的执行速度忽然大幅降低。为了缓解这种情况，虚拟机提供了一种称为“增量式并发收集器”（Incremental Concurrent Mark Sweep/i-CMS）的CMS收集器变种，所做的事情和以前单核处理器年代PC机操作系统靠抢占式多任务来模拟多核并行多任务的思想一样，是在并发标记、清理的时候让收集器线程、用户线程交替运行，尽量减少垃圾收集线程的独占资源的时间，这样整个垃圾收集的过程会更长，但对用户程序的影响就会显得较少一些，直观感受是速度变慢的时间更多了，但速度下降幅度就没有那么明显。实践证明增量式的CMS收集器效果很一般，从JDK 7开始，i-CMS模式已经被声明为“deprecated”，即已过时不再提倡用户使用，到JDK 9发布后i-CMS模式被完全废弃。 由于CMS收集器无法处理“浮动垃圾”（Floating Garbage），有可能出现“Con-current Mode Failure”失败进而导致另一次完全“Stop The World”的Full GC的产生。在CMS的并发标记和并发清理阶段，用户线程是还在继续运行的，程序在运行自然就还会伴随有新的垃圾对象不断产生，但这一部分垃圾对象是出现在标记过程结束以后，CMS无法在当次收集中处理掉它们，只好留待下一次垃圾收集时再清理掉。这一部分垃圾就称为“浮动垃圾”。同样也是由于在垃圾收集阶段用户线程还需要持续运行，那就还需要预留足够内存空间提供给用户线程使用，因此CMS收集器不能像其他收集器那样等待到老年代几乎完全被填满了再进行收集，必须预留一部分空间供并发收集时的程序运作使用。在JDK 5的默认设置下，CMS收集器当老年代使用了68%的空间后就会被激活，这是一个偏保守的设置，如果在实际应用中老年代增长并不是太快，可以适当调高参数-XX：CMSInitiatingOccu-pancyFraction的值来提高CMS的触发百分比，降低内存回收频率，获取更好的性能。到了JDK 6时，CMS收集器的启动阈值就已经默认提升至92%。但这又会更容易面临另一种风险：要是CMS运行期间预留的内存无法满足程序分配新对象的需要，就会出现一次“并发失败”（Concurrent Mode Failure），这时候虚拟机将不得不启动后备预案：冻结用户线程的执行，临时启用Serial Old收集器来重新进行老年代的垃圾收集，但这样停顿时间就很长了。所以参数-XX：CMSInitiatingOccupancyFraction设置得太高将会很容易导致大量的并发失败产生，性能反而降低，用户应在生产环境中根据实际应用情况来权衡设置。 CMS是一款基于“标记-清除”算法实现的收集器，这意味着收集结束时会有大量空间碎片产生。空间碎片过多时，将会给大对象分配带来很大麻烦，往往会出现老年代还有很多剩余空间，但就是无法找到足够大的连续空间来分配当前对象，而不得不提前触发一次Full GC的情况。为了解决这个问题，CMS收集器提供了一个-XX：+UseCMS-CompactAtFullCollection开关参数（默认是开启的，此参数从JDK 9开始废弃），用于在CMS收集器不得不进行Full GC时开启内存碎片的合并整理过程，由于这个内存整理必须移动存活对象，（在Shenandoah和ZGC出现前）是无法并发的。这样空间碎片问题是解决了，但停顿时间又会变长，因此虚拟机设计者们还提供了另外一个参数-XX：CMSFullGCsBefore-Compaction（此参数从JDK 9开始废弃），这个参数的作用是要求CMS收集器在执行过若干次（数量由参数值决定）不整理空间的Full GC之后，下一次进入Full GC前会先进行碎片整理（默认值为0，表示每次进入Full GC时都进行碎片整理）。 4.7 G1回收器​ G1是一款面向服务端应用的垃圾收集器。HotSpot开发团队赋予它的使命是（在比较长期的）未来可以替换掉JDK 1.5中发布的CMS收集器。与其他GC收集器相比，G1具备如下特点。 并行与并发：G1能充分利用多CPU、多核环境下的硬件优势，使用多个CPU（CPU或者CPU核心）来缩短Stop-The-World停顿的时间，部分其他收集器原本需要停顿Java线程执行的GC动作，G1收集器仍然可以通过并发的方式让Java程序继续执行。 分代收集：与其他收集器一样，分代概念在G1中依然得以保留。虽然G1可以不需要其他收集器配合就能独立管理整个GC堆，但它能够采用不同的方式去处理新创建的对象和已经存活了一段时间、熬过多次GC的旧对象以获取更好的收集效果。 空间整合：与CMS的“标记—清理”算法不同，G1从整体来看是基于“标记—整理”算法实现的收集器，从局部（两个Region之间）上来看是基于“复制”算法实现的，但无论如何，这两种算法都意味着G1运作期间不会产生内存空间碎片，收集后能提供规整的可用内存。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次GC。 可预测的停顿：这是G1相对于CMS的另一大优势，降低停顿时间是G1和CMS共同的关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了在G1收集器中，Region之间的对象引用以及其他收集器中的新生代与老年代之间的对象引用，虚拟机都是使用Remembered Set来避免全堆扫描的。G1中每个Region都有一个与之对应的Remembered Set，虚拟机发现程序在对Reference类型的数据进行写操作时，会产生一个Write Barrier暂时中断写操作，检查Reference引用的对象是否处于不同的Region之中（在分代的例子中就是检查是否老年代中的对象引用了新生代中的对象），如果是，便通过CardTable把相关引用信息记录到被引用对象所属的Region的Remembered Set之中。当进行内存回收时，在GC根节点的枚举范围中加入Remembered Set即可保证不对全堆扫描也不会有遗漏。如果不计算维护Remembered Set的操作，G1收集器的运作大致可划分为以下几个步骤： 初始标记（Initial Marking） 并发标记（Concurrent Marking） 最终标记（Final Marking） 筛选回收（Live Data Counting and Evacuation） 可以看出来G1的前几个步骤的运作过程和CMS有很多相似之处。初始标记阶段仅仅只是标记一下GC Roots能直接关联到的对象，并且修改TAMS（Next Top at Mark Start）的值，让下一阶段用户程序并发运行时，能在正确可用的Region中创建新对象，这阶段需要停顿线程，但耗时很短。并发标记阶段是从GC Root开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行。而最终标记阶段则是为了修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，虚拟机将这段时间对象变化记录在线程Remembered Set Logs里面，最终标记阶段需要把Remembered Set Logs的数据合并到Remembered Set中，这阶段需要停顿线程，但是可并行执行。最后在筛选回收阶段首先对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划，从Sun公司透露出来的信息来看，这个阶段其实也可以做到与用户程序一起并发执行，但是因为只回收一部分Region，时间是用户可控制的，而且停顿用户线程将大幅提高收集效率。通过下图可以比较清楚地看到G1收集器的运作步骤中并发和需要停顿的阶段。5. 内存分配策略 对象优先在Eden分配：大多数情况下，对象在新生代Eden区中分配。当Eden区没有足够空间进行分配时，虚拟机将发起一次Minor GC 大对象直接进入老年代：所谓的大对象是指，需要大量连续内存空间的Java对象，最典型的大对象就是那种很长的字符串以及数组 长期存活的对象将进入老年代：既然虚拟机采用了分代收集的思想来管理内存，那么内存回收时就必须能识别哪些对象应放在新生代，哪些对象应放在老年代中。为了做到这点，虚拟机给每个对象定义了一个对象年龄（Age）计数器。如果对象在Eden出生并经过第一次Minor GC后仍然存活，并且能被Survivor容纳的话，将被移动到Survivor空间中，并且对象年龄设为1。对象在Survivor区中每“熬过”一次Minor GC，年龄就增加1岁，当它的年龄增加到一定程度（默认为15岁），就将会被晋升到老年代中。对象晋升老年代的年龄阈值，可以通过参数-XX:MaxTenuringThreshold设置。 动态对象年龄判定：为了能更好地适应不同程序的内存状况，虚拟机并不是永远地要求对象的年龄必须达到了MaxTenuringThreshold才能晋升老年代，如果在Survivor空间中相同年龄所有对象大小的总和大于Survivor空间的一半，年龄大于或等于该年龄的对象就可以直接进入老年代，无须等到MaxTenuringThreshold中要求的年龄。参考 《深入理解java虚拟机》" }, { "title": "JVM虚拟机 - JVM内存管理", "url": "/posts/JVM%E8%99%9A%E6%8B%9F%E6%9C%BA-JVM%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/", "categories": "编程语言, jvm", "tags": "jvm, java", "date": "2020-04-29 00:08:59 +0800", "snippet": "1.JVM 内存概述 程序计数器 Java虚拟机栈（Stack Frame）栈帧 本地方法栈 堆（Heap）：JVM管理的最大一块内存空间 运行时常量池（Runtime Constant Pool） 直接内存（Direct Memory）2.程序计数器​ 程序计数器（Program Counter Register）是一块较小的内存空间，它可以看作是当前线程所执行的字节码的行号指示器。在Java虚拟机的概念模型里 ，字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，它是程序控制流的指示器，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。​ 由于Java虚拟机的多线程是通过线程轮流切换、分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）都只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。​ 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是本地（Native）方法，这个计数器值则应为空（Undefined）。此内存区域是唯一一个在《Java虚拟机规范》中没有规定任何OutOfMemoryError情况的区域。上图所标记出来的就是一个跳转操作3. Java虚拟机栈​ 与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stack）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的线程内存模型：每个方法被执行的时候，Java虚拟机都会同步创建一个栈帧 （Stack Frame）用于存储局部变量表、操作数栈、动态连接、方法出口等信息。每一个方法被调用直至执行完毕的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。​ 经常有人把Java内存区域笼统地划分为堆内存（Heap）和栈内存（Stack），这种划分方式直接继承自传统的C、C++程序的内存布局结构，在Java语言里就显得有些粗糙了，实际的内存区域划分要比这更复杂。不过这种划分方式的流行也间接说明了程序员最关注的、与对象内存分配关系最密切的区域是“堆”和“栈”两块。其中，“堆”在稍后笔者会专门讲述，而“栈”通常就是指这里讲的虚拟机栈，或者更多的情况下只是指虚拟机栈中局部变量表部分。​ 局部变量表存放了编译期可知的各种Java虚拟机基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它并不等同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。​ 这些数据类型在局部变量表中的存储空间以局部变量槽（Slot）来表示，其中64位长度的long和double类型的数据会占用两个变量槽，其余的数据类型只占用一个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在栈帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。请读者注意，这里说的“大小”是指变量槽的数量，虚拟机真正使用多大的内存空间（譬如按照1个变量槽占用32个比特、64个比特，或者更多）来实现一个变量槽，这是完全由具体的虚拟机实现自行决定的事情。​ 在《Java虚拟机规范》中，对这个内存区域规定了两类异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果Java虚拟机栈容量可以动态扩展 ，当栈扩展时无法申请到足够的内存会抛出OutOfMemoryError异常。以下面程序为例 0：将int类型的1推送到（push）到常量表中 1：将1存储到常量表中 2-3：如上 4-5：从常量表中加载1、2并推送到操作栈中（operand stack） 6：弹出操作栈中最上面两个值（1,2）进行相加，再讲结果推到栈顶 7：将值存入第一个局部变量 8：返回4.本地方法栈​ 本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别只是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的本地（Native）方法服务。​ 《Java虚拟机规范》对本地方法栈中方法使用的语言、使用方式与数据结构并没有任何强制规定，因此具体的虚拟机可以根据需要自由实现它，甚至有的Java虚拟机（譬如Hot-Spot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈也会在栈深度溢出或者栈扩展失败时分别抛出StackOverflowError和OutOfMemoryError异常。5.Java堆​ 对于Java应用程序来说，Java堆（Java Heap）是虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，Java世界里“几乎”所有的对象实例都在这里分配内存。在《Java虚拟机规范》中对Java堆的描述是：“所有的对象实例以及数组都应当在堆上分配 ”，而这里笔者写的“几乎”是指从实现角度来看，随着Java语言的发展，现在已经能看到些许迹象表明日后可能出现值类型的支持，即使只考虑现在，由于即时编译技术的进步，尤其是逃逸分析技术的日渐强大，栈上分配、标量替换优化手段已经导致一些微妙的变化悄然发生，所以说Java对象实例都分配在堆上也渐渐变得不是那么绝对了。​ Java堆是垃圾收集器管理的内存区域，因此一些资料中它也被称作“GC堆”（Garbage Collected Heap，幸好国内没翻译成“垃圾堆”）。从回收内存的角度看，由于现代垃圾收集器大部分都是基于分代收集理论设计的，所以Java堆中经常会出现“新生代”“老年代”“永久代”“Eden空间”“From Survivor空间”“To Survivor空间”等名词，在这里想先说明的是这些区域划分仅仅是一部分垃圾收集器的共同特性或者说设计风格而已，而非某个Java虚拟机具体实现的固有内存布局，更不是《Java虚拟机规范》里对Java堆的进一步细致划分。不少资料上经常写着类似于“Java虚拟机的堆内存分为新生代、老年代、永久代、Eden、Survivor……”这样的内容。在十年之前（以G1收集器的出现为分界），作为业界绝对主流的HotSpot虚拟机，它内部的垃圾收集器全部都基于“经典分代” 来设计，需要新生代、老年代收集器搭配才能工作，在这种背景下，上述说法还算是不会产生太大歧义。但是到了今天，垃圾收集器技术与十年前已不可同日而语，HotSpot里面也出现了不采用分代设计的新垃圾收集器，再按照上面的提法就有很多需要商榷的地方了。​ 如果从分配内存的角度看，所有线程共享的Java堆中可以划分出多个线程私有的分配缓冲区（Thread Local Allocation Buffer，TLAB），以提升对象分配时的效率。不过无论从什么角度，无论如何划分，都不会改变Java堆中存储内容的共性，无论是哪个区域，存储的都只能是对象的实例，将Java堆细分的目的只是为了更好地回收内存，或者更快地分配内存。​ 根据《Java虚拟机规范》的规定，Java堆可以处于物理上不连续的内存空间中，但在逻辑上它应该被视为连续的，这点就像我们用磁盘空间去存储文件一样，并不要求每个文件都连续存放。但对于大对象（典型的如数组对象），多数虚拟机实现出于实现简单、存储高效的考虑，很可能会要求连续的内存空间。​ Java堆既可以被实现成固定大小的，也可以是可扩展的，不过当前主流的Java虚拟机都是按照可扩展来实现的（通过参数-Xmx和-Xms设定）。如果在Java堆中没有内存完成实例分配，并且堆也无法再扩展时，Java虚拟机将会抛出OutOfMemoryError异常。6.方法区​ 方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类型信息、常量、静态变量、即时编译器编译后的代码缓存等数据。虽然《Java虚拟机规范》中把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫作“非堆”（Non-Heap），目的是与Java堆区分开来。​ 说到方法区，不得不提一下“永久代”这个概念，尤其是在JDK 8以前，许多Java程序员都习惯在HotSpot虚拟机上开发、部署程序，很多人都更愿意把方法区称呼为“永久代”（Permanent Generation），或将两者混为一谈。本质上这两者并不是等价的，因为仅仅是当时的HotSpot虚拟机设计团队选择把收集器的分代设计扩展至方法区，或者说使用永久代来实现方法区而已，这样使得HotSpot的垃圾收集器能够像管理Java堆一样管理这部分内存，省去专门为方法区编写内存管理代码的工作。但是对于其他虚拟机实现，譬如BEA JRockit、IBM J9等来说，是不存在永久代的概念的。原则上如何实现方法区属于虚拟机实现细节，不受《Java虚拟机规范》管束，并不要求统一。但现在回头来看，当年使用永久代来实现方法区的决定并不是一个好主意，这种设计导致了Java应用更容易遇到内存溢出的问题（永久代有-XX：MaxPermSize的上限，即使不设置也有默认大小，而J9和JRockit只要没有触碰到进程可用内存的上限，例如32位系统中的4GB限制，就不会出问题），而且有极少数方法（例如String::intern()）会因永久代的原因而导致不同虚拟机下有不同的表现。当Oracle收购BEA获得了JRockit的所有权后，准备把JRockit中的优秀功能，譬如Java Mission Control管理工具，移植到HotSpot虚拟机时，但因为两者对方法区实现的差异而面临诸多困难。考虑到HotSpot未来的发展，在JDK 6的时候HotSpot开发团队就有放弃永久代，逐步改为采用本地内存（Native Memory）来实现方法区的计划了，到了JDK 7的HotSpot，已经把原本放在永久代的字符串常量池、静态变量等移出，而到了JDK 8，终于完全废弃了永久代的概念，改用与JRockit、J9一样在本地内存中实现的元空间（Meta-space）来代替，把JDK 7中永久代还剩余的内容（主要是类型信息）全部移到元空间中。​ 《Java虚拟机规范》对方法区的约束是非常宽松的，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，甚至还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域的确是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收效果比较难令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收有时又确实是必要的。以前Sun公司的Bug列表中，曾出现过的若干个严重的Bug就是由于低版本的HotSpot虚拟机对此区域未完全回收而导致内存泄漏。​ 根据《Java虚拟机规范》的规定，如果方法区无法满足新的内存分配需求时，将抛出OutOfMemoryError异常。7.运行时常量池​ 运行时常量池（Runtime Constant Pool）是方法区的一部分。Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有一项信息是常量池表（Constant Pool Table），用于存放编译期生成的各种字面量与符号引用，这部分内容将在类加载后存放到方法区的运行时常量池中。​ Java虚拟机对于Class文件每一部分（自然也包括常量池）的格式都有严格规定，如每一个字节用于存储哪种数据都必须符合规范上的要求才会被虚拟机认可、加载和执行，但对于运行时常量池，《Java虚拟机规范》并没有做任何细节的要求，不同提供商实现的虚拟机可以按照自己的需要来实现这个内存区域，不过一般来说，除了保存Class文件中描述的符号引用外，还会把由符号引用翻译出来的直接引用也存储在运行时常量池中 。​ 运行时常量池相对于Class文件常量池的另外一个重要特征是具备动态性，Java语言并不要求常量一定只有编译期才能产生，也就是说，并非预置入Class文件中常量池的内容才能进入方法区运行时常量池，运行期间也可以将新的常量放入池中，这种特性被开发人员利用得比较多的便是String类的intern()方法。​ 既然运行时常量池是方法区的一部分，自然受到方法区内存的限制，当常量池无法再申请到内存时会抛出OutOfMemoryError异常。8.直接内存​ 直接内存（Direct Memory）并不是虚拟机运行时数据区的一部分，也不是《Java虚拟机规范》中定义的内存区域。但是这部分内存也被频繁地使用，而且也可能导致OutOfMemoryError异常出现，所以我们放到这里一起讲解。​ 在JDK 1.4中新加入了NIO（New Input/Output）类，引入了一种基于通道（Channel）与缓冲区（Buffer）的I/O方式，它可以使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆里面的DirectByteBuffer对象作为这块内存的引用进行操作。这样能在一些场景中显著提高性能，因为避免了在Java堆和Native堆中来回复制数据。​ 显然，本机直接内存的分配不会受到Java堆大小的限制，但是，既然是内存，则肯定还是会受到本机总内存（包括物理内存、SWAP分区或者分页文件）大小以及处理器寻址空间的限制，一般服务器管理员配置虚拟机参数时，会根据实际内存去设置-Xmx等参数信息，但经常忽略掉直接内存，使得各个内存区域总和大于物理内存限制（包括物理的和操作系统级的限制），从而导致动态扩展时出现OutOfMemoryError异常。" }, { "title": "JVM虚拟机 - 字节码分析", "url": "/posts/JVM%E8%99%9A%E6%8B%9F%E6%9C%BA-%E5%AD%97%E8%8A%82%E7%A0%81%E5%88%86%E6%9E%90/", "categories": "编程语言, jvm", "tags": "jvm, java", "date": "2020-04-17 20:58:06 +0800", "snippet": "Java字节码分析Java字节码整体结构 类型 名称 数量（字节） u4 magic(魔数) 1 u2 minor_version(次版本号) 1 u2 major_version(主版本号) 1 u2 constant_pool_count(常量个数) 1 cp_info constant_pool(常量池) constant_pool_count-1 u2 access_flags(类的访问控制权限) 1 u2 this_class(类名) 1 u2 super_class(父类名) 1 u2 interfaces_count(接口个数) 1 u2 interfaces(接口名) interfaces_count u2 fileds_count(字段个数) 1 field_info fields(字段的表) fields_count u2 methods_count(方法的个数) 1 method_info methods(方法表) methods_count u2 attributes_count(附加属性的个数) 1 attribute_info attributes(附加属性的表) attributes_count 在上表中 u2、u4分别代表为占2、4个字节上表顺序即为解析class文件的中数据所对应的顺序 可以使用javap -v命令分析一个字节码文件，就可以查看到以上信息 或者使用idea中插件jclasslib同样可以查询代码（后续内容均通过以下代码进行分析）：Javap -v :javap -v classname.classjclasslibJava字节码内容详细解析（访问标志前所有信息）魔数、版本号、常量池 可以从表中看出java常量池前包含以下信息：魔数、版本号、常量池下面通过一张图来介绍： 第一段为魔术：所有的.class字节码文件的前4个字节都是魔数，魔数固定值为0xCAFEBABE 第二段即魔数后面为版本信息，前两个字节表示minor version（次版本号），后面两个字节表示major version（主版本号），换算成十进制表示为次版本号为0，主版本号为55，对应着Java11 第三段和后面一部分数据为常量池：一个Java类的很多信息都是由常量池来维护和描述的，可以将常量池看做是Class文件的资源仓库，比如说Java类中定义的方法与变量信息，都是存储在常量池中的。常量池中主要存储两类常量：字面常量与符号引用，字面常量如文本字符串，Java中声明为final的常量值等，而符号引用如类和接口的全局限定名，字段的名称和描述符，方法的名称和描述符等 常量池的总体结构：常量池中常量的数量和常量池组（即常量的具体信息）。常量池的数量紧跟在主版本号后面，即图中第三段所标记，占据2个字节。常量池组则紧跟着常量数量后面。常量池组与一般的数组不同的是，常量词组中不同元素类型、长度、结构都是不同的，但是每个常量都有一个u1类型的数据来对该常量进行表示，占据一个字节。JVM在解析常量池时会根据u1类型的数据来判断该常量的类型。 通过将常量池数量的数值转换为十进制（1E -&amp;gt; 30），但值得注意的是，常量池组中元素的个数 = 常量池数（30）-1（期中0暂时不使用），目的就是为了满足某些常量池索引值得数据在特定情况下需要表达不引用任何一个常量池的含义；根本原因在于，索引0也是一个常量（保留常量），只不过它位于常量表中，这个常量对应的就是null值，所以常量池索引是从1而非0开始的 常量池数据类型结构有了这张表就可以对照着常量池字节码数据一一找出期中包含的信息，下面以后续7个常量为例：期中每个颜色为一个常量 0A (10)，即tag值为10，CONSTANT_Methodref_info，方法信息，期中包含两个u2类型的索引项，第一个指向00 05(5)，第二个指向00 15(21)即表示他们指向的第5个和第21个常量信息 0A(10)，同上 09(9),tag值为9，CONSTANT_Methodref_info,字段信息，期中包含两个u2类型的索引项，一个为声明字段的类或结构的描述符信息的索引00 04(4)，另一个为其类型类型的索引00 18(24) 07(7),tag值为7，CONSTANT_Class_info,包含一个u2的类型的数据，类的全限定名索引00 19(25)，即指向第25个常量 07,tag值为7，同上 01,tag值为1，CONSTANT_Utf8_info，一个utf-8的字符串常量，包含两个信息，一个为u2类型数据，表示字符串长度，以及紧跟着的字符串 同上P.S.： 在jvm规范中，每个变量/字段都有描述信息，描述信息主要的作用是描述字段的数据类型、方法的参数列表（包括数量、类型与顺序）与返回值。根据描述符规则，基本数据类型和代表无返回值的void类型都用一个大写的字符来表示，对象则使用字符L加对象的全限定名称来表示，例如： B - byte,C - char,D - double ,F - float ,I - int ,J - long ,S - short,Z - boolean,V - void,L-对象类型,如String - Ljava/lang/String; 对于数组类型来说，每一个维度使用一个前置的[来表示，例如：int[]表示为[I，String[][]表示为[[Ljava/lang/String 用描述符来描述方法时，按照先参数列表，后返回值的顺序描述，参数列表按照参数的严格顺序放在一组小括号“()”之内。如方法void inc()的描述符为()V，方法java.lang.String toString()的描述符为”()Ljava/lang/String；，方法int indexOf（char[]source,int sourceOffset,int sourceCount,char[]target,int targetOffset,int targetCount,int fromIndex）的描述符为（[CII[CIII）I。到此为止 常量池的所有信息就已分析完访问标志 按照整体结构来看，在常量池结束之后，紧接着的两个字节代表访问标志（access_flags），这个标志用于识别一些类或者接口层次的访问信息，包括：这个Class是类还是接口；是否定义为public类型；是否定义为abstract类型；如果是类的话，是否被声明为final等。具体的标志位以及标志的含义见表6-7access_flags中一共有16个标志位可以使用，当前只定义了其中8个 ，没有使用到的标志位要求一律为0。以上述代码为例为例，MyTest1是一个普通Java类，不是接口、枚举或者注解，被public关键字修饰但没有被声明为final和abstract，并且它使用了JDK 1.2之后的编译器进行编译，因此它的ACC_PUBLIC、ACC_SUPER标志应当为真，而ACC_FINAL、ACC_INTERFACE、ACC_ABSTRACT、ACC_SYNTHETIC、ACC_ANNOTATION、ACC_ENUM这6个标志应当为假，因此它的access_flags的值应为：0x0001+0x0020=0x0021。从图中可以看出，access_flags标志的确为0x0021。类索引、父类索引与接口索引集合​ 类索引（this_class）和父类索引（super_class）都是一个u2类型的数据，而接口索引集合（interfaces）是一组u2类型的数据的集合，Class文件中由这三项数据来确定这个类的继承关系。类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名。由于Java语言不允许多重继承，所以父类索引只有一个，除了java.lang.Object之外，所有的Java类都有父类，因此除了java.lang.Object外，所有Java类的父类索引都不为0。接口索引集合就用来描述这个类实现了哪些接口，这些被实现的接口将按implements语句（如果这个类本身是一个接口，则应当是extends语句）后的接口顺序从左到右排列在接口索引集合中。​ 类索引、父类索引和接口索引集合都按顺序排列在访问标志之后，类索引和父类索引用两个u2类型的索引值表示，它们各自指向一个类型为CONSTANT_Class_info的类描述符常量，通过CONSTANT_Class_info类型的常量中的索引值可以找到定义在CONSTANT_Utf8_info类型的常量中的全限定名字符串。对于接口索引集合，入口的第一项——u2类型的数据为接口计数器（interfaces_count），表示索引表的容量。如果该类没有实现任何接口，则该计数器值为0，后面接口的索引表不再占用任何字节。如上图所示字段表集合字段表（field_info）用于描述接口或者类中声明的变量。字段（field）包括类级变量以及实例级变量，但不包括在方法内部声明的局部变量。我们可以想一想在Java中描述一个字段可以包含什么信息？可以包括的信息有：字段的作用域（public、private、protected修饰符）、是实例变量还是类变量（static修饰符）、可变性（final）、并发可见性（volatile修饰符，是否强制从主内存读写）、可否被序列化（transient修饰符）、字段数据类型（基本类型、对象、数组）、字段名称。上述这些信息中，各个修饰符都是布尔值，要么有某个修饰符，要么没有，很适合使用标志位来表示。而字段叫什么名字、字段被定义为什么数据类型，这些都是无法固定的，只能引用常量池中的常量来描述。下图列出了字段表的最终格式。字段修饰符放在access_flags项目中，它与类中的access_flags项目是非常类似的，都是一个u2的数据类型，其中可以设置的标志位和含义见下图​ 很明显，在实际情况中，ACC_PUBLIC、ACC_PRIVATE、ACC_PROTECTED三个标志最多只能选择其一，ACC_FINAL、ACC_VOLATILE不能同时选择。接口之中的字段必须有ACC_PUBLIC、ACC_STATIC、ACC_FINAL标志，这些都是由Java本身的语言规则所决定的。​ 跟随access_flags标志的是两项索引值：name_index和descriptor_index。它们都是对常量池的引用，分别代表着字段的简单名称以及字段和方法的描述符。方法表集合​ Class文件存储格式中对方法的描述与对字段的描述几乎采用了完全一致的方式，方法表的结构如同字段表一样，依次包括了访问标志（access_flags）、名称索引（name_index）、描述符索引（descriptor_index）、属性表集合（attributes）几项，见下图。这些数据项目的含义也非常类似，仅在访问标志和属性表集合的可选项中有所区别。​ 因为volatile关键字和transient关键字不能修饰方法，所以方法表的访问标志中没有了ACC_VOLATILE标志和ACC_TRANSIENT标志。与之相对的，synchronized、native、strictfp和abstract关键字可以修饰方法，所以方法表的访问标志中增加了ACC_SYNCHRONIZED、ACC_NATIVE、ACC_STRICTFP和ACC_ABSTRACT标志。对于方法表，所有标志位及其取值可参见下图。 方法里的Java代码，经过编译器编译成字节码指令后，存放在方法属性表集合中一个名为”Code”的属性里面，属性表作为Class文件格式中最具扩展性的一种数据项目，将在后面详细讲解。​ 我们继续以代码中的Class文件为例对方法表集合进行分析，如下图所示，方法表集合的入口地址为：第一个u2类型的数据（即是计数器容量）的值为0x0003，代表集合中有三个方法（这三个方法为编译器添加的实例构造器＜init＞和源码中的两个方法）。第一个方法的访问标志值为0x001，也就是只有ACC_PUBLIC标志为真，名称索引值为0x0008，查常量池得方法名为”＜init＞”，描述符索引值为0x0009，对应常量为”()V”，属性表计数器attributes_count的值为0x0001就表示此方法的属性表集合有一项属性，属性名称索引为0x000A，即对应着，对应常量为”Code”，说明此属性是方法的字节码描述，在往后面没有用颜色进行标记的就是attribute_info，里面包含着方法的详细。Code属性详情： 00 0A：访问标记，通过查看上表6-12可以看出是public访问修饰符 00 00 00 3B：attribute属性长度59，既往后面59个字节都是该属性 00 02：操作数栈（Operand Stacks）深度的最大值为2。在方法执行的任意时刻，操作数栈都不会超过这个深度。虚拟机运行的时候需要根据这个值来分配栈帧（Stack Frame）中的操作栈深度。 00 01：代表了局部变量表所需的存储空间。在这里，max_locals的单位是Slot,Slot是虚拟机为局部变量分配内存所使用的最小单位。对于byte、char、float、int、short、boolean和returnAddress等长度不超过32位的数据类型，每个局部变量占用1个Slot，而double和long这两种64位的数据类型则需要两个Slot来存放。方法参数（包括实例方法中的隐藏参数”this”，也就是这里的1）、显式异常处理器的参数（Exception Handler Parameter，就是try-catch语句中catch块所定义的异常）、方法体中定义的局部变量都需要使用局部变量表来存放。另外，并不是在方法中用到了多少个局部变量，就把这些局部变量所占Slot之和作为max_locals的值，原因是局部变量表中的Slot可以重用，当代码执行超出一个局部变量的作用域时，这个局部变量所占的Slot可以被其他局部变量所使用，Javac编译器会根据变量的作用域来分配Slot给各个变量使用，然后计算出max_locals的大小。 00 00 00 0D：字节码指令的长度为13，即后面13个字节为字节码指令 2A - B1：对应的字节码指令，Oracle 指令信息以上所有字节码均可以找到对应指令信息 00 00：异常相关信息，由于这里是0，所以这里没有exception_info，其实exception自身也有自己的结构体，在这里就不进行过多展开 00 02：attribute_count信息以及后面2字节为attribute_info 00 00 00 0A​ 在Java语言中，要重载（Overload）一个方法，除了要与原方法具有相同的简单名称之外，还要求必须拥有一个与原方法不同的特征签名 ，特征签名就是一个方法中各个参数在常量池中的字段符号引用的集合，也就是因为返回值不会包含在特征签名中，因此Java语言里面是无法仅仅依靠返回值的不同来对一个已有方法进行重载的。但是在Class文件格式中，特征签名的范围更大一些，只要描述符不是完全一致的两个方法也可以共存。也就是说，如果两个方法有相同的名称和特征签名，但返回值不同，那么也是可以合法共存于同一个Class文件中的。属性表集合​ 分析完前面的方法后就只剩下最后的属性表（attribute_info）了，结构如图所示对应字节码：对应jclasslib：总结​ 以上就是对一个简单的java程序字节码的分析，关于方发表和属性表集合这两块本身还有更多的东西还没有展示，其实也没有太多知道的必要。了解其主要结构，配置字节码查看工具能读懂即可参考 深入理解JVM-张龙 深入理解JVM虚拟机（第二版）" }, { "title": "JVM虚拟机 - 类的双亲委托模型", "url": "/posts/JVM%E8%99%9A%E6%8B%9F%E6%9C%BA-%E7%B1%BB%E7%9A%84%E5%8F%8C%E4%BA%B2%E5%A7%94%E6%89%98%E6%A8%A1%E5%9E%8B/", "categories": "编程语言, jvm", "tags": "jvm, java", "date": "2020-04-06 05:56:08 +0800", "snippet": "类的双亲委托机制 有一个类（A.class）需要类加载器去加载，如果有父类，先让父类去加载，如此向上追溯，知道根 类加载器，然后根类加载器尝试去加载，加载成功则结束，加载失败，又往下，一层层的尝试去加载，最终如果都没有加载成功，则报错classnotfound；但是并不是所有的jvm都是这样，hotspot遵循这样规则。类加载时的动作 隐式装载， 程序在运行过程中当碰到通过new 等方式生成对象时，隐式调用类装载器加载对应的类到jvm中。 显式装载， 通过class.forname()等方法，显式加载需要的类 一个应用程序总是由n多个类组成，Java程序启动时，并不是一次把所有的类全部加载后再运行，它总是先把保证程序运行的基础类一次性加载到jvm中，其它类等到jvm用到的时候再加载，这样的好处是节省了内存的开销，因为java最早就是为嵌入式系统而设计的，内存宝贵，这是一种可以理解的机制，而用到时再加载这也是java动态性的一种体现数组class对象是jvm虚拟机在运行时动态创建的类的卸载 当MySample类被加载、连接和初始化后，它的生命周期就开始了。当代表MySample类的Class对象不再被引用，即不可触及是，Class对象就会结束生命周期，MySample类在方法去的数据就会被卸载，从而结束MySample类的生命周期 一个类何时结束生命周期，取决于Class对象合适结束生命周期 Java虚拟机自带的类加载器所加载的类，在虚拟机的生命周期中，始终不会被卸载。Java虚拟机自带的类加载器包括根类加载器、拓展类加载器和系统类加载器。Java虚拟机本身会始终引用这些类加载器，而这些类加载器则会始终引用他们所加载的Class对象，因此这些Class对象是始终可以触及的，只有自己自定义的类加载所加载的Class对象才存在被卸载的可能 命名空间 每个类加载器都有自己的命名空间，命名空间由该加载器及所有父加载器的类组成 在同一个命名空间中，不会出现类的完整名字（包括类的包名）相同的两个类 在不同的命名空间中，可能出现类的完整名（包括类的包名）相同的两个类不同类加载器的命名空间关系 同一命名空间的类是相互可见的 子类的命名空间包含所有父类加载器的命名空间。因此由子加载器加载的类能看见父类加载器加载的类，例如系统加载器加载的类能看见根类加载器加载的类 由父加载器加载的类不能看见子加载器加载的类 如果两个加载器之间没有直接或间接的父子关系，那么它们各自加载的类相互不可见例子：// 初始化两个自定义类加载器ClassLoader cl1 = new MyClassLoader();ClassLoader cl2 = new MyClassLoader();Class&amp;lt;?&amp;gt; clazz1 = cl1.loadClass(&quot;com.example.MyClass1&quot;)Class&amp;lt;?&amp;gt; clazz2 = cl2.loadClass(&quot;com.example.MyClass1&quot;)System.out.println(clazz1==clazz2) //false在上述例子中clazz1和clazz2就属于两个不同命名空间中相同的类，虽然他们是加载的相同的类，但是在JVM中他们并不相同。线程上下文类加载器概念 线程上下文类加载器是从JDK1.2开始引入的，类Thread中的getContextClassLoader()与setContextClassLoader(Classloader c)分别用来获取和设置上下文类加载器。 如果没有通过setContextClassLoader(Classloader c)来进行设置的话，线程将继承其弗雷德上下文类加载器。Java应用运行时的初始线程的上下文类加载器是系统类加载器。在线程中运行的代码可以通过该类加载器来类与资源。作用 父ClassLoader可以使用当前线程的Thread.currentThread().getContextClassLoader()所指定的ClassLoader加载类。这就改变了父ClassLoader不能使用子ClassLoader或其他没有直接父子关系的ClassLoader加载的类的情况，即改变了双亲委托模型。 在双亲委托模型下，类的加载是自下而上的，即下层的类加载器会委托上层进行加载。但是对于SPI（Service Provider Interface）来说，有些接口是Java核心库所提供的，而Java核心库是有类加载来进行加载的，而这些接口的实现确是来自不同厂商提供的Jar包，Java的启动类加载器默认是不同加载其他来源的Jar包，这样传统的双亲委托模型就无法满足SPI的要求，而通过给当前线程设置上线文类加载器，就可以由设置的上下文类加载器来实现对于接口类的加载。线程上下文的一般使用模式获取 - 使用 - 还原ClassLoader loader = Thread.currentThread().getContextClassLoader(); //获取try { //使用 Thread.currentThread().setContextClassLoader(targetTccl); myMethod();}finally { Thread.currentThread().setContextClassLoader(loader); //还原}总结类加载器双亲委托模型的好处： 可以确保Java和核心库的类型安全：所有Java应用都至少会引用java.lang.Object类，也就是说在运行期间，java.lang.Object这个类会被加载到Java虚拟机中；如果这个加载过程是由Java应用自己的类加载器完成，那么很可能就会在JVM中存在不同版本的java.lang.Object类，而且这些类相互不兼容，相互不可见。借助双亲委托机制，Java核心类库的加载工作都是由启动类加载器统一完成，从而确保Java所有应用使用的都是同一版本的Java核心类库，他们之间是相互兼容的 可以确保Java核心类库所提供的类不会被自定义的类所打扰。例如，我们自定义一个java.lang.Object类，他是无论如果不会被加载到JVM中的，而是由根类加载器去核心库中寻找 不同的类加载器可以为相同的名称（binary name）的类创建额外的命名空间。相同名称的类可以并存在Java虚拟机中，只需要用不同的类加载器来加载他们即可。不同类加载器所加载的类之间是不兼容的，这就相当于在Java虚拟机内部创建了一个又一个相互隔离的Java类空间，这类技术在很多框架中得到实际应用" }, { "title": "Flink - Flink编程结构", "url": "/posts/Flink-Flink%E7%BC%96%E7%A8%8B%E7%BB%93%E6%9E%84/", "categories": "大数据框架, flink", "tags": "flink", "date": "2020-01-15 00:00:00 +0800", "snippet": "Flink程序结构概述 任何程序都是需要有输入、处理、输出。那么Flink同样也是，Flink专业术语对应Source，map，Sink。而在进行这些操作前，需要根据需求初始化运行环境执行环境Flink 执行模式分为两种，一个是流处理、另一个是批处理。再选择好执行模式后，为了开始编写Flink程序，需要根据需求创建一个执行环境。Flink目前支持三种环境的创建方式： 获取一个已经有的environment 创建一个本地environment 创建一个远程environment 通常，只需要使用getExecutionEnvironment()。 它会根据你的环境来选择。 如果你在IDE中的本地环境中执行，那么它将启动本地执行环境。 否则，如果正在执行JAR，则Flink集群管理器将以分布式方式执行该程序。 流处理程序部分代码：val env = StreamExecutionEnvironment.getExecutionEnvironmentval text = env.socketTextStream(&quot;localhost&quot;, 9999)批处理程序部分代码：val env = ExecutionEnvironment.getExecutionEnvironmentval text = env.fromElements( &quot;Who&#39;s she?&quot;,&quot;Alice&quot;)数据源（Source）Flink的source到底是什么？为了更好地理解，我们这里给出下面一个简单典型的wordcount程序。//初始化流处理环境val env = StreamExecutionEnvironment.getExecutionEnvironment//指定数据源val text = env.socketTextStream(host, port, &#39;\\n&#39;)//对数据源传入的数据进行处理val windowCounts = text.flatMap { w =&amp;gt; w.split(&quot;\\\\s&quot;) } .map { w =&amp;gt; WordWithCount(w, 1) } .keyBy(&quot;word&quot;) .timeWindow(Time.seconds(5)) .sum(&quot;count&quot;)//输出结果windowCounts.print()在上述代码中val text = env.socketTextStream(host, port, &#39;\\n&#39;)就是指定数据源。Flink的source多种多样，例如我们可以根据不同的需求来自定义source。DataStream API基于Socket socketTextStream(host,port)：从套接字读取数据,只需指定要从中读取数据的主机和端口 socketTextStream(hostName,port,delimiter)：指定分隔符 socketTextStream(hostName,port,delimiter,maxRetry)：API尝试最大次数基于文件 readTextFile(path) ： 读取文本类型文件 readFile(fileInputFormat, path) ：读取非文本文件，需要指定输入格式 readFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) ：该方法为前两个方法的内部调用方法，可以根据给定的输入格式读取指定路径的文件。watchType、interval分别指定监控类型和监控间隔，监控类型包括三种： 当系统应仅处理新文件时使用FileMonitoringFunction.WatchType.ONLY_NEW_FILES 当系统仅追加文件内容时使用FileMonitoringFunction.WatchType.PROCESS_ONLY_APPENDED 当系统不仅要重新处理文件的追加内容而且还要重新处理文件中的先前内容时，将使用FileMonitoringFunction.WatchType.REPROCESS_WITH_APPENDED 自定义 addSource：附加新的source函数。 例如，要从Apache Kafka读取，可以使用addSource（new FlinkKafkaConsumer08 &amp;lt;&amp;gt;（…））。 请参阅connectors以获取更多内容。DataSet API基于文件 readTextFile（path）/ TextInputFormat ： 按行读取文件并将它们作为字符串返回。 readCsvFile（path）/ CsvInputFormat ： 解析逗号（或其他字符）分隔字段的文件。 返回元组，案例类对象或POJO的DataSet。 支持基本java类型及其Value对应作为字段类型。 readFileOfPrimitives（path，delimiter）/ PrimitiveInputFormat ： 使用给定的分隔符解析新行（或其他char序列）分隔的原始数据类型（如String或Integer）的文件。 readSequenceFile（Key，Value，path）/ SequenceFileInputFormat ； 创建JobConf并从指定路径读取文件，类型为SequenceFileInputFormat，Key class和Value类，并将它们返回为Tuple2 &amp;lt;Key，Value&amp;gt;。基于Collection fromCollection（Seq） ：从Seq创建数据集。 集合中的所有元素必须属于同一类型。 fromCollection（Iterator） ：从迭代器创建数据集。 该类指定迭代器返回的元素的数据类型。 fromElements（elements：_ *） ： 根据给定的对象序列创建数据集。 所有对象必须属于同一类型。 fromParallelCollection（SplittableIterator） ： 并行地从迭代器创建数据集。 该类指定迭代器返回的元素的数据类型。 generateSequence（from，to） ： 并行生成给定间隔中的数字序列。通用 readFile（inputFormat，path）/ FileInputFormat ： 接受文件输入格式。 createInput（inputFormat）/ InputFormat ： 接受通用输入格式。 处理在读取数据源以后就开始了数据的处理val windowCounts = text.flatMap { w =&amp;gt; w.split(&quot;\\\\s&quot;) } .map { w =&amp;gt; WordWithCount(w, 1) } .keyBy(&quot;word&quot;) .timeWindow(Time.seconds(5)) .sum(&quot;count&quot;)flatMap 、map 、keyBy、timeWindow、sum那么这些就是对数据的处理。更多算子信息： DataStream Transformations（流处理） DataSet Transformations（批处理）保存数据（Sink）在上述代码中windowCounts.print()也就是改程序的保存数据这里输出可以说是非常简单的。而sink当然跟source一样也是可以自定义的。因为Flink数据要保存到myslq，是不能直接保存的，所以需要自定义一个sink。不定义sink可以吗？可以的，那就是自己在写一遍，每次调用都复制一遍，这样造成大量的重复，所以我们需要自定义sink。那么常见的sink有哪些?如下：flink在批处理中常见的sink1.基于本地集合的sink（Collection-based-sink）2.基于文件的sink（File-based-sink）DataStream Data SinkDataSet Data Sink参考Flink程序结构" }, { "title": "Flink - Flink架构概述", "url": "/posts/Flink-Flink%E6%9E%B6%E6%9E%84%E6%A6%82%E8%BF%B0/", "categories": "大数据框架, flink", "tags": "flink", "date": "2020-01-08 00:00:00 +0800", "snippet": "Flink概述1.Flink架构至下而上： Deploy（部署）：Flink 支持本地运行、能在独立集群或者在被 YARN 或 Mesos 管理的集群上运行， 也能部署在云上，即一共有三种部署模式：本地部署、Yarn模式、远程模式。 Runtim（运行）：Flink 的核心是分布式流式数据引擎，意味着数据以一次一个事件的形式被处理。 API：DataStream、DataSet、Table、SQL API。 拓展库：Flink 还包括用于复杂事件处理，机器学习，图形处理和 Apache Storm 兼容性的专用代码库。2.Flink组件Flink工作原理Job Managers、Task Managers、客户端（Clients）Flink程序需要提交给Client。 然后，Client将作业提交给Job Manager。 Job Manager负责协调资源分配和作业执行。 它首先要做的是分配所需的资源。 资源分配完成后，任务将提交给相应的Task Manager。 在接收任务时，Task Manager启动一个线程以开始执行。 执行到位时，Task Manager会继续向Job Manager报告状态更改。 可以有各种状态，例如开始执行，正在进行或已完成。 作业执行完成后，结果将发送回Client。Flink 运行时包含两类进程： JobManagers （也称为 masters）协调分布式计算。它们负责调度任务、协调 checkpoints、协调故障恢复等。每个 Job 至少会有一个 JobManager。高可用部署下会有多个 JobManagers，其中一个作为 leader，其余处于 standby 状态。这个进程由三个不同的组件组成： ResourceManager ResourceManager 负责 Flink 集群中的资源提供、回收、分配 - 它管理 task slots，这是 Flink 集群中资源调度的单位（请参考TaskManagers）。Flink 为不同的环境和资源提供者（例如 YARN、Kubernetes 和 standalone 部署）实现了对应的 ResourceManager。在 standalone 设置中，ResourceManager 只能分配可用 TaskManager 的 slots，而不能自行启动新的 TaskManager。 Dispatcher Dispatcher 提供了一个 REST 接口，用来提交 Flink 应用程序执行，并为每个提交的作业启动一个新的 JobMaster。它还运行 Flink WebUI 用来提供作业执行信息。 JobMaster JobMaster 负责管理单个JobGraph的执行。Flink 集群中可以同时运行多个作业，每个作业都有自己的 JobMaster。 TaskManagers（也称为 workers）执行 dataflow 中的 tasks（准确来说是 subtasks ），并且缓存和交换数据 streams。每个 Job 至少会有一个 TaskManager。JobManagers 和 TaskManagers 有多种启动方式：直接在机器上启动（该集群称为 standalone cluster），在容器或资源管理框架，如 YARN 或 Mesos，中启动。TaskManagers 连接到 JobManagers，通知后者自己可用，然后开始接手被分配的工作。客户端（Client）虽然不是运行时（runtime）和作业执行时的一部分，但它是被用作准备和提交 dataflow 到 JobManager 的。提交完成之后，客户端可以断开连接，也可以保持连接来接收进度报告。客户端既可以作为触发执行的 Java / Scala 程序的一部分，也可以在命令行进程中运行./bin/flink run ...。Task Slots 的隔离&amp;amp;共享TaskManager并不是最细粒度的概念，每个TaskManager像一个容器一样，包含一个多或多个Slot。Slot是TaskManager资源粒度的划分，每个Slot都有自己独立的内存。所有Slot平均分配TaskManger的内存，比如TaskManager分配给Solt的内存为8G，两个Slot，每个Slot的内存为4G，四个Slot，每个Slot的内存为2G，值得注意的是，Slot仅划分内存，不涉及cpu的划分。同时Slot是Flink中的任务执行器，每个Slot可以运行多个task，而且一个task会以单独的线程来运行。Slot主要的好处有以下几点： 可以起到隔离内存的作用，防止多个不同job的task竞争内存。 Slot的个数就代表了一个Flink程序的最高并行度，简化了性能调优的过程 允许多个Task共享Slot，提升了资源利用率默认情况下，Flink 允许 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，只要它们来自同一个 job。因此，一个 slot 可能会负责这个 job 的整个管道（pipeline）。允许 slot sharing 有两个好处： Flink 集群需要与 job 中使用的最高并行度一样多的 slots。这样不需要计算作业总共包含多少个 tasks（具有不同并行度）。 更好的资源利用率。在没有 slot sharing 的情况下，简单的 subtasks（source/map()）将会占用和复杂的 subtasks （window）一样多的资源。通过 slot sharing，将示例中的并行度从 2 增加到 6 可以充分利用 slot 的资源，同时确保繁重的 subtask 在 TaskManagers 之间公平地获取资源。Flink 内部通过 SlotSharingGroup 和 CoLocationGroup 来定义哪些 task 可以共享一个 slot， 哪些 task 必须严格放到同一个 slot。根据经验，合理的 slots 数量应该和 CPU 核数相同。在使用超线程（hyper-threading）时，每个 slot 将会占用 2 个或更多的硬件线程上下文（hardware thread contexts）。参考Flink 基本工作原理分布式运行时环境" }, { "title": "JVM虚拟机 - 类的加载、连接、初始化", "url": "/posts/JVM%E8%99%9A%E6%8B%9F%E6%9C%BA-%E7%B1%BB%E7%9A%84%E5%8A%A0%E8%BD%BD-%E8%BF%9E%E6%8E%A5-%E5%88%9D%E5%A7%8B%E5%8C%96/", "categories": "编程语言, java", "tags": "java", "date": "2019-12-20 00:00:00 +0800", "snippet": "类的加载、连接、初始化 在Java代码中，类型的加载、连接和初始化过程都是在程序运行期间完成的 由此提供了更大的灵活性以及更多的可能性 加载：查找并加载二进制类 连接 验证：确保加载类的正确性 类文件的结构检测 语义检测 字节码验证 二进制兼容性检测 准备：为类的静态变量分配内存，并将其初始化为默认值。例如有static int a=1，其中int类型的默认值为0，则a的值为0 解析：把类中的符号引用转换为直接引用，类之间的引用采用指针的方式直接指定 初始化：为静态变量赋予正确的初始值，例如将前面的a复制为1。所有的Java虚拟机实现必须在每个类或接口被Java程序“首次主动使用”才会初始化他们 类的使用 主动使用（类实例化） 被动使用 垃圾回收和对象终结 类卸载类的加载定义 类的加载是指将类的.class文件的二进制数据读入到内存中，将其放在运行时数据的方法区内，然后在内存中创建一个java.lang.Class对象（规范并未说Class对象放在哪，HotSpSpot虚拟机将其放在方法区中），其中Class对象封装了方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口 JVM规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或者存在错误，类加载器必须在程序首次主动使用该类才会抛出错误（LinkageError错误），也就是说如果程序一直没主动使用该类，则类加载器就不会报错 ps：与初始化不同，类的加载并不需要等到某个类被“首次主动使用”时才加载他加载的流程加载时类加载过程的第一个阶段，在加载阶段，虚拟机需要完成以下三件事情: 通过一个类的全限定名来获取其定义的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在Java堆中生成一个代表这个类的java.lang.Class对象，作为对方法区中这些数据的访问入口。加载器一共存在两类类加载器 Java虚拟机自带的加载器 根类加载器（BootStrap） 拓展类加载器（Extension） 系统（应用）类加载器（System） 用户自定义加载器 java.lang.ClassLoader的子类 用户可以自定义类的加载方式 类的加载方式 从本地系统中直接加载 通过网络下载.class文件 从zip、jar等归档文件中加载.class文件 从转有的数据库中提取.class文件 将java源文件动态编译为.class文件（动态代理）连接验证：确保被加载的类的正确性验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。验证阶段大致会完成4个阶段的检验动作: 文件格式验证: 验证字节流是否符合Class文件格式的规范；例如: 是否以0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。 元数据验证: 对字节码描述的信息进行语义分析(注意: 对比javac编译阶段的语义分析)，以保证其描述的信息符合Java语言规范的要求；例如: 这个类是否有父类，除了java.lang.Object之外。 字节码验证: 通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证: 确保解析动作能正确执行。 验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。准备：为类的静态变量分配内存，并将其初始化为默认值准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意: 这时候进行内存分配的仅包括类变量(static)，而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。 这里所设置的初始值通常情况下是数据类型默认的零值(如0、0L、null、false等)，而不是被在Java代码中被显式地赋予的值。假设一个类变量的定义为: public static int value = 3；那么变量value在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何Java方法，而把value赋值为3的put static指令是在程序编译后，存放于类构造器&amp;lt;clinit&amp;gt;()方法之中的，所以把value赋值为3的动作将在初始化阶段才会执行。解析：把类中的符号引用转换为直接引用解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。符号引用就是一组符号来描述目标，可以是任何字面量。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。类的初始化 在初始化阶段，Java虚拟机执行类的初始化语句，为类的静态变量赋初值。在程序中，静态变量的初始化有两种途径：（1）在静态变量声明处进行初始化；（2）在静态代码块中初始化类的主动使用 创建类的实例。例如，new Object(),但是声明对象不会 为实例的对象分配内存 为实例的变量赋默认值 为实例变量赋正确的初始值（此过程与初始化过程差不多） 访问某个类的静态方法，或者对改静态方法复制 调用某类的静态方法 反射，例如，Class.forName(&quot;com.test.Test&quot;) 初始化一个类的子类 Java虚拟机启动时被标明为启动类的类 JDK1.7开始提供动态语言支持：java.lang.invoke.MethodHandle实例的解析结果REF_getStatic，REF_putStatic，REF_invokeStatic句柄对应的类没有初始化，则初始化除了以上7种情况，其他使用Java类的方式均被看作类的被动使用，都不会导致类的初始化几种特殊情况 情形一通过子类对象调用父类的静态变量，并未主动使用子类，子类的静态代码块不会被初始化，但是此种情况依然会加载MySon.class文件public class MyTest1 { public static void main(String[] args) { System.out.println(MySon.str); }}class MyParent{ public static String str = &quot;it&#39;s parent obj&quot;; static { System.out.println(&quot;hello world&quot;); }}class MySon extends MyParent{ static { System.out.println(&quot;hello world son&quot;); }} 情形二将MyParen类修改中str字符串添加final修饰符，则MyParent和MySon均不会被加载也不会被初始化，也不会被加载public class MyTest1 { public static void main(String[] args) { System.out.println(MySon.str); System.out.println(MyParent.uuid); }}class MyParent{ public static final String str = &quot;it&#39;s parent obj&quot;; public static final String uuid = UUID.randomUUID().toString(); static { System.out.println(&quot;hello world&quot;); }}原因：final修饰的常量在编译阶段会存入调用这个常量的类的常量池中，本质上调用类并没有直接引用到定义常量的类，因此并不会触发定义常量的类的初始化。但是如果是使用的UUID.randomUUID()这样的变量，还是会初始化该对象 情形三初始化某个对象的数组并不会导致该类被初始化public class MyTest3 { public static void main(String[] args) { MyParent3[][] myParent3s = new MyParent3[4][4]; System.out.println(myParent3s.getClass()); }}class MyParent3{ static { System.out.println(&quot;hello world&quot;); }}//结果//class [[Lvideo_lecture.classloader.MyParent3;从打印的类来看，Java虚拟机在运行时会生成一个数组类型的类，如果是二维数组，则class名含有两个[[` 情形四当一个接口初始化时，并不要求其父类接口都完成初始化只有当真正使用到父接口的时候（如引用接口所定义的常量时），才会初始化public class MyTest4 { public static void main(String[] args) { /*并不会打印&quot;im parent4&quot;，但是将interface改为class时会打印*/ System.out.println(MySon4.t2); }}interface MyParent4{ public static final Thread t1 = new Thread(){ { System.out.println(&quot;im parent4&quot;); } };}interface MySon4 extends MyParent4{ public static final Thread t2 = new Thread(){ { System.out.println(&quot;im son4&quot;); } };} 情形五ClassLoader类的loadClass并不会导致类的主动使用，也就是并不会导致类的初始化但是Class.forName()，即反射是会导致类的主动使用的public class MyTest6 { public static void main(String[] args) throws ClassNotFoundException { final ClassLoader loader = ClassLoader.getSystemClassLoader(); Class&amp;lt;?&amp;gt; clazz = loader.loadClass(&quot;video_lecture.classloader.CL&quot;); System.out.println(clazz); System.out.println(&quot;----------------&quot;); clazz = Class.forName(&quot;video_lecture.classloader.CL&quot;); System.out.println(clazz); }}class CL{ static { System.out.println(&quot;im static block&quot;); }}类的使用类访问方法区内的数据结构的接口， 对象是Heap区的数据。类的卸载Java虚拟机将结束生命周期的几种情况 执行了System.exit()方法 程序正常执行结束 程序在执行过程中遇到了异常或错误而异常终止 由于操作系统出现错误而导致Java虚拟机进程终止助记符ldc：int、float、String类型常量值从常量池中推送至栈顶bipush：将单字节（-128~127）的常量值推送至栈顶sipush：表示将一个短整型常量值（ -32768-32767）推送至栈顶iconst_m1-iconst_5:将-1-5推送至栈顶（一共7个）参考 JVM加载机制" }, { "title": "设计模式 - 行为型模式", "url": "/posts/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A1%8C%E4%B8%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/", "categories": "编程语言, 设计模式", "tags": "设计模式", "date": "2019-10-16 00:00:00 +0800", "snippet": "结构型模式 行为型模式(Behavioral Pattern)是对在不同的对象之间划分责任和算法的抽象化。 行为型模式不仅仅关注类和对象的结构，而且重点关注它们之间的相互作用。 通过行为型模式，可以更加清晰地划分类与对象的职责，并研究系统在运行时实例对象 之间的交互。在系统运行时，对象并不是孤立的，它们可以通过相互通信与协作完成某些复杂功能，一个对象在运行时也将影响到其他对象的运行。 行为型模式分为类行为型模式和对象行为型模式两种： 类行为型模式：类的行为型模式使用继承关系在几个类之间分配行为，类行为型模式主要通过多态等方式来分配父类与子类的职责。 对象行为型模式：对象的行为型模式则使用对象的聚合关联关系来分配行为，对象行为型模式主要是通过对象关联等方式来分配两个或多个类的职责。根据“合成复用原则”，系统中要尽量使用关联关系来取代继承关系，因此大部分行为型设计模式都属于对象行为型设计模式。 包含模式 职责链模式(Chain of Responsibility) 命令模式(Command) 解释器模式(Interpreter) 迭代器模式(Iterator) 中介者模式(Mediator) 备忘录模式(Memento) 观察者模式(Observer) 状态模式(State) 策略模式(Strategy) 模板方法模式(Template Method) 访问者模式(Visitor)模块方法模式模块方法模式讲解对应代码" }, { "title": "设计模式 - 结构型模式", "url": "/posts/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E7%BB%93%E6%9E%84%E5%9E%8B%E6%A8%A1%E5%BC%8F/", "categories": "编程语言, 设计模式", "tags": "设计模式", "date": "2019-10-16 00:00:00 +0800", "snippet": "结构型模式 结构型模式(Structural Pattern)描述如何将类或者对 象结合在一起形成更大的结构，就像搭积木，可以通过 简单积木的组合形成复杂的、功能更为强大的结构。 结构型模式可以分为类结构型模式和对象结构型模式： 类结构型模式关心类的组合，由多个类可以组合成一个更大的 系统，在类结构型模式中一般只存在继承关系和实现关系。 - 对象结构型模式关心类与对象的组合，通过关联关系使得在一 个类中定义另一个类的实例对象，然后通过该对象调用其方法。 根据“合成复用原则”，在系统中尽量使用关联关系来替代继 承关系，因此大部分结构型模式都是对象结构型模式。\\ 适配器模式 桥接模式 装饰模式 组合模式 外观模式 享元模式 代理模式适配器模式适配器模式讲解对应代码桥接模式桥接模式讲解对应代码装饰模式装饰模式讲解对应代码组合模式组合模式讲解对应代码享元模式享元模式讲解对应代码代理模式代理模式讲解对应代码外观模式外观模式讲解对应代码" }, { "title": "设计模式 - 创建型模式", "url": "/posts/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%88%9B%E5%BB%BA%E5%9E%8B%E6%A8%A1%E5%BC%8F/", "categories": "编程语言, 设计模式", "tags": "设计模式", "date": "2019-10-16 00:00:00 +0800", "snippet": "创建型模式 单例模式 抽象工厂模式 原型模式 创建者模式 工厂模式单例模式单例模式介绍 所谓类的单例设计模式，就是采取一定的方法保证在整个的软件系统中，对某个类只能存在一个对象实例，并且该类只提供一个取得其对象实例的方法(静态方法)。 比如 Hibernate 的 SessionFactory，它充当数据存储源的代理，并负责创建 Session 对象。SessionFactory 并不是轻量级的，一般情况下，一个项目通常只需要一个 SessionFactory 就够，这是就会使用到单例模式。单例模式中的实现 构造器私有化 (防止 new ) 类的内部创建对象 向外暴露一个静态的公共方法。getInstance代码实现如下：单例模式的7种实现单例模式注意事项和细节说明 单例模式保证了 系统内存中该类只存在一个对象，节省了系统资源，对于一些需要频繁创建销毁的对象，使用单例模式可以提高系统性能 当想实例化一个单例类的时候，必须要记住使用相应的获取对象的方法，而不是使用 new 单例模式使用的场景：需要频繁的进行创建和销毁的对象、创建对象时耗时过多或耗费资源过多(即：重量级对象)，但又经常用到的对象、 工具类对象、频繁访问数据库或文件的对象(比如 数据源、session 工厂等)工厂模式工厂模式讲解对应代码原型模式定义 原型模式是创建型模式的一种，其特点在于通过“复制”一个已经存在的实例来返回新的实例,而不是新建实例。被复制的实例就是我们所称的“原型”，这个原型是可定制的。 原型模式多用于创建复杂的或者耗时的实例，因为这种情况下，复制一个已经存在的实例使程序运行更高效；或者创建值相等，只是命名不一样的同类数据。解决问题在传统模式下，复制一个对象的缺点： 优点是比较好理解，简单易操作。 在创建新的对象时，总是需要重新获取原始对象的属性，如果创建的对象比较复杂时，效率较低 总是需要重新初始化对象，而不是动态地获得对象运行时的状态, 不够灵活Java 中 Object 类是所有类的根类，Object 类提供了一个 clone()方法，该方法可以将一个 Java 对象复制一份，但是需要实现 clone的Java类必须要实现一个接口Cloneable，该接口表示该类能够复制且具有复制的能力 =&amp;gt;原型模式" }, { "title": "设计模式 - 七大原则", "url": "/posts/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E4%B8%83%E5%A4%A7%E5%8E%9F%E5%88%99/", "categories": "编程语言, 设计模式", "tags": "设计模式", "date": "2019-10-11 00:00:00 +0800", "snippet": "设计模式七大原则 单一职责原则 接口隔离原则 依赖倒转原则 里氏替换原则 开闭原则 迪米特法则 合成复用原则单一职责原则定义： 即对类来说，一个类应该只负责一个职责。例如，存在一个A类，负责两个不同职责，职责1、职责1，当职责1需求变更需要改变代码时，会对职责2产生影响，则需要将A类拆分为两个不同的类来处理两个职责图解：注意事项和细节 降低类的复杂度，一个类只负责一个职责 提高类的可读性，可维护性 降低变更引起的风险接口隔离原则定义： 客户端不应该依赖它不需要的接口，即一个类对另外一个类的依赖性应当是建立在最小的接口上的。 一个接口代表一个角色，不应当将不同的角色都交给一个接口。没有关系的接口合并在一起，形成一个臃肿的大接口，这是对角色和接口的污染。图解：依赖倒转原则定义： 高层模块不应该依赖底层模块，二者都应该依赖抽象 抽象不应该依赖细节，细节应该依赖抽象 依赖倒转（倒置）的中心思想是面向接口编程 依赖倒转是基于这样的设计理念：相对于细节的多变性，抽象的东西要稳定的多。以抽象搭建的框架会比以细节搭建的框架稳定。在Java中，抽象指接口或者抽象类，细节就是具体实现的类 使用接口或抽象类的目的就是制定规范，而不涉及任何操作，把展示细节的任务交给具体实现的类 图解：&amp;lt;img src=&quot;https://blog-1253533258.cos.ap-shanghai.myqcloud.com/2019-10-8/%E4%BE%9D%E8%B5%96%E5%80%92%E8%BD%AC%E5%8E%9F%E5%88%99.png&quot; style=&quot;zoom:67%;&quot; /&amp;gt;注意事项和细节 低级模块尽量都有自己的接口或抽象类，或者两者都有，程序稳定性会更好 变量的声明类型尽量是接口或抽象类，这样我们的变量引用和实际对象间就存在一个缓冲层，利于程序拓展优化 继承时遵循里氏原则里氏原则定义： 有类为T1的对象o1，和继承类T1的子类T2的对象o2，在程序中将o1替换为o2，程序不受影响。即，派生类（子类）对象可以在程式中代替其基类（超类）对象图解：注意事项和细节 在使用继承时，尽量遵循里氏替换原则，在子类中尽量不要重写父类的方法 里氏替换原则告诉我们，继承实际上让两个类耦合性增强了，在适当的情况下，可以通过 聚合、 组合、赖、依赖来解决问题。开闭原则定义： 开闭原则（Open Closed Principle）是编程中最基础、最重要的设计原则 一个软件实体如类，模块和函数应该对扩展开放( 对提供方)，对修改关闭( 对使用方)。用抽象构建框架，用实现扩展细节。 当软件需要变化时，尽量通过扩展软件实体的行为来实现变化，而不是通过修改已有的代码来实现变化。 图解：迪米特法则定义： 一个对象应该对其他对象保持最少的了解 类与类关系越密切，耦合度越大 迪米特法则(Demeter Principle)又叫最少知道原则，即一个类对自己依赖的类知道的越少越好。也就是说，对于被依赖的类不管多么复杂，都尽量将逻辑封装在类的内部。对外除了提供的 public 方法，不对外泄露任何信息 迪米特法则还有个更简单的定义：只与直接的朋友通信 直接的朋友：每个对象都会与其他对象有耦合关系，只要两个对象之间有耦合关系，我们就说这两个对象之间是朋友关系。耦合的方式很多，依赖，关联，组合，聚合等。其中，我们称出现成员变量， 方法参数， 方法返回值中的类为直接的朋友，而出现在局部变量中的类不是直接的朋友。也就是说，陌生的类最好不要以局部变量的形式出现在类的内部 代码解析：//学校总部员工类class Employee { private String id; //省略get&amp;amp;set方法 ...}//学院的员工类class CollegeEmployee { private String id; //省略get&amp;amp;set方法 ...}//管理学院员工的管理类class CollegeManager { //返回学院的所有员工 public List&amp;lt;CollegeEmployee&amp;gt; getAllEmployee() { List&amp;lt;CollegeEmployee&amp;gt; list = newArrayList&amp;lt;CollegeEmployee&amp;gt;(); for (int i = 0; i &amp;lt; 10; i++) { //这里我们增加了 10 个员工到 list CollegeEmployee emp = new CollegeEmployee(); emp.setId(&quot;学院员工 id= &quot; + i); list.add(emp); } return list; }}//学校管理类//分析 SchoolManager 类的直接朋友类有哪些 Employee、CollegeManager//CollegeEmployee 不是 直接朋友 而是一个陌生类，这样违背了 迪米特法则class SchoolManager { //返回学校总部的员工 public List&amp;lt;Employee&amp;gt; getAllEmployee() { List&amp;lt;Employee&amp;gt; list = newArrayList&amp;lt;Employee&amp;gt;(); for (int i = 0; i &amp;lt; 5; i++) { //这里我们增加了 5 个员工到 list Employee emp = new Employee(); emp.setId(&quot;学校总部员工 id= &quot; + i); list.add(emp); } return list; } //该方法完成输出学校总部和学院员工信息(id) void printAllEmployee(CollegeManager sub) { //分析问题 //1. 这里的 CollegeEmployee 不是 SchoolManager 的直接朋友 //2. CollegeEmployee 是以局部变量方式出现在 SchoolManager //3. 违反了 迪米特法则 //获取到学院员工 List&amp;lt;CollegeEmployee&amp;gt; list1 = sub.getAllEmployee(); System.out.println(&quot;------------学院员工------------&quot;); for (CollegeEmployee e : list1) { System.out.println(e.getId()); } //获取到学校总部员工 List&amp;lt;Employee&amp;gt; list2 = this.getAllEmployee(); System.out.println(&quot;------------学校总部员工------------&quot;); for (Employee e : list2) { System.out.println(e.getId()); } }}代码改进方向 前面设计的问题在于 SchoolManager 中，CollegeEmployee **类并不是SchoolManager**类的直接朋友 按照迪米特法则，应该避免类中出现这样非直接朋友关系的耦合 对代码按照迪米特法则，应当将打印的方法学院员工的代码块进行转移 //管理学院员工的管理类class CollegeManager { //返回学院的所有员工 public List&amp;lt;CollegeEmployee&amp;gt; getAllEmployee() { List&amp;lt;CollegeEmployee&amp;gt; list = newArrayList&amp;lt;CollegeEmployee&amp;gt;(); for (int i = 0; i &amp;lt; 10; i++) { //这里我们增加了 10 个员工到 list CollegeEmployee emp = new CollegeEmployee(); emp.setId(&quot;学院员工 id= &quot; + i); list.add(emp); } return list; } //输出学院员工的信息，移除掉SchoolManager中的打印学院员工代码 public void printEmployee() { //获取到学院员工 List&amp;lt;CollegeEmployee&amp;gt; list1 = getAllEmployee(); System.out.println(&quot;------------学院员工------------&quot;); for (CollegeEmployee e : list1) { System.out.println(e.getId()); } }}注意事项和细节 迪米特法则的核心是降低类之间的耦合 但是注意：由于每个类都减少了不必要的依赖，因此迪米特法则只是要求降低类间(对象间)耦合关系， 并不是要求完全没有依赖关系合成复用原则（Composite Reuse Principle） 原则是尽量使用关联/聚合的方式，而不是使用继承图解：设计原则核心思想 找出应用中可能需要变化之处，把它们独立出来，不要和那些不需要变化的代码混在一起。 针对接口编程，而不是针对实现编程。 为了交互对象之间的 松耦合设计而努力" }, { "title": "Spark - 累加器的陷阱", "url": "/posts/Spark-%E7%B4%AF%E5%8A%A0%E5%99%A8%E7%9A%84%E9%99%B7%E9%98%B1/", "categories": "大数据框架, spark", "tags": "spark", "date": "2019-09-14 00:00:00 +0800", "snippet": "目录 累加器（Accumulator）简介 累加器使用陷阱 原因分析&amp;amp;解决方案累加器（Accumulator）简介累加器（Accumulator）是Spark提供的累加器，顾名思义，该变量只能够增加。由Driver端进行初始变量，Task再对声明的变量进行累加操作。可以为Accumulator命名，这样就会在Spark web ui中看到每个节点的计数，以及累加后的值，可以帮助你了解程序运行的情况。累加器使用的陷阱在前段时间写项目时用累加器稽核数据量，结果发现稽核的数据输入量和输出量明显不同，此时要么是程序存在问题，要么是累加器使用有问题，从最终生成的结果文件中可以看出，是累加器的使用问题下面来看一个Demoval conf = new SparkConf() .setAppName(&quot;Accumulator Demo&quot;) .setMaster(&quot;local&quot;) val sc = new SparkContext(conf) sc.setLogLevel(&quot;WARN&quot;) val example = sc.longAccumulator(&quot;Example&quot;) val byKey = sc .parallelize(1 to 10) .map(x=&amp;gt;{ if(x%2==1){ example.add(-1) (&quot;奇数&quot;,1) }else{ (&quot;偶数&quot;,1) } }) byKey.foreach(println(_)) println(&quot;累加后的值：&quot;+example.value) println(byKey.count()) println(&quot;累加后的值：&quot;+example.value)结果：可以看出，如果一个算子在最终计算两次，则累加器也会同样增加两次那我们如果将涉及到累加的算子缓存会怎么样呢，修改部分代码val byKey = sc .parallelize(1 to 10) .map(x=&amp;gt;{ if(x%2==1){ example.add(1) (&quot;奇数&quot;,1) }else{ (&quot;偶数&quot;,1) } }).persist() //将计算结果进行缓存结果：原因分析&amp;amp;解决方案官方对这个问题的解释如下描述:For accumulator updates performed inside actions only, Spark guarantees that each task’s update to the accumulator will only be applied once, i.e. restarted tasks will not update the value. In transformations, users should be aware of that each task’s update may be applied more than once if tasks or job stages are re-executed.我们都知道，spark中的一系列transform操作会构成一串长的任务链，此时需要通过一个action操作来触发，accumulator也是一样。因此在一个action操作之前，你调用value方法查看其数值，肯定是没有任何变化的。所以在第一次foreach(action操作)之后，我们发现累加器的数值变成了5，是我们要的答案。之后又对新产生的的byKey进行了一次count(action操作)，其实这个时候又执行了一次map(transform)操作，所以累加器又增加了5。最终获得的结果变成了10。既然已经知道了造成的原因，那就是使用累加器的过程中只能使用一次action的操作才能保证结果的准确性。当然也可以通过切断依赖关系，例如触发一次Shuffle，Spark 会自动缓存Shuffle后生成的RDD（使用的Spark2.1，其他版本暂时不清楚），当然也可以通过Cache()、Persist()进行切断参考 Spark累加器使用的陷阱" }, { "title": "Spark - RDD&amp;持久化", "url": "/posts/Spark-RDD&%E6%8C%81%E4%B9%85%E5%8C%96/", "categories": "大数据框架, spark", "tags": "spark", "date": "2019-09-04 00:00:00 +0800", "snippet": "1.RDD持久化简介Spark 中一个很重要的能力是将数据持久化（或称为缓存），在多个操作间都可以访问这些持久化的数据。当持久化一个 RDD 时，每个节点的其它分区都可以使用 RDD 在内存中进行计算，在该数据上的其他 action 操作将直接使用内存中的数据。这样会让以后的 action 操作计算速度加快（通常运行速度会加速 10 倍）。缓存是迭代算法和快速的交互式使用的重要工具。RDD 可以使用 persist() 方法或 cache() 方法进行持久化。数据将会在第一次 action 操作时进行计算，并缓存在节点的内存中。Spark 的缓存具有容错机制，如果一个缓存的 RDD 的某个分区丢失了，Spark 将按照原来的计算过程，自动重新计算并进行缓存。在 shuffle 操作中（例如 reduceByKey），即便是用户没有调用 persist 方法，Spark 也会自动缓存部分中间数据。这么做的目的是，在 shuffle 的过程中某个节点运行失败时，不需要重新计算所有的输入数据。如果用户想多次使用某个 RDD，强烈推荐在该 RDD 上调用 persist 方法。2. RDD的cache和persist的区别先看看cache()源码 /** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) /** * Persist this RDD with the default storage level (`MEMORY_ONLY`). */ def cache(): this.type = persist()可以看出cache调用的任然是persist()方法，默认级别是StorageLevel.MEMORY_ONLY，也就是persis可以自定义缓存级别，而cache为默认级别，当然直接使用persist()也是默认级别3. Persist的各个缓存级别persist的各个级别都存在StorageLevel对象里面，直接看看StorageLevel里的方法 val NONE = new StorageLevel(false, false, false, false) val DISK_ONLY = new StorageLevel(true, false, false, false) val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(true, true, true, false, 1)可以看出，一共有12个级别，在看看StorageLevel的构造方法class StorageLevel private( private var _useDisk: Boolean, private var _useMemory: Boolean, private var _useOffHeap: Boolean, private var _deserialized: Boolean, private var _replication: Int = 1) extends Externalizable { ...}可以看到StorageLevel类的主构造器包含了5个参数： useDisk：使用硬盘（外存） useMemory：使用内存 useOffHeap：使用堆外内存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。 deserialized：反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象 replication：备份数（在多个节点上备份）理解了这5个参数，StorageLevel 的12种缓存级别就不难理解了。 MEMORY_ONLY : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，部分数据分区将不再缓存，在每次需要用到这些数据时重新进行计算。这是默认的级别。 MEMORY_AND_DISK : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，将未缓存的数据分区存储到磁盘，在需要使用这些分区时从磁盘读取。 MEMORY_ONLY_SER : 将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式会比反序列化对象的方式节省很多空间，尤其是在使用 fast serializer时会节省更多的空间，但是在读取时会增加 CPU 的计算负担。 MEMORY_AND_DISK_SER : 类似于 MEMORY_ONLY_SER ，但是溢出的分区会存储到磁盘，而不是在用到它们时重新计算。 DISK_ONLY : 只在磁盘上缓存 RDD。 MEMORY_ONLY_2，MEMORY_AND_DISK_2，等等 : 与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本。 OFF_HEAP（实验中）: 类似于 MEMORY_ONLY_SER ，但是将数据存储在 off-heap memory，这需要启动 off-heap 内存。4.如何选择存储级别Spark 的存储级别的选择，核心问题是在内存使用率和 CPU 效率之间进行权衡。建议按下面的过程进行存储级别的选择 : 如果使用默认的存储级别（MEMORY_ONLY），存储在内存中的 RDD 没有发生溢出，那么就选择默认的存储级别。默认存储级别可以最大程度的提高 CPU 的效率,可以使在 RDD 上的操作以最快的速度运行。 如果内存不能全部存储 RDD，那么使用 MEMORY_ONLY_SER，并挑选一个快速序列化库将对象序列化，以节省内存空间。使用这种存储级别，计算速度仍然很快。 除了在计算该数据集的代价特别高，或者在需要过滤大量数据的情况下，尽量不要将溢出的数据存储到磁盘。因为，重新计算这个数据分区的耗时与从磁盘读取这些数据的耗时差不多。 如果想快速还原故障，建议使用多副本存储级别（例如，使用 Spark 作为 web 应用的后台服务，在服务出故障时需要快速恢复的场景下）。所有的存储级别都通过重新计算丢失的数据的方式，提供了完全容错机制。但是多副本级别在发生数据丢失时，不需要重新计算对应的数据库，可以让任务继续运行。5.删除数据Spark 自动监控各个节点上的缓存使用率，并以最近最少使用的方式（LRU）将旧数据块移除内存。如果想手动移除一个 RDD，而不是等待该 RDD 被 Spark 自动移除，可以使用 RDD.unpersist() 方法参考 Spark持久化 spark中cache和persist的区别" }, { "title": "Java- 自己手动编译OpenJDK8", "url": "/posts/Java-%E8%87%AA%E5%B7%B1%E6%89%8B%E5%8A%A8%E7%BC%96%E8%AF%91OpenJDK8/", "categories": "编程语言, java", "tags": "java", "date": "2019-08-27 00:00:00 +0800", "snippet": "[TOC]Ubuntu下载Ubuntu版本：16.04下载链接：http://releases.ubuntu.com/16.04/ubuntu-16.04.6-desktop-amd64.iso虚拟软件：VMWareOpenJDK源码下载据原文说法，OpenJDK 使用Mercurial进行版本管理。另外一个名叫AdoptOpenJDK project.提供了OpenJDK的镜像，可以让我们用git下载。站点的官网如下：https://adoptopenjdk.net/about.html主页上说他们的目标就是：Provide a reliable source of OpenJDK binaries for all platforms, for the long term future.据我的使用体验来说，之前编译过一次OpenJDK，各种报错，各种改源码才能编译通过。这次确实编译很顺，代码一句没改。下载代码（第一次需要安装git）git clone --depth 1 -b master https://github.com/AdoptOpenJDK/openjdk-jdk8u.git下载Boot JDK编译openJDK任然需要使用JDK来编译这边使用的Oracle的1.7链接：https://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html下载完后解压，然后配置环境变量export JAVA_HOME=/usr/local/jdk1.7.0_80(替换成自己的jdk路径)export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH安装依赖sudo apt install \\ libx11-dev \\ libxext-dev \\ libxrender-dev \\ libxtst-dev \\ libxt-dev \\ libcups2-dev \\ libfreetype6-dev \\ libasound2-dev \\ libfontconfig1-dev配置编译脚本解压下载需要编译的openJDK，并进入其解压后的路径build.shbash ./configure --with-target-bits=64 --with-boot-jdk=/usr/local/jdk1.7.0_80/ --with-debug-level=slowdebug --enable-debug-symbols ZIP_DEBUGINFO_FILES=0make all ZIP_DEBUGINFO_FILES=0运行build.shchmod +x build.sh./build.sh编译成功切换到指定路径下查看编译后的结果cd ~/jdk/openjdk-jdk8u/build/linux-x86_64-normal-server-slowdebug/jdk/bin./java -version参考 源码编译OpenJdk 8，Netbeans调试Java原子类在JVM中的实现（Ubuntu 16.04）" }, { "title": "Java - 静态绑定与动态绑定", "url": "/posts/Java-%E9%9D%99%E6%80%81%E7%BB%91%E5%AE%9A%E4%B8%8E%E5%8A%A8%E6%80%81%E7%BB%91%E5%AE%9A/", "categories": "编程语言, java", "tags": "java", "date": "2019-07-14 00:00:00 +0800", "snippet": "程序绑定的概念 绑定指的是一个方法的调用与方法所在的类(方法主体)关联起来。对java来说，绑定分为静态绑定和动态绑定；或者叫做前期绑定和后期绑定. 静态绑定：在程序执行前方法已经被绑定，此时由编译器或其它连接程序实现。例如：C。 也就是说在编译过程中就已经知道这个方法到底是哪个类中的方法； 针对java简单的可以理解为程序编译期的绑定；这里特别说明一点，java当中的方法只有final，static，private和构造方法是前期绑定（静态绑定） 动态绑定：在运行时根据具体对象的类型进行绑定。 若一种语言实现了后期绑定，同时必须提供一些机制，可在运行期间判断对象的类型，并分别调用适当的方法。也就是说，编译器此时依然不知道对象的类型，但方法调用机制能自己去调查，找到正确的方法主体。不同的语言对后期绑定的实现方法是有所区别的。但我们至少可以这样认为：它们都要在对象中安插某些特殊类型的信息。 Java中动态绑定的过程： 虚拟机提取对象的实际类型的方法表； 虚拟机搜索方法签名； 调用方法。 关于final，static，private和构造方法是静态绑定的理解 private:对于private的方法，首先一点它不能被继承，既然不能被继承那么就没办法通过它子类的对象来调用，而只能通过这个类自身的对象来调用。因此就可以说private方法和定义这个方法的类绑定在了一起。 final:方法虽然可以被继承，但不能被重写（覆盖），虽然子类对象可以调用，但是调用的都是父类中所定义的那个final方法，（由此我们可以知道将方法声明为final类型，一是为了防止方法被覆盖，二是为了有效地关闭java中的动态绑定)。 static:具体的原理我也说不太清。不过根据网上的资料和我自己做的实验可以得出结论：static方法可以被子类继承，但是不能被子类重写（覆盖），但是可以被子类隐藏。（这里意思是说如果父类里有一个static方法，它的子类里如果没有对应的方法，那么当子类对象调用这个方法时就会使用父类中的方法。而如果子类中定义了相同的方法，则会调用子类的中定义的方法。唯一的不同就是，当子类对象上转型为父类对象时，不论子类中有没有定义这个静态方法，该对象都会使用父类中的静态方法。因此这里说静态方法可以被隐藏而不能被覆盖。这与子类隐藏父类中的成员变量是一样的。隐藏和覆盖的区别在于，子类对象转换成父类对象后，能够访问父类被隐藏的变量和方法，而不能访问父类被覆盖的方法）由上面我们可以得出结论，如果一个方法不可被继承或者继承后不可被覆盖，那么这个方法就采用的静态绑定。参考Java静态绑定与动态绑定我的博客即将同步至腾讯云+社区，邀请大家一同入驻：https://cloud.tencent.com/developer/support-plan?invite_code=2rig80ucqd6oc" }, { "title": "Java - 按值传递的方式", "url": "/posts/Java-%E6%8C%89%E5%80%BC%E4%BC%A0%E9%80%92%E7%9A%84%E6%96%B9%E5%BC%8F/", "categories": "编程语言, java", "tags": "java", "date": "2019-07-14 00:00:00 +0800", "snippet": "先来看一个简单的Demopublic class Demo45 { public static void main(String[] args) { List&amp;lt;Integer&amp;gt; list = new ArrayList&amp;lt;Integer&amp;gt;(); for (int i = 0; i &amp;lt; 10; i++) { list.add(i); } add(list); for (Integer j : list) { System.err.print(j+&quot;,&quot;); } System.err.println(&quot;*********************&quot;); String a=&quot;A&quot;; append(a); System.err.println(a); int num = 5; addNum(num); System.err.println(num); } static void add(List&amp;lt;Integer&amp;gt; list){ list.add(100); } static void append(String str){ str+=&quot;is a&quot;; } static void addNum(int a){ a=a+10; }}输出结果0,1,2,3,4,5,6,7,8,9,100,*********************A5这个时候可能会有疑问了，为什么add方法可以修改List数组，但是append和addNum却没有修改传进来的值第一步，先搞清楚Java中的基本类型和引用类型的不同之处int num = 10;String str = &quot;hello&quot;;如图所示，num是基本类型，值就直接保存在变量中。而str是引用类型，变量中保存的只是实际对象的地址。一般称这种变量为”引用”，引用指向实际对象，实际对象中保存着内容。 第二步，搞清楚赋值运算符（=）的作用num = 20;str = &quot;java&quot;;对于基本类型 num ，赋值运算符会直接改变变量的值，原来的值被覆盖掉。对于引用类型 str，赋值运算符会改变引用中所保存的地址，原来的地址被覆盖掉。但是原来的对象不会被改变（重要）。如上图所示，”hello” 字符串对象没有被改变。（没有被任何引用所指向的对象是垃圾，会被垃圾回收器回收） 第三步，在调用的时候发生了什么Java 程序设计语言总是采用按值调用。也就是说，方法得到的是所有参数值的一个拷贝，特别是，方法不能修改传递给它的任何参数变量的内容 。现在再回到最开始的例子，/** * 首先add方法中的list对象是传入参数的一个拷贝，但是这个拷贝对象指向的是同一个List，所以这个拷 * 象中的add(100)是操作list指向的List数组 */add(List&amp;lt;Integer&amp;gt; list)/** * str同样是传入字符串的一个拷贝对象，而String是一个finnal修饰的类，也就是无法对其进行修改，所以 * str的+=操作会生成一个新的String对象，也就是拷贝对象变成了一个新的对象，而原str并未发生改变 */append(String str)/** * 最后这个addNum中传入的是一个Java的基本类型，也就是方法里的a是传入参数的一个拷贝，对a进行操作不 * 会对原数值产生影响 */addNum(int a)这个过程说明：Java 程序设计语言对对象采用的不是引用调用，实际上，对象引用是按值传递的。下面总结一下 Java 中方法参数的使用情况： 一个方法不能修改一个基本数据类型的参数（即数值型或布尔型）。 一个方法可以改变一个对象参数的状态 。 一个方法不能让对象参数引用一个新的对象。参考 Java 到底是值传递还是引用传递？ Java核心技术；4.5 方法参数" }, { "title": "Linux - 常用命令", "url": "/posts/Linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/", "categories": "操作系统", "tags": "linux", "date": "2019-06-14 00:00:00 +0800", "snippet": "dirname、basename作用 去除路径最后一级，即/home/test.sh 输出:/home 输出路径最后一级，即/home/test.sh 输出:test.sh shell 脚本变量$#,$@,$0,$1,$2,$*,$,$? 的含义 $# 是传给脚本的参数个数 $0 执行脚本时的名称，即 /home/test.sh 输出:/home/test.sh $1 是传递给该shell脚本的第一个参数 $2 是传递给该shell脚本的第二个参数 $@ 是传给脚本的所有参数的列表 $* 是以一个单字符串显示所有向脚本传递的参数，与位置变量不同，参数可超过9个 $$ 是脚本运行的当前进程ID号 $? 是显示最后命令的退出状态，0表示没有错误，其他表示有错误 $! Shell最后运行的后台Process的PID grep/egrep作用 egrep相当于grep -E,可以通过正则匹配到需要的文本 -o 只输出匹配到的文本 sed命令使用 -n ：使用安静(silent)模式。在一般 sed 的用法中，所有来自 STDIN 的数据一般都会被列出到终端上。但如果加上 -n 参数后，则只有经过sed 特殊处理的那一行(或者动作)才会被列出来。 -e ：直接在命令列模式上进行 sed 的动作编辑； -f ：直接将 sed 的动作写在一个文件内， -f filename 则可以运行 filename 内的 sed 动作； -r ：sed 的动作支持的是延伸型正规表示法的语法。(默认是基础正规表示法语法) -i ：直接修改读取的文件内容，而不是输出到终端。 动作使用说明： [n1[,n2]]function n1, n2 ：不见得会存在，一般代表『选择进行动作的行数』，举例来说，如果我的动作是需要在 10 到 20 行之间进行的，则『 10,20[动作行为] 』function： a ：新增， a 的后面可以接字串，而这些字串会在新的一行出现(目前的下一行)～ c ：取代， c 的后面可以接字串，这些字串可以取代 n1,n2 之间的行！ d ：删除，因为是删除啊，所以 d 后面通常不接任何咚咚； i ：插入， i 的后面可以接字串，而这些字串会在新的一行出现(目前的上一行)； p ：列印，亦即将某个选择的数据印出。通常 p 会与参数 sed -n 一起运行～ s ：取代，可以直接进行取代的工作哩！通常这个 s 的动作可以搭配正规表示法！例如 1,20s/old/new/g 就是啦！ 切片 a=abcdefg echo ${a:0:4} out: abcdCut命令 cut命令用来显示行中的指定部分，删除文件中指定字段。cut经常用来显示文件的内容用法：cut(选项)(参数)-b：仅显示行中指定直接范围的内容(字节)；-c：仅显示行中指定范围的字符；-d：指定字段的分隔符，默认的字段分隔符为“TAB”；-f：显示指定字段的内容；-n：与“-b”选项连用，不分割多字节字符；--complement：补足被选择的字节、字符或字段；用例：# 使用;作文分隔符切割xxx文本，取切割后的第一个# -d的默认分隔符为\\t，-f后面可以接:1,3、1-3等多种形式cut -d &quot;;&quot; -f 1 xxx.txt# 切割xxx文本中的第1到3个字符，-c切割字节 -b切割字符cut -c 1-3 xxx.txtHere Document Here Document 是 Shell 中的一种特殊的重定向方式，用来将输入重定向到一个交互式 Shell 脚本或程序。 它的基本的形式如下：它的作用是将两个 delimiter 之间的内容(document) 作为输入传递给 command。command &amp;lt;&amp;lt; delimiter documentdelimiter# 例如删除HBase里的表hbase shell &amp;lt;&amp;lt; delimiter disable &#39;hbase:xxx&#39; drop &#39;hbase:xxx&#39;delimiter注意： 结尾的delimiter 一定要顶格写，前面不能有任何字符，后面也不能有任何字符，包括空格和 tab 缩进。 开始的delimiter前后的空格会被忽略掉。 “delimiter” 可以自定义名称" }, { "title": "nohup和&amp;的使用说明", "url": "/posts/Linux-nohup%E5%92%8C&%E7%9A%84%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/", "categories": "操作系统, linux", "tags": "linux", "date": "2019-06-14 00:00:00 +0800", "snippet": "1.nohup和&amp;amp;后台运行1.1 nohup功能：不挂断运行命令语法：nohup Command [ Arg … ] [　&amp;amp; ]​ 无论是否将 nohup 命令的输出重定向到终端，输出都将附加到当前目录的 nohup.out 文件中。　　如果当前目录的 nohup.out 文件不可写，输出重定向到 $HOME/nohup.out 文件中。　　如果没有文件能创建或打开以用于追加，那么 Command 参数指定的命令不可调用。退出状态：该命令返回下列出口值： 　　　　126： 可以查找但不能调用 Command 参数指定的命令。 　　　　127： nohup 命令发生错误或不能查找由 Command 参数指定的命令。 　　　　否则，nohup 命令的退出状态是 Command 参数指定命令的退出状态。1.2 &amp;amp;功能：命令在后台运行，功能与Ctrl+z相同，一般配合nohup一起使用eg：nohup ~/user/test.sh&amp;gt;output.log 2&amp;gt;&amp;amp;1 &amp;amp;命令详解： nohup ~/user/test.sh&amp;gt;output.log 不挂断运行test.sh，输出结果重定向到当前目录的output.log 最后的&amp;amp; 表示后台运行 2&amp;gt;&amp;amp;1 0表示键盘输入，1屏幕输出即标准输出，2表示错误输出。其中2&amp;gt;&amp;amp;1表示将错误信息重定向到标准输出 试想一下，如果2&amp;gt;&amp;amp;1指将错误信息重定向到标准输出，那2&amp;gt;1指什么？ 分别尝试2&amp;gt;1，2&amp;gt;&amp;amp;1 $ ls &amp;gt;outfile$ cat outlog outlogtest.sh$ ls xxx&amp;gt;outfilels: cannot access xxx: No such file or directory$ cat outfile (这里是空)$ ls xxx 2&amp;gt;1$ cat 1(可以看出，将错误信息重定向到文件1里面了)ls: cannot access xxx: No such file or directory​ 也就是说2&amp;gt;1会将错误信息重定向到文件1里面，所以2&amp;gt;&amp;amp;1中的&amp;amp;1指标准输出2. 查看后台运行的进程2.1 jobs的使用 jobs命令用于显示Linux中的任务列表及任务状态，包括后台运行的任务。该命令可以显示任务号及其对应的进程号。其中，任务号是以普通用户的角度进行的，而进程号则是从系统管理员的角度来看的。一个任务可以对应于一个或者多个进程号。语法： jobs(选项)(参数)选项 -l：显示进程号；-p：仅任务对应的显示进程号；-n：显示任务状态的变化；-r：仅输出运行状态（running）的任务；-s：仅输出停止状态（stoped）的任务。常用命令： jobs -l其中，输出信息的第一列表示任务编号，第二列表示任务所对应的进程号，第三列表示任务的运行状态，第四列表示启动任务的命令。缺点：jobs命令只看当前终端生效的，关闭终端后，在另一个终端jobs已经无法看到后台跑得程序了，此时利用ps（进程查看命令）2.2 ps的使用 ps命令用于报告当前系统的进程状态。可以搭配kill指令随时中断、删除不必要的程序。ps命令是最基本同时也是非常强大的进程查看命令，使用该命令可以确定有哪些进程正在运行和运行的状态、进程是否结束、进程有没有僵死、哪些进程占用了过多的资源等等，总之大部分信息都是可以通过执行该命令得到的。常用命令：ps -aux a:显示所有程序 u:以用户为主的格式来显示 x:显示所有程序，不以终端机来区分通常与nohup &amp;amp;配合使用，用于查看后台进程ID 配合 kill命令杀掉程序常用命令：ps -aux|grep test.sh| grep -v grep注：grep -v grep 用grep -v参数可以将grep命令排除掉" }, { "title": "Spark Streaming - 写出文件自定义文件名", "url": "/posts/Spark-Streaming-%E5%86%99%E5%87%BA%E6%96%87%E4%BB%B6%E8%87%AA%E5%AE%9A%E4%B9%89%E6%96%87%E4%BB%B6%E5%90%8D/", "categories": "大数据框架, spark", "tags": "spark", "date": "2019-05-30 00:00:00 +0800", "snippet": "1.背景​ 在工作中碰到了个需求，需要将Spark Streaming中的文件写入到Hive表中，但是Spark Streaming中的saveAsTextFiles会自己定义很多文件夹，不符合Hive读取文件的规范且saveAsTextFiles中的参数只能定义文件夹的名字，第二个是采用Spark Streaming中的foreachRDD，这个方法会将DStream转成再进行操作，但是Spark Streaming中的是多批次处理的结构，也就是很多RDD，每个RDD的saveAsTextFile都会将前面的数据覆盖，所以最终采用的方法是重写saveAsTextFile输出时的文件名2.分析2.1 分析代码既然是重写saveAsTextFile输出逻辑，那先看看他是如何实现输出的def saveAsTextFile(path: String): Unit = withScope { // https://issues.apache.org/jira/browse/SPARK-2075 // // NullWritable is a `Comparable` in Hadoop 1.+, so the compiler cannot find an implicit // Ordering for it and will use the default `null`. However, it&#39;s a `Comparable[NullWritable]` // in Hadoop 2.+, so the compiler will call the implicit `Ordering.ordered` method to create an // Ordering for `NullWritable`. That&#39;s why the compiler will generate different anonymous // classes for `saveAsTextFile` in Hadoop 1.+ and Hadoop 2.+. // // Therefore, here we provide an explicit Ordering `null` to make sure the compiler generate // same bytecodes for `saveAsTextFile`. val nullWritableClassTag = implicitly[ClassTag[NullWritable]] val textClassTag = implicitly[ClassTag[Text]] val r = this.mapPartitions { iter =&amp;gt; val text = new Text() iter.map { x =&amp;gt; text.set(x.toString) (NullWritable.get(), text) } } RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null) .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path) }可以看出saveAsTextFile是依赖saveAsHadoopFile进行输出，因为saveAsHadoopFile接受PairRDD，所以在saveAsTextFile中通过rddToPairRDDFunctions转成(NullWritable,Text)类型的RDD，再通过saveAsHadoopFile进行输出可以看出输出的逻辑还是Hadoop的那一套，所以我们可以通过重写TextOutputFormat来解决输出文件名的相同的问题2.2 代码编写2.2.1 saveAsHadoopFile算子首先先看下官方提供的saveAsHadoopFile算子说明/** * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class * supporting the key and value types K and V in this RDD. */ def saveAsHadoopFile[F &amp;lt;: OutputFormat[K, V]]( path: String)(implicit fm: ClassTag[F]): Unit = self.withScope { saveAsHadoopFile(path, keyClass, valueClass, fm.runtimeClass.asInstanceOf[Class[F]]) }/** * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class * supporting the key and value types K and V in this RDD. Compress the result with the * supplied codec. */ def saveAsHadoopFile[F &amp;lt;: OutputFormat[K, V]]( path: String, codec: Class[_ &amp;lt;: CompressionCodec])(implicit fm: ClassTag[F]): Unit = self.withScope { val runtimeClass = fm.runtimeClass saveAsHadoopFile(path, keyClass, valueClass, runtimeClass.asInstanceOf[Class[F]], codec) } /** * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class * supporting the key and value types K and V in this RDD. Compress with the supplied codec. */def saveAsHadoopFile( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &amp;lt;: OutputFormat[_, _]], codec: Class[_ &amp;lt;: CompressionCodec]): Unit = self.withScope { saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass, new JobConf(self.context.hadoopConfiguration), Some(codec)) }/** * Output the RDD to any Hadoop-supported file system, using a Hadoop `OutputFormat` class * supporting the key and value types K and V in this RDD. * * Note that, we should make sure our tasks are idempotent when speculation is enabled, i.e. do * not use output committer that writes data directly. * There is an example in https://issues.apache.org/jira/browse/SPARK-10063 to show the bad * result of using direct output committer with speculation enabled. */ def saveAsHadoopFile( path: String, keyClass: Class[_], valueClass: Class[_], outputFormatClass: Class[_ &amp;lt;: OutputFormat[_, _]], conf: JobConf = new JobConf(self.context.hadoopConfiguration), codec: Option[Class[_ &amp;lt;: CompressionCodec]] = None): Unit = self.withScope {...}这里我们使用的是def saveAsHadoopFile(path: String, keyClass: Class[_],valueClass: Class[_], outputFormatClass: Class[_ &amp;lt;: OutputFormat[_, _]], codec: Class[_ &amp;lt;: CompressionCodec]): Unit = self.withScope { }依次传入 path：路径、keyClass：key类型、valueClass：value类型、outputFormatClass：outformat方式，剩下两个参数为默认值2.2.2 MultipleTextOutputFormat分析/** * This abstract class extends the FileOutputFormat, allowing to write the * output data to different output files. There are three basic use cases for * this class. * * Case one: This class is used for a map reduce job with at least one reducer. * The reducer wants to write data to different files depending on the actual * keys. It is assumed that a key (or value) encodes the actual key (value) * and the desired location for the actual key (value). * * Case two: This class is used for a map only job. The job wants to use an * output file name that is either a part of the input file name of the input * data, or some derivation of it. * * Case three: This class is used for a map only job. The job wants to use an * output file name that depends on both the keys and the input file name, */@InterfaceAudience.Public@InterfaceStability.Stablepublic abstract class MultipleOutputFormat&amp;lt;K, V&amp;gt;extends FileOutputFormat&amp;lt;K, V&amp;gt; { /** * Create a composite record writer that can write key/value data to different * output files * * @param fs * the file system to use * @param job * the job conf for the job * @param name * the leaf file name for the output file (such as part-00000&quot;) * @param arg3 * a progressable for reporting progress. * @return a composite record writer * @throws IOException */ public RecordWriter&amp;lt;K, V&amp;gt; getRecordWriter(FileSystem fs, JobConf job, String name, Progressable arg3) throws IOException { final FileSystem myFS = fs; final String myName = generateLeafFileName(name); final JobConf myJob = job; final Progressable myProgressable = arg3; return new RecordWriter&amp;lt;K, V&amp;gt;() { // a cache storing the record writers for different output files. TreeMap&amp;lt;String, RecordWriter&amp;lt;K, V&amp;gt;&amp;gt; recordWriters = new TreeMap&amp;lt;String, RecordWriter&amp;lt;K, V&amp;gt;&amp;gt;(); public void write(K key, V value) throws IOException { // get the file name based on the key String keyBasedPath = generateFileNameForKeyValue(key, value, myName); // get the file name based on the input file name String finalPath = getInputFileBasedOutputFileName(myJob, keyBasedPath); // get the actual key K actualKey = generateActualKey(key, value); V actualValue = generateActualValue(key, value); RecordWriter&amp;lt;K, V&amp;gt; rw = this.recordWriters.get(finalPath); if (rw == null) { // if we don&#39;t have the record writer yet for the final path, create // one // and add it to the cache rw = getBaseRecordWriter(myFS, myJob, finalPath, myProgressable); this.recordWriters.put(finalPath, rw); } rw.write(actualKey, actualValue); }; public void close(Reporter reporter) throws IOException { Iterator&amp;lt;String&amp;gt; keys = this.recordWriters.keySet().iterator(); while (keys.hasNext()) { RecordWriter&amp;lt;K, V&amp;gt; rw = this.recordWriters.get(keys.next()); rw.close(reporter); } this.recordWriters.clear(); }; }; } /** * Generate the leaf name for the output file name. The default behavior does * not change the leaf file name (such as part-00000) * * @param name * the leaf file name for the output file * @return the given leaf file name */ protected String generateLeafFileName(String name) { return name; } /** * Generate the file output file name based on the given key and the leaf file * name. The default behavior is that the file name does not depend on the * key. * * @param key * the key of the output data * @param name * the leaf file name * @return generated file name */ protected String generateFileNameForKeyValue(K key, V value, String name) { return name; } /** * Generate the actual key from the given key/value. The default behavior is that * the actual key is equal to the given key * * @param key * the key of the output data * @param value * the value of the output data * @return the actual key derived from the given key/value */ protected K generateActualKey(K key, V value) { return key; } /** * Generate the actual value from the given key and value. The default behavior is that * the actual value is equal to the given value * * @param key * the key of the output data * @param value * the value of the output data * @return the actual value derived from the given key/value */ protected V generateActualValue(K key, V value) { return value; } /** * Generate the outfile name based on a given anme and the input file name. If * the {@link JobContext#MAP_INPUT_FILE} does not exists (i.e. this is not for a map only job), * the given name is returned unchanged. If the config value for * &quot;num.of.trailing.legs.to.use&quot; is not set, or set 0 or negative, the given * name is returned unchanged. Otherwise, return a file name consisting of the * N trailing legs of the input file name where N is the config value for * &quot;num.of.trailing.legs.to.use&quot;. * * @param job * the job config * @param name * the output file name * @return the outfile name based on a given anme and the input file name. */ protected String getInputFileBasedOutputFileName(JobConf job, String name) { String infilepath = job.get(MRJobConfig.MAP_INPUT_FILE); if (infilepath == null) { // if the {@link JobContext#MAP_INPUT_FILE} does not exists, // then return the given name return name; } int numOfTrailingLegsToUse = job.getInt(&quot;mapred.outputformat.numOfTrailingLegs&quot;, 0); if (numOfTrailingLegsToUse &amp;lt;= 0) { return name; } Path infile = new Path(infilepath); Path parent = infile.getParent(); String midName = infile.getName(); Path outPath = new Path(midName); for (int i = 1; i &amp;lt; numOfTrailingLegsToUse; i++) { if (parent == null) break; midName = parent.getName(); if (midName.length() == 0) break; parent = parent.getParent(); outPath = new Path(midName, outPath); } return outPath.toString(); } /** * * @param fs * the file system to use * @param job * a job conf object * @param name * the name of the file over which a record writer object will be * constructed * @param arg3 * a progressable object * @return A RecordWriter object over the given file * @throws IOException */ abstract protected RecordWriter&amp;lt;K, V&amp;gt; getBaseRecordWriter(FileSystem fs, JobConf job, String name, Progressable arg3) throws IOException;}可以看出，在写每条记录之前，MultipleOutputFormat将调用generateFileNameForKeyValue方法来确定文件名，所以在只需要重写generateFileNameForKeyValue方法即可2.2.3 MultipleOutFormat重写class RDDMultipleTextOutputFormat extends MultipleTextOutputFormat[Any, Any] { private val start_time = System.currentTimeMillis() override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = { val service_date = start_time + &quot;-&quot; + name //.split(&quot;-&quot;)(0) service_date }}Spark Streaming 代码修改...//业务代码.map(x=&amp;gt;(x,&quot;&quot;))//由于saveAsHadoopFile接受PariRDD，所以需要转成这样 .saveAsHadoopFile(finallpath,classOf[String],classOf[String],classOf[RDDMultipleTextOutputFormat])到此，已经可以解决覆盖问题参考Spark(Streaming)写入数据到文件" }, { "title": "Unicode&amp;UTF&amp;码点关系", "url": "/posts/Unicode&UTF&%E7%A0%81%E7%82%B9%E5%85%B3%E7%B3%BB/", "categories": "编程语言", "tags": "编码", "date": "2019-04-13 02:09:27 +0800", "snippet": "转自：https://github.com/acmerfight/insight_python/edit/master/Unicode_and_Character_Sets.md字符编码你是否认为 ASCII 码就是一个字符，一个字节就是一个字符，一个字符就是 8 比特？你是否认为 UTF-8 就是用 8 比特表示一个字符？如果真的是这样认为这篇文章就很适合你。为什么要有编码？首先大家需要明确的是在计算机里所有的数据都是字节的形式存储和处理的。我们需要字节来表示计算机里的信息，但是这些字节本身又是没有任何意义的。我们需要对这些字节赋予实际的意义，制定各种编码标准。编码模型首先需要知道的是存在两种编码模型简单字符集在这种编码模型里，一个字符集定义了这个字符集里包含什么字符，同时把每个字符如何对应成计算机里的比特也进行了定义。例如 ASCII，在 ASCII 里直接定义了 A -&amp;gt; 0100 0001。现代编码模型在现代编码模型里要知道一个字符如何映射成计算机里比特，需要经过如下几个步骤: 知道一个系统需要支持哪些字符，这些字符的集合被称为字符表（Character repertoire） 给字符表里的抽象字符编上一个数字，也就是字符集合到一个整数集合的映射。这种映射称为编码字符集（CCS:Coded Character Set）, unicode 是属于这一层的概念，unicode 跟计算机里的什么进制啊没有任何关系，它是完全数学的抽象的。 将 CCS 里字符对应的整数转换成有限长度的比特值，便于以后计算机使用一定长度的二进制形式表示该整数。这个对应关系被称为字符编码表（CEF:Character Encoding Form）UTF-8, UTF-16 都属于这层。 对于 CEF 得到的比特值具体如何在计算机中进行存储，传输。因为存在大端小端的问题，这就会跟具体的操作系统相关了。这种解决方案称为字符编码方案（CES:Character Encoding Scheme）。平常我们所说的编码都在第三步的时候完成了,并没有涉及到 CES。所以 CES 并不在本文的讨论范围之内。现在也许有人会想为什么要有现代的编码模型？为什么在现在的编码模型要拆分出这么多概念？直接像原始的编码模型直接都规定好所有的信息不行吗？这些问题在下文的编码发展史中都会有所阐述。编码的发展史ASCIIASCII 出现在上个世纪 60 年代的美国，ASCII 一共定义了 128 个字符，使用了一个字节的 7 位。定义的这些字符包括英文字母 A-Z，a-z，数字 0-9，一些标点符号和控制符号。在 Shell 里输入man ASCII，可以看到完整的 ASCII 字符集。ASCII 采用的编码模型是简单字符集，它直接定义了一个字符的比特值表示。例如上文提到的A -&amp;gt; 0100 0001。也就是 ASCII 直接完成了现代编码模型的前三步工作。在英语系国家里 ASCII 标准很完美。但是不要忘了世界上可有好几千种语言，这些语言里不仅只有这些符号啊。如果使用这些语言的人也想使用计算机，ASCII 就远远不够了。所以到这里编码进入了混乱的时代。混乱时代人们知道计算机的一个字节是 8 位，可以表示 256 个字符。ASCII 却只使用了 7 位，所以人们决定把剩余的一位也利用起来。这时问题出现了，人们对于已经规定好的 128 个字符是没有异议的，但是不同语系的人对于其他字符的需求是不一样的，所以对于剩下的 128 个字符的扩展会千奇百怪。而且更加混乱的是，在亚洲的语言系统中有更多的字符，一个字节无论如何也满足不了需求了。例如仅汉字就有 10 万多个，一个字节的 256 表示方式怎么能够满足呢。于是就又产生了各种多字节的表示一个字符方法(gbk 就是其中一种)，这就使整个局面更加的混乱不堪。（希望看到这里的你不再认为一个字节就是一个字符，一个字符就是8比特）。每个语系都有自己特定的编码页（code pages）的状况，使得不同的语言出现在同一台计算机上，不同语系的人在网络上进行交流都成了痴人说梦。这时 Unicode 出现了。UnicodeUnicode 就是给计算机中所有的字符各自分配一个代号。Unicode 通俗来说是什么呢？就是现在实现共产主义了，各国人民不在需要自己特定的国家身份证，而是给每人一张全世界通用的身份证。Unicode 是属于编码字符集（CCS）的范围。Unicode 所做的事情就是将我们需要表示的字符表中的每个字符映射成一个数字，这个数字被称为相应字符的码点（code point）。例如“严”字在 Unicode 中对应的码点是 U+0x4E25。到目前为止，我们只是找到了一堆字符和数字之间的映射关系而已，只到了CCS的层次。这些数字如何在计算机和网络中存储和展示还没有提到。字符编码前面还都属于字符集的概念，现在终于到 CEF 的层次了。为了便于计算的存储和处理，现在我们要把哪些纯数学数字对应成有限长度的比特值了。最直观的设计当然是一个字符的码点是什么数字，我们就把这个数字转换成相应的二进制表示，例如“严”在 Unicode 中对应的数字是 0x4E25,他的二进制是100 1110 0010 0101，也就是严这个字需要两个字节进行存储。按照这种方法大部分汉字都可以用两个字节来表示了。但是还有其他语系的存在，没准儿他们所使用的字符用这种方法转换就需要 4 个字节。这样问题又来了到底该使用几个字节表示一个字符呢？如果规定两个字节，有的字符会表示不出来，如果规定较多的字节表示一个字符，很多人又不答应，因为本来有些语言的字符两个字节处理就可以了，凭什么用更多的字节表示，多么浪费。这时就会想可不可以用变长的字节来存储一个字符呢？如果使用了变长的字节表示一个字符，那就必须要知道是几个字节表示了一个字符，要不然计算机可没那么聪明。下面介绍一下最常用的 UTF-8（UTF 是Unicode Transformation Format的缩写）的设计。请看下图（来自阮一峰的博客）x 表示可用的位通过 UTF-8 的对应关系可以把每个字符在Unicode 中对应的码点，转换成相应的计算机的二进制表示。可以发现按照 UTF-8 进行转换是完全兼容原先的 ASCII 的；而且在多字节表示一个字符时，开头有几个 1 就表示这个字符按照 UTF-8 转换后由几个字节表示。下面一个实例子来自阮一峰的博客 已知“严”的unicode是4E25（100111000100101），根据上表，可以发现4E25处在第三行的范围内（0000 0800-0000 FFFF），因此“严”的UTF-8编码需要三个字节，即格式是“1110xxxx 10xxxxxx 10xxxxxx”。然后，从“严”的最后一个二进制位开始，依次从后向前填入格式中的x，多出的位补0。这样就得到了，“严”的UTF-8编码是“11100100 10111000 10100101”，转换成十六进制就是0xE4B8A5。除了 UTF-8 这种转换方法，还存在 UTF-16，UTF-32 等等转换方法。这里就不再多做介绍。（注意UTF后边的数字代表的是码元的大小。码元（Code Unit）是指一个已编码的文本中具有最短的比特组合的单元。对于 UTF-8 来说，码元是 8 比特长；对于 UTF-16 来说，码元是 16 比特长。换一种说法就是 UTF-8 的是以一个字节为最小单位的，UTF-16 是以两个字节为最小单位的。）结束语花了两天时间终于写完了，相信看到这里大家对于字符编码有了较为清楚的认识，当然文章中肯定存在不准确之处，希望大家批评指正。邮箱：acmerfight圈gmail.com参考资料字符编码The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!)字符编码笔记：ASCII，Unicode和UTF-8字符集和字符编码Windows 记事本的 ANSI、Unicode、UTF-8 这三种编码模式有什么区别？如何向非技术人员解释 Unicode 是什么字符编解码的故事（ASCII，ANSI，Unicode，Utf-8）" }, { "title": "K- 近邻算法", "url": "/posts/K-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/", "categories": "框架, 机器学习", "tags": "机器学习", "date": "2019-02-27 06:18:05 +0800", "snippet": "概述简单地说，k近邻算法采用测量不同特征值之间的距离方法进行分类。k-近邻算法 优点：精度高、对异常值不敏感、无数据输入假定。 缺点：计算复杂度高、空间复杂度高。 适用数据范围：数值型和标称型。工作原理：存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前k个最相似的数据，这就是k-近邻算法中k的出处，通常k是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。算法简单实现创建训练数据集&amp;amp;待分类数据# 导包import numpy as npimport matplotlib.pyplot as plttrain_data = np.array([np.random.random()*10 for x in range(20)]).reshape(10,2) # 创建训练集train_lable = np.array([0,1,1,1,0,0,1,1,0,1]) # 给每个向量一个标签test_data = np.array([np.random.random()*10,np.random.random()*10]) # 待分类的目标值查看数据分布计算分类点与数据集的距离K=3len = np.array(np.sqrt(np.sum((train_data-test_data)**2,axis=1))) #axis=1 表示每次只计算一行# 对值进行排序并取出下标的前K项predict_lable = [train_lable[x] for x in len).argsort()[:K]]取标签最后结果作为预测结果vote =Counter(predict_lable) # 对待分类值进行预测vote.most_common(1)[0][0] # 输出预测结果scikit-learn 中的KNN导包&amp;amp;创建训练集import numpy as npimport matplotlib.pyplot as pltfrom sklearn.neighbors import KNeighborsClassifier # 导包train_data = np.array([np.random.random()*10 for x in range(20)]).reshape(10,2) # 创建训练集train_lable = np.array([0,1,1,1,0,0,1,1,0,1]) # 给每个向量一个标签创建KNN算法knn_clf = KNeighborsClassifier()模型训练knn_clf.fit(train_data,train_label)# out:# KNeighborsClassifier(algorithm=&#39;auto&#39;, leaf_size=30, metric=&#39;minkowski&#39;,# metric_params=None, n_jobs=1, n_neighbors=2, p=2,# weights=&#39;uniform&#39;)分类预测test_data = np.array([np.random.random()*10,np.random.random()*10]) # 待分类的目标值knn_clf.predict(test_data) # 使用训练类型进行预测手动模型性能评估加载sklearn中鸢尾花的数据import numpy as npimport matplotlib.pyplot as pltfrom sklearn import datasetsiris = datasets.load_iris() # 加载数据x = iris.data # 查看数据内容y = iris.target # 数据对应的标签数据下标重排np.random.seed(123) #输入随机种子shuffle_indexes = np.random.permutation(len(x)) # 重排下标数据集分割（训练集80%，测试集20%）test_radio = 0.2train_len = int(0.2*len(shuffle_indexes))train_index = shuffle_indexes[train_len:]test_index = shuffle_indexes[:train_len]# 训练集特征&amp;amp;标签train_data = x[train_index]train_lable = y[train_index]# 测试集特征&amp;amp;标签test_data = x[test_index]test_label = y[test_index]模型训练&amp;amp;数据预测from sklearn.neighbors import KNeighborsClassifierknn_clf = KNeighborsClassifier()knn_clf.fit(train_data,train_lable)predict_lable = knn_clf.predict(test_data)精准度评估np.sum(test_label == predict_lable)/len(test_label)scikit-learn中的模型性能评估数据获取import numpy as npimport matplotlib.pyplot as pltfrom sklearn import datasetsfrom sklearn.neighbors import KNeighborsClassifieriris = datasets.load_iris() # 加载数据x = iris.data # 查看数据内容y = iris.target # 数据对应的标签数据分割from sklearn.model_selection import train_test_splittrain_data,test_data,train_lable,test_lable = train_test_split(x,y,test_size=0.2,random_state=123)模型训练&amp;amp;精准度计算# 模型训练knn_clf = KNeighborsClassifier()knn_clf.fit(train_data,train_lable)# 精准度计算knn_clf.score(test_data,test_lable)超参数&amp;amp;模型参数 超参数：在模型运行前需要决定的参数 模型参数：算法过程中学习的参数显然，KNN算法中没有模型参数寻找最佳超参数sklearn_KNeighborsClassifier APIclass sklearn.neighbors.KNeighborsClassifier(n_neighbors=5, weights=’uniform’, algorithm=’auto’, leaf_size=30, p=2, metric=’minkowski’, metric_params=None, n_jobs=None, **kwargs)从方法上可以看出一共有如下几个超参数 n_neighbors：从待分类点最近K个值中进行判断，默认为5 weights：距离权重，可选参数 ‘uniform’：最近的K个点权重相同 ‘distance’：最近的K个点中，近的点权重比远的点更高 p :距离公式参数 n_jobs：多少个job共同进行，-1为使用全部进程可以采用 for 循环来便利超参数来计算出最佳超参数也可以使用网格搜索来计算网格搜索param_grid=[ { &#39;weights&#39;:[&#39;uniform&#39;], &#39;n_neighbors&#39;:[i for i in range(1,11)] }, { &#39;weights&#39;:[&#39;distance&#39;], &#39;n_neighbors&#39;:[i for i in range(1,11)], &#39;p&#39;:[i for i in range(1,6)] }]from sklearn.neighbors import KNeighborsClassifierknn_clf=KNeighborsClassifier()from sklearn.model_selection import GridSearchCVgrid_search = GridSearchCV(knn_clf,param_grid,verbose=2)%%timegrid_search.fit(train_data,train_lable) # 训练模型grid_search.best_estimator_ # 返回最佳模型grid_search.best_score_ # 返回精准度归一化 最值归一化 均值方差归一化 " }, { "title": "Pandas入门操作", "url": "/posts/Pandas%E5%85%A5%E9%97%A8%E6%93%8D%E4%BD%9C/", "categories": "编程语言, python", "tags": "python", "date": "2019-02-26 07:53:56 +0800", "snippet": "Pandas导入import pandas as pdimport numpy as np创建DataFram# 手动穿件数据集df = pd.DataFrame([ [1001,&#39;Mike&#39;,20], [1002,&#39;Bob&#39;,21], [1003,&#39;Alice&#39;,22],])# 从磁盘导入数据集df = pd.read_excel(&#39;c:/Users/58212/Desktop/house_info_001.xlsx&#39;)添加列名df.columns=[&#39;编号&#39;,&#39;姓名&#39;,&#39;年龄&#39;]读取前&amp;amp;后几行df.head() # 默认读取前5行df.tail() # 默认读取后5行查看DataFrame描述信息df.infoDataFrame 简单的统计量df.describe().T切片# 获取单列df[&#39;首付&#39;]# 获取多列df[[&#39;首付&#39;,&#39;建筑面积&#39;]]# 获取指定几行指定几列df.loc[1:7,[&#39;单价&#39;,&#39;建筑面积&#39;]]筛选df[df[&#39;首付&#39;]&amp;gt;250]# 交集df[(df[&#39;首付&#39;]&amp;gt;=150) &amp;amp; (df[&#39;朝向&#39;] == &#39;南北&#39;)].head()# 并集df[(df[&#39;首付&#39;]&amp;gt;=150) | (df[&#39;朝向&#39;] == &#39;南北&#39;)].head()添加&amp;amp;删除&amp;amp;修改一列# 新增列df[&#39;测试&#39;]=Truedf.head()# 删除列del df[&#39;测试&#39;]# 新增并设置为空df[&#39;测试列&#39;] = np.nan# 修改某个元素df.loc[2,&#39;住宅类别&#39;]=&#39;普通住宅&#39;检查缺失值df[&#39;住宅类别&#39;].isnull() # 输出‘住宅类别中’所有的值是否为空df[&#39;住宅类别&#39;].isnull().any() # 检查‘住宅类别中’是否有一列为空df.isnull().any() # 检查所有列中是否含有控制df.isnull().sum() # 对所有列中的空值进行计数移除缺失值# 函数作用：删除含有空值的行或列# axis:维度，axis=0表示index行,axis=1表示columns列，默认为0# how:&quot;all&quot;表示这一行或列中的元素全部缺失（为nan）才删除这一行或列，&quot;any&quot;表示这一行或列中只要有元素缺失，就删除这一行或列# thresh:一行或一列中至少出现了thresh个才删除。# subset：在某些列的子集中选择出现了缺失值的列删除，不在子集中的含有缺失值得列或行不会删除（有axis决定是行还是列）# inplace：刷选过缺失值得新数据是存为副本还是直接在原数据上进行修改df = df.dropna(axis=0, how=&#39;any&#39;, thresh=None, subset=None, inplace=False)填补缺失行# 函数作用：填充缺失值# # value:需要用什么值去填充缺失值# axis:确定填充维度，从行开始或是从列开始# method：ffill:用缺失值前面的一个值代替缺失值，如果axis =1，那么就是横向的前面的值替换后面的缺失值，如果axis=0，那么则是上面的值替换下面的缺失值。backfill/bfill，缺失值后面的一个值代替前面的缺失值。注意这个参数不能与value同时出现# limit:确定填充的个数，如果limit=2，则只填充两个缺失值。df=df.fillna(value=None, method=None, axis=None, inplace=False, limit=None, downcast=None, **kwargs)数据转换# 修改数值df[&#39;单价&#39;]=df[&#39;单价&#39;]*1000# 移除单位df[&#39;建筑面积&#39;]=df[&#39;建筑面积&#39;].map(lambda x:x.split(&#39;平&#39;)[0])df[&#39;建筑年代&#39;]=df[&#39;建筑年代&#39;].map(lambda x:x.split(&#39;年&#39;)[0])# 转换类型df[[&#39;单价&#39;,&#39;建筑面积&#39;,&#39;首付&#39;]]=df[[&#39;单价&#39;,&#39;建筑面积&#39;,&#39;首付&#39;]].astype(&#39;float&#39;)# 正则表达式df[[&#39;室&#39;,&#39;厅&#39;,&#39;卫&#39;]]=df[&#39;户型&#39;].str.extract(&#39;(\\d)室(\\d)厅(\\d)卫&#39;) #将户型转成了3列# 统计某列所有的值df[&#39;住宅类别&#39;].value_counts()分类数据硬编码&amp;amp;One-Hot编码# 分类数据硬编码,将某列的值转成对应数值，离散特征的取值有大小的意义house_mapping={ &#39;普通住宅&#39;:0, &#39;商住楼&#39;:1, &#39;公寓&#39;:2}df[&#39;住宅类别&#39;]=df[&#39;住宅类别&#39;].map(house_mapping)# One-Hot编码,离散特征的取值之间没有大小的意义df=df.join(pd.get_dummies(df[&#39;楼层&#39;])) 探索性数据分析叙述性统计量df.describe().T便捷绘图# 直方图，单价直方图df[&#39;单价&#39;]=hist(bins=20) # 20个容器plt.show()# 箱线图,单价箱线图p=df.boxplot(column=&#39;单价&#39;)# 散点图import matplotlib.pyplot as pltplt.scatter(df[&#39;单价&#39;],df[&#39;首付&#39;])plt.show()# 皮尔逊相关系数,其其他参数的线性关系值df.corr()" }, { "title": "Matplotlib快速入门", "url": "/posts/Matplotlib%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8/", "categories": "编程语言, python", "tags": "python", "date": "2019-02-26 06:38:54 +0800", "snippet": "什么是 Matplotlib?简单来说，Matplotlib 是 Python 的一个绘图库。它包含了大量的工具，你可以使用这些工具创建各种图形，包括简单的散点图，正弦曲线，甚至是三维图形。Python 科学计算社区经常使用它完成数据可视化的工作。你可以在他们的网站上了解到更多 Matplotlib 背后的设计思想，但是我强烈建议你先浏览一下他们的图库，体会一下这个库的各种神奇功能。Matplotlib使用导包import matplotlib.pyplot as pltimport numpy as np简单的绘图# 从 -10 到 10 等分成100份x = np.linspace(-10,10,100)# 分别这100个值的cos&amp;amp;sincos_y=np.cos(x)sin_y=np.sin(x)# 前连个参数分别对应 x轴y轴，第三个参数代表着红色虚线，第四个参数代表这条线的名称plt.plot(x,cos_y,&#39;r--&#39;,label=&#39;cos_y&#39;)plt.plot(x,sin_y,&#39;b--&#39;,label=&#39;sin_y&#39;)plt.legend() # 图上显示这条线的名称plt.title(&#39;cos sin demo&#39;) # 图的标题plt.show() 颜色： 蓝色 - ‘b’ 绿色 - ‘g’ 红色 - ‘r’ 青色 - ‘c’ 品红 - ‘m’ 黄色 - ‘y’ 黑色 - ‘k’（’b’代表蓝色，所以这里用黑色的最后一个字母） 白色 - ‘w’线： 直线 - ‘-‘ 虚线 - ‘–’ 点线 - ‘:’ 点划线 - ‘-.’ 常用点标记 点 - ‘.’ 像素 - ‘,’ 圆 - ‘o’ 方形 - ‘s’ 三角形 - ‘^’ 更多点标记样式点击这里正态分布图# 正态分布的中心，标准差，生成的个数x=np.random.normal(0,1,100000)plt.hist(x,100) # 传入数组，容器个数plt.show参考 十分钟入门Matplotlib" }, { "title": "Numpy入门", "url": "/posts/Numpy%E5%85%A5%E9%97%A8/", "categories": "编程语言, python", "tags": "python", "date": "2019-02-26 06:22:18 +0800", "snippet": "什么是NumpyNumPy(Numerical Python) 是 Python 语言的一个扩展程序库，支持大量的维度数组与矩阵运算，此外也针对数组运算提供大量的数学函数库。Numpy使用Numpy导入import numpy as np向量操作向量求和np_arr1=np.array([1,2,3])np_arr2=np.array([2,3,4])np_arr1+np_arr2# array([3, 5, 7])向量乘法np_arr1=np.array([1,2,3])np_arr1*3# array([2,4,6)向量点乘np_arr1=np.array([1,2,3])np_arr2=np.array([2,3,4])np_arr1.dot(np_arr2)dot(np_arr1,np_arr2)#20矩阵操作创建矩阵np_matrix = np.array([ [1,2,3], [3,4,5], [4,5,6] ])查看矩阵结构n.shape# (3,3)查看矩阵某个元素np_matrix[1,1]# 4查看矩阵维度np_matrix.ndim# 2修改矩阵结构np_arr3=np.arange(1,11)print(np_arr3)np_arr3=np_arr3.reshape(2,5)print(np_arr3)# [ 1 2 3 4 5 6 7 8 9 10]# [[ 1 2 3 4 5]# [ 6 7 8 9 10]]矩阵求和np_arr1=np.array([1,2,3])np_arr2=np.array([2,3,4])np_arr1+np_arr2# array([3, 5, 7])方阵行列式的值np_matrix = np.array([ [2,2,3], [2,3,4], [3,4,5] ])np.linalg.det(np_matrix)# -1.0000000000000004求逆np_matrix = np.array([ [2,2,3], [2,3,4], [3,4,5] ])np.linalg.inv(np_matrix)# array([[ 1., -2., 1.], [-2., -1., 2.], [ 1., 2., -2.]])零&amp;amp;单位矩阵# 零矩阵np.zeros([3,4])# 单位矩阵np.ones([3,4])" }, { "title": "CDH安装指南", "url": "/posts/CDH%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97/", "categories": "大数据框架, cdh", "tags": "cdh", "date": "2019-01-29 07:04:47 +0800", "snippet": "CDH安装1. 准备工作1.1 环境准备 个人电脑一台，操作系统需安装ssh、ftp工具 本次安装使用个人电脑使用Win10，ssh工具：xshell、ftp工具：xftp 服务器 主机名 物理内存(G) CPU核数(核) 数据磁盘(G) IP地址 hadoop1 6 2 60 192.168.120.101 hadoop2 4 3 60 192.168.120.102 hadoop3 4 3 60 192.168.120.103 操作系统：CentOS - 7 - x86 -Minimal 软件包下载 CentOS7.2 Packages wget -r -p -np -k http://vault.centos.org/7.2.1511/os/x86_64/Packages/ （6G） CDH Parcel和manifest文件、CM wget http://archive.cloudera.com/cdh5/parcels/5.7.2/CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel （1.3G）wget http://archive.cloudera.com/cdh5/parcels/5.7.2/CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha1 （41K）wget http://archive.cloudera.com/cdh5/parcels/5.7.2/manifest.json （49K）wget http://archive.cloudera.com/cm5/cm/5/cloudera-manager-centos7-cm5.7.2_x86_64.tar.gz（600M） JDK：jdk-8u25 2. 基础环境搭建2.1 修改IP本次使用的是虚拟机，NAT模式，将Ip修改为静态模式，方便后续cdh配置以hadoop1为例vi /etc/sysconfig/network-scripts/ifcfg-eno16777728# 修改或添加一下内容BOOTPROTO=&quot;static&quot;IPADDR=&quot;你的静态IP&quot;PREFIX=&quot;24&quot;GATEWAY=&quot;网关地址&quot;DNS1=&quot;网关地址&quot;# 重启网卡service network restart2.2 HTTP软件仓库这是一个可选项，安装后可不依赖网络，直接从本地软件仓库下载软件# 解压Package包tar -zxvf centos# 解压后目录结构：/data01/Packages# 安装httpd软件cd /data01/Packagesrpm -ivh apr-1.4.8-3.el7.x86_64.rpm rpm -ivh apr-util-1.5.2-6.el7.x86_64.rpm rpm -ivh httpd-tools-2.4.6-40.el7.centos.x86_64.rpm rpm -ivh mailcap-2.1.41-2.el7.noarch.rpmrpm -ivh httpd-2.4.6-40.el7.centos.x86_64.rpm# 启动httpd，并设置开机启动systemctl start httpdsystemctl enable httpd# 安装createrepo仓库构建工具cd /data01/Packagesrpm -ivh deltarpm-3.6-3.el7.x86_64.rpm rpm -ivh python-deltarpm-3.6-3.el7.x86_64.rpm rpm -ivh libxml2-python-2.9.1-5.el7_1.2.x86_64.rpm rpm -ivh libxml2-2.9.1-5.el7_1.2.x86_64.rpm rpm -ivh createrepo-0.9.9-23.el7.noarch.rpm # 创建软链接cd /var/www/html/mkdir /var/www/html/CentOS7.2/ln -s /data01/Packages /var/www/html/CentOS7.2/Packageschmod 755 -R /var/www/html/CentOS7.2/Packages # 关闭防火墙setenforce 0 createrepo安装成功仓库搭建成功2.2.1 配置repo给所有直接配置repo源# 备份默认文件cd /etc/yum.repos.d/mkdir bakmv CentOS-*.repo bak/# 创建源文件vi base.repo# 一下为文件内容[base]name=CentOS-Packagesbaseurl=http://192.160.120.101/CentOS7.2/Packages/gpgkey=path=/enabled=1gpgcheck=0# 检查是否配置成功# 清理yum源yum clean all# 重构yum缓存yum makecache# 在yum源中找到vimyum list|grep vim 2.3 修改主机名以及Host文件在所有主机上进行操作# 修改主机名hostnamectl --static set-hostname hadoop 1.2.3hostname# 修改hostvi /etc/hosts127.0.0.1 localhost192.168.120.101 hadoop1192.168.120.102 hadoop2192.168.120.103 hadoop32.4 禁用SELinuxSELinux是Security Enhance Linux的缩写，是NASA开发的一套严格的资源权限管理系统，由于使用起来比较复杂，所以一般选择关闭SELinux有三种模式： enforcing：强制模式 permissive：宽容模式 disabled：关闭模式# 先临时关闭setenforce 0# 修改模式sed -i &#39;s/^SELINUX=.*/SELINUX=disabled/g&#39; /etc/selinux/configsed -i &#39;s/^SELINUX=.*/SELINUX=disabled/g&#39; /etc/sysconfig/selinux# 重启生效reboot# 查看修改结果，显示为disable 为成功cat /etc/selinux/config|grep SELINUX=2.5 配置SSH免密登录SSH免密登录可以帮助集群内部互相访问不需要密码，使用方式ssh username@ipaddress配置过程：每台主机生成公钥和私钥，将所有主机的公钥（id_rsa.pub）写入到每台主机的authorized_keys，这样就实现了免密登录# 在所有主机中# 备份ssh配置文件cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak# 修改配置文件sed -i &#39;s/\\#RSAAuthentication/RSAAuthentication/g&#39; /etc/ssh/sshd_config# 修改重启服务/bin/systemctl restart sshd.service 1&amp;gt;/dev/null# 生成公钥和私钥,一直按回车就可以了cd ~ssh-keygen -t rsa# 将所有的公钥发送到hadoop1scp /root/.ssh/id_rsa.pub root@hadoop(1,2,3):/root/hadoop1.pub# 在hadoop1中# 将所有公钥写入authorized_keyscat /root/*.pub &amp;gt;&amp;gt; /root/.ssh/authorized_keys# 分发写好的授权文件到各台节点scp /root/.ssh/authorized_keys root@hadoop(2,3):/root/.ssh/authorized_keys2.6 时间同步2.6.1 时区设置# 所有节点# 时区修改timedatectl set-timezone &quot;Asia/Shanghai&quot;# 时区查看timedatectl2.6.2 时间同步# 在所有主机上yum -y install ntp# 在hadoop1中# 备份原始配置文件&amp;amp;修改配置文件cp /etc/ntp.conf /etc/ntp.conf.bakvim /etc/ntp.conf# 将一下内容注释掉server 0.centos.pool.ntp.org iburstserver 1.centos.pool.ntp.org iburstserver 2.centos.pool.ntp.org iburstserver 3.centos.pool.ntp.org iburst# 并在后面追加restrict 192.168.120.101 mask 255.255.255.0 nomodify notrapserver 127.127.1.1# 修改完成后退出，重启ntpdsystemctl restart ntpdsystemctl enable ntpd# 在除hadoop1的其他主机上# 注释掉相同内容，并添加一下代码server 192.168.120.101 perfer# 时间校队ntpdate hadoop1service ntpd startntpq -p# 如果是Ubuntu# 设置计划任务crontab -e# 写入如下内容*/5 * * * * /usr/sbin/ntpdate 192.168.120.101 &amp;gt;/dev/null 2&amp;gt;&amp;amp;12.7 一些影响集群效率的配置# 关闭THPecho never &amp;gt; /sys/kernel/mm/transparent_hugepage/enabledecho never &amp;gt; /sys/kernel/mm/transparent_hugepage/defrag# 修改swappinessecho &quot;vm.swappiness=0&quot; &amp;gt;&amp;gt; /etc/sysctl.conf sysctl -p cat /proc/sys/vm/swappiness2.8 安装JDK# 所有主机# 首先将jdk包上传到/root目录下cd /rootmkdir /usr/lib/java/tar zxvf /root/jdk-8u25-linux-x64.tar.gz -C /usr/lib/java/echo &quot;export JAVA_HOME=/usr/lib/java/jdk1.8.0_25/&quot; &amp;gt;&amp;gt;/etc/profileecho &#39;export PATH=$PATH:$JAVA_HOME/bin/&#39; &amp;gt;&amp;gt;/etc/profilesource /etc/profilejava -versionrm -rf /root/jdk-8u25-linux-x64.tar.gz如果java -version 显示为openjdk，那就卸载掉openjdk2.9安装MySql# 在hadoop1上# 安装mysqlyum -y install mariadb-server mariadb# 启动并设置开机自启systemctl start mariadb.servicesystemctl enable mariadb.service# 进入mysql 并设置密码，默认为空mysql -u root -p一下为mysql上的操作show databases;use mysql;update user set password=password(&#39;123456&#39;) where user= &#39;root&#39;;grant all privileges on *.* to root@&#39;%&#39; identified by &#39;123456&#39;;grant all privileges on *.* to root@&#39;hadoop155&#39; identified by &#39;123456&#39;;grant all privileges on *.* to root@&#39;localhost&#39; identified by &#39;123456&#39;;FLUSH PRIVILEGES;将编码修改为utf-8vim /etc/my.cnf# 在[mysqld]下加入下面内容character_set_server = utf8 # 重启数据库systemctl restart mariadb 在MySql中创建CDH所需要的数据库create database hive DEFAULT CHARSET latin1 COLLATE latin1_general_ci;create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database monitor DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;CREATE USER &#39;hive&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;hive&#39;; GRANT ALL PRIVILEGES ON hive.* TO &#39;hive&#39;@&#39;localhost&#39;; CREATE USER &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;hive&#39;; GRANT ALL PRIVILEGES ON hive.* TO &#39;hive&#39;@&#39;%&#39;; CREATE USER &#39;hive&#39;@&#39;hadoop1&#39; IDENTIFIED BY &#39;hive&#39;; GRANT ALL PRIVILEGES ON hive.* TO &#39;hive&#39;@&#39;hadoop1&#39;;CREATE USER &#39;oozie&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;oozie&#39;; GRANT ALL PRIVILEGES ON oozie.* TO &#39;oozie&#39;@&#39;localhost&#39;; CREATE USER &#39;oozie&#39;@&#39;%&#39; IDENTIFIED BY &#39;oozie&#39;; GRANT ALL PRIVILEGES ON oozie.* TO &#39;oozie&#39;@&#39;%&#39;; CREATE USER &#39;oozie&#39;@&#39;hadoop1&#39;IDENTIFIED BY &#39;oozie&#39;; GRANT ALL PRIVILEGES ON oozie.* TO &#39;oozie&#39;@&#39;hadoop1&#39;; CREATE USER &#39;monitor&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;monitor&#39;; GRANT ALL PRIVILEGES ON monitor.* TO &#39;monitor&#39;@&#39;localhost&#39;; CREATE USER &#39;monitor&#39;@&#39;%&#39; IDENTIFIED BY &#39;monitor&#39;; GRANT ALL PRIVILEGES ON monitor.* TO &#39;monitor&#39;@&#39;%&#39;; CREATE USER &#39;monitor&#39;@&#39;hadoop1&#39;IDENTIFIED BY &#39;monitor&#39;; GRANT ALL PRIVILEGES ON monitor.* TO &#39;monitor&#39;@&#39;hadoop1&#39;; FLUSH PRIVILEGES; 在所有主机上安装mysql-connector驱动# 安装完成后的目录 /usr/share/java/mysql-connector-java.jar yum -y install mysql-connector-java卸载依赖包# 安装以后依赖包yum -y install psmiscyum –y install perlyum install nfs-utils portmapsystemctl stop nfssystemctl start rpcbindsystemctl enable rpcbind至此，所有的基础环境已经搭建完成接下来是CDH的搭建3. CDH搭建3.1 安装CM所有主机上传CM相关包cloudera-manager-centos7-cm5.7.2_x86_64.tar.gz到硬盘/data01，也就是第一步下载的东西# 所有节点cd /data01mkdir /opt/cloudera-managertar -zxvf /data01/cloudera-manager-centos7-cm5.7.2_x86_64.tar.gz -C /opt/cloudera-manager# 授权useradd --system --home=/opt/cloudera-manager/cm-5.7.2/run/cloudera-scm-server --no-create-home --shell=/bin/false --comment &quot;Cloudera SCM User&quot; cloudera-scmusermod -aG wheel cloudera-scm# 修改发送心跳的路径sed -i &quot;s/^server_host=.*/server_host=hadoop1/g&quot; /opt/cloudera-manager/cm-5.7.2/etc/cloudera-scm-agent/config.ini# 确认修改成 显示：server_host=hadoop1cat /opt/cloudera-manager/cm-5.7.2/etc/cloudera-scm-agent/config.ini|grep server_host# 设置数据库驱动（创建软连接）ln -s /usr/share/java/mysql-connector-java.jar /opt/cloudera-manager/cm-5.7.2/share/cmf/lib/mysql-connector-java.jar# hadoop1上mkdir /var/cloudera-scm-serverchown -R cloudera-scm:cloudera-scm /var/cloudera-scm-serverchown -R cloudera-scm:cloudera-scm /opt/cloudera-manager# 所有节点上mkdir -p /opt/cloudera/parcelschown cloudera-scm:cloudera-scm /opt/cloudera/parcels# 在hadoop1上# 上传CDH parcelsmkdir /opt/cloudera/parcel-repocp /data01/CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel /opt/cloudera/parcel-repo/cp /data01/CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.sha1 /opt/cloudera/parcel-repo/CDH-5.7.2-1.cdh5.7.2.p0.18-el7.parcel.shacp /data01/manifest.json /opt/cloudera/parcel-repo/chown -R cloudera-scm:cloudera-scm /opt/cloudera/parcel-repoll /opt/cloudera/parcel-repo# 初始化数据库/opt/cloudera-manager/cm-5.7.2/share/cmf/schema/scm_prepare_database.sh mysql -hhadoop1 -uroot -p123456 --scm-host hadoop1 scmdbn scmdbu scmdbp出现下图中的 All done，your SCM database is configured correctly！ 则说明数据库配置正确 如果出现Access denied for user ‘root’@’localhost’ (using password: YES)： 请检查/etc/hosts里面是否有 127.0.0.1 localhost，然后确认mariadb的数据库创建语句中权限分配是否正确。 如果出现Can’t create database ‘scmdbn’; database exists： 请登录mysql，删除该scmdbn数据库，再重新执行上面的初始化数据库的SQL命令，删除该数据库的SQL语句为：drop database scmdbn; 启动CM server# 在hadoop1上/opt/cloudera-manager/cm-5.7.2/etc/init.d/cloudera-scm-server start/opt/cloudera-manager/cm-5.7.2/etc/init.d/cloudera-scm-server status# 在所有节点上/opt/cloudera-manager/cm-5.7.2/etc/init.d/cloudera-scm-agent start/opt/cloudera-manager/cm-5.7.2/etc/init.d/cloudera-scm-agent status# 如果需要关闭CM,将start改成stop即可卸载openJdk# 查看是否安装openjdkrpm -qa|grep java# 卸载掉所有包含openjdk的东西rpm -e --nodeps java-1.7.0-openjdk-1.7.0.9-2.3.4.1.el6_3.i686rpm -e --nodeps java-1.6.0-openjdk-1.6.0.0-1.50.1.11.5.el6_3.i6863.2 CDH安装 账号密码均为admin 在这里选择免费 选择所有主机 选择已经下载好的parcel 等待分配和安装 根据需求选择所需要的服务 根据规划选择相应的服务到不同的主机上去 一般来说，Master-Slave结构的服务Master只能安装到一台主机上（例如NameNode），而Slave则可以同时安装到多台(&amp;gt;=2)主机上（例如DataNode） NameNode和SecondaryNameNode不能安装在同一主机上，这两台主机的硬件配置需要一致 建议所有DataNode配置一致，并且DataNode主机都安装Yarn的RegionServer 各服务最好合理分配到不同节点，避免单个节点同时运行多种任务，导致负载过大 配置hive，oozie的数据库信息，填写之前已经写好的的账号密码即可 服务具体配置，默认即可，有需求的可以根据自己的需求进行更改 配置完成后，开始安装服务" }, { "title": "spark源码分析 - DAGScheduler实现", "url": "/posts/spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-DAGScheduler%E5%AE%9E%E7%8E%B0/", "categories": "大数据框架, spark", "tags": "spark", "date": "2019-01-27 00:12:26 +0800", "snippet": "DAGScheduler实现1.DAGScheduler的创建TaskScheduler和DAGScheduler都是在SparkContext创建的时候创建的。其中TaskScheduler是通过org.apache.spark.SparkContext#createTaskScheduler创建的，而DAGScheduler是直接调用构造函数创建的。只不过DAGScheduler中保存了TaskScheduler的引用,因此需要在TaskScheduler创建之后创建SparkContext// 在SparkContext中创建DAGScheduler_dagScheduler = new DAGScheduler(this)// 构造函数的实现def this(sc: SparkContext) = this(sc, sc.taskScheduler)// 继续跟进def this(sc: SparkContext, taskScheduler: TaskScheduler) = { this( sc, taskScheduler, sc.listenerBus, sc.env.mapOutputTracker.asInstanceOf[MapOutputTrackerMaster], sc.env.blockManager.master, sc.env)}//里面的thisclass DAGScheduler( private[scheduler] val sc: SparkContext, private[scheduler] val taskScheduler: TaskScheduler, listenerBus: LiveListenerBus, mapOutputTracker: MapOutputTrackerMaster, blockManagerMaster: BlockManagerMaster, env: SparkEnv, clock: Clock = new SystemClock()) extends Logging { ... }2. Job的提交用户提交的Job最终会执行DAGScheduler的runjob，以foreach为例，过程如下 org.apache.spark.rdd.RDD#foreach org.apache.spark.SparkContext#runJob org.apache.spark.scheduler.DAGScheduler#runJob org.apache.spark.scheduler.DAGScheduler#submitJob org.apache.spark.scheduler.DAGSchedulerEventProcessLoop#doOnReceive(case JobSubmitted) org.apache.spark.scheduler.DAGScheduler#handleJobSubmitted 简单描述一下，foreach会触发SparkContext中的runjob，SparkContex中的runjob会不断调用SparkContext中的其他重载的runjob，最终会调用DAGScheduler中的runjobrunjob // 调用submitJob来处理 val waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties) // 接受处理完成后的状态 waiter.awaitResult() match { case JobSucceeded =&amp;gt; logInfo(&quot;Job %d finished: %s, took %f s&quot;.format (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9)) case JobFailed(exception: Exception) =&amp;gt; logInfo(&quot;Job %d failed: %s, took %f s&quot;.format (waiter.jobId, callSite.shortForm, (System.nanoTime - start) / 1e9)) // SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler. val callerStackTrace = Thread.currentThread().getStackTrace.tail exception.setStackTrace(exception.getStackTrace ++ callerStackTrace) throw exception }submitJob// 首先会生成JobIDval jobId = nextJobId.getAndIncrement() ...// 然后创建一个waiter用来监控Job的执行状态val waiter = new JobWaiter(this, jobId, partitions.size, resultHandler)// 最后向eventProcessLoop提交Job eventProcessLoop.post(JobSubmitted( jobId, rdd, func2, partitions.toArray, callSite, waiter, SerializationUtils.clone(properties))) waiter最终eventProcessLoop接收到JobSubmitted，然后调用handleJobSubmitted处理Job private def doOnReceive(event: DAGSchedulerEvent): Unit = event match { case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&amp;gt; dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) ... }3. Stage的划分3.1 什么是Stage用户提交的计算任务是由一个RDD构成的DAG，如果DAG在转换的时候需要做Shuffle，那么Shuffle的过程就给这个DAG分成了不同的Stage。其中不同的Stage不同并行计算，因为需要计算的数据来源于上一个Stage，而同一个Stage由一组完全独立的Task组成，每个Task计算逻辑完全相同，但是所处理的数据不同，这些数据也就是Partition，所以Task的数量是与Partition一一对应的具体Shuffle的流程以及宽依赖窄依赖可以看另一篇博文：Spark内部原理3.2 划分流程简述一下划分过程： 首先Stage会从最后一个开始划分，也就是触发Action的那个，也就是图中的 RDD G RDD G 依赖 RDD B、RDD F，会随机选择一个进行处理，这里我以RDD B 开始 由于RDD G 和 RDD B之间为窄依赖，所以RDD B 会和 RDD G 划分在同一个Stage （Stage 3） RDD F 和 RDD G 之间为宽依赖 ，则 RDD F 和 RDD G 被划分到不同的Stage （Stage3、Stage2），其中Stage3 的parentRDD 就是Stage2 继续处理RDD B ，由于RDD B与RDD A为宽依赖，所以被划分到不同的Stage（Stage3、Stage1） RDD F 同理，与其他几个RDD 均为窄依赖，所以全部划分到Stage 23.3 实现细节handleJobSubmitted 会通过调用 org.apache.spark.scheduler.DAGScheduler#newResultStage 来创建finnalStage，即途中的Stage3handleJobSubmittedfinalStage = newResultStage(finalRDD, func, partitions, jobId, callSite)newResultStageval (parentStages: List[Stage], id: Int) = getParentStagesAndId(rdd, jobId)继续跟进，getParentStagesAndIdprivate def getParentStagesAndId(rdd: RDD[_], firstJobId: Int): (List[Stage], Int) = { // 根据JobId获取parentStage val parentStages = getParentStages(rdd, firstJobId) // 获取当前id，并自增，所以父Stage 的id是最小 val id = nextStageId.getAndIncrement() (parentStages, id)}继续查看getParentStagesprivate def getParentStages(rdd: RDD[_], firstJobId: Int): List[Stage] = { // 存储parent Stage val parents = new HashSet[Stage] // 存储已经访问的RDD val visited = new HashSet[RDD[_]] // 存储需要被处理的RDD val waitingForVisit = new Stack[RDD[_]] def visit(r: RDD[_]) { if (!visited(r)) { // 对已经便利过的RDD进行标记 visited += r for (dep &amp;lt;- r.dependencies) { // 匹配当前RDD的依赖，如果为ShuffleDependency，则生成新的stage，如果不是则继续压栈，那么也就是同一个stage dep match { case shufDep: ShuffleDependency[_, _, _] =&amp;gt; parents += getShuffleMapStage(shufDep, firstJobId) case _ =&amp;gt; waitingForVisit.push(dep.rdd) } } } } // 将RDD 进行压栈 然后进行处理 waitingForVisit.push(rdd) while (waitingForVisit.nonEmpty) { visit(waitingForVisit.pop()) } parents.toList}在上述代码中，对指定的RDD的依赖进行了广度优先级便利，遇到窄依赖则归为统一stage，如果是宽依赖，则生成一个新的stage。显然，在这里只是对finalStage的依赖进行了便利，并没有对所有的RDD的依赖都进行便利。继续跟进 getShuffleMapStageprivate def getShuffleMapStage( shuffleDep: ShuffleDependency[_, _, _], firstJobId: Int): ShuffleMapStage = { shuffleToMapStage.get(shuffleDep.shuffleId) match { // 若是已经存在stage，则直接返回 case Some(stage) =&amp;gt; stage // 不存在则生成新的stage case None =&amp;gt; // 继续便利当前rdd 的依赖，并生成Stage getAncestorShuffleDependencies(shuffleDep.rdd).foreach { dep =&amp;gt; shuffleToMapStage(dep.shuffleId) = newOrUsedShuffleStage(dep, firstJobId) } // 为当前shuffle 生成新的stage val stage = newOrUsedShuffleStage(shuffleDep, firstJobId) shuffleToMapStage(shuffleDep.shuffleId) = stage stage }}如果是第一次进行shuffleToMapStage，那么结果肯定是none，继续跟进getAncestorShuffleDependencies private def getAncestorShuffleDependencies(rdd: RDD[_]): Stack[ShuffleDependency[_, _, _]] = { // 保存所有的shuffle依赖，注意：与getParentStage不同的是，这里是保存所有的 val parents = new Stack[ShuffleDependency[_, _, _]] // 记录已经被访问的RDD val visited = new HashSet[RDD[_]] // 建立Stack，保存等待被访问的RDD val waitingForVisit = new Stack[RDD[_]] def visit(r: RDD[_]) { if (!visited(r)) { // 打上标记 visited += r // 便利当前RDD所有依赖 for (dep &amp;lt;- r.dependencies) { dep match { // 如果是shuffle依赖，并判断stage 并没有存在，则添加到parents中 case shufDep: ShuffleDependency[_, _, _] =&amp;gt; if (!shuffleToMapStage.contains(shufDep.shuffleId)) { parents.push(shufDep) } case _ =&amp;gt; } // 注意：与getParentStage不同的是，即使是shuffleDependency的rdd也要继续遍历 waitingForVisit.push(dep.rdd) } } } waitingForVisit.push(rdd) while (waitingForVisit.nonEmpty) { visit(waitingForVisit.pop()) } parents }这里似乎和getParentStages很像，但是这里确实便利了所有的祖先的依赖关系，而不是当前RDD的依赖关系到此为止已经完成了所有shuffleStage的生成，来看看是如何生成的 newOrUsedShuffleStageprivate def newOrUsedShuffleStage( shuffleDep: ShuffleDependency[_, _, _], firstJobId: Int): ShuffleMapStage = { val rdd = shuffleDep.rdd val numTasks = rdd.partitions.length val stage = newShuffleMapStage(rdd, numTasks, shuffleDep, firstJobId, rdd.creationSite) // 首先会判断当前stage是否被计算过，被计算过则重新计算一个新的stage if (mapOutputTracker.containsShuffle(shuffleDep.shuffleId)) { // stage已经被计算过，从mapOutputTracker中获取计算结果 val serLocs = mapOutputTracker.getSerializedMapOutputStatuses(shuffleDep.shuffleId) val locs = MapOutputTracker.deserializeMapStatuses(serLocs) (0 until locs.length).foreach { i =&amp;gt; if (locs(i) ne null) { // 将计算结果拷贝到stage中 stage.addOutputLoc(i, locs(i)) } } } else { // stage未被计算过，则向mapOutputTracker中对stage进行注册 logInfo(&quot;Registering RDD &quot; + rdd.id + &quot; (&quot; + rdd.getCreationSite + &quot;)&quot;) mapOutputTracker.registerShuffle(shuffleDep.shuffleId, rdd.partitions.length) } stage}到此位置，由finalRDD往前追溯递归生成Stage，最前面的ShuffleStage先生成，最终生成ResultStage，至此，DAGScheduler对Stage的划分已经完成。3.3 任务的生成先回到org.apache.spark.scheduler.DAGScheduler#handleJobSubmittedfinalStage = newResultStage(finalRDD, func, partitions, jobId, callSite)...submitStage(finalStage)在完成stage划分后就开始提交stage了，submitStageprivate def submitStage(stage: Stage) { val jobId = activeJobForStage(stage) if (jobId.isDefined) { logDebug(&quot;submitStage(&quot; + stage + &quot;)&quot;) // 如果该stage没有等待其他parent stage返回，没有正在运行，且没有失败提示，阿么就提交他 if (!waitingStages(stage) &amp;amp;&amp;amp; !runningStages(stage) &amp;amp;&amp;amp; !failedStages(stage)) { val missing = getMissingParentStages(stage).sortBy(_.id) logDebug(&quot;missing: &quot; + missing) if (missing.isEmpty) { logInfo(&quot;Submitting &quot; + stage + &quot; (&quot; + stage.rdd + &quot;), which has no missing parents&quot;) // 确保所有的parent stage都已经完成，那么提交该stage所包含的task submitMissingTasks(stage, jobId.get) } else { // 如果没有则递归提交他 for (parent &amp;lt;- missing) { submitStage(parent) } waitingStages += stage } } } else {// 无效stage，直接停掉 abortStage(stage, &quot;No active job for stage &quot; + stage.id, None) }}org.apache.spark.scheduler.DAGScheduler#submitMissingTasks会最终完成DAGScheduler所有的工作，即向TaskScheduler提交Task。提交的顺序图首先，取得需要计算的partition，对于最后的stage，它对应的是ResultTask，因此需要判断该Partition的ResultTask是否结束，如果结束则无需计算；对于其他的Stage，它们对应的Task都是ShuffleMapTask，因此只需要判断Stage是否有缓存结果即可在判断出Partition需要计算后，就会为每个Partition生成Task，然后封装成TaskSet，最后提交给TaskScheduler，从逻辑上有上图变成了下图TaskSet 保存了Stage的一组完全相同的Task，每个Task处理的逻辑完全相同，不同的是处理的数据，每个Task负责处理一个Partition，他们从数据源获取逻辑，然后按照拓扑顺序，顺序执行" }, { "title": "spark RPC原理", "url": "/posts/spark-RPC%E5%8E%9F%E7%90%86/", "categories": "大数据框架, spark", "tags": "spark", "date": "2019-01-01 04:10:20 +0800", "snippet": "1. 概述Spark-1.6以后RPC默认使用Netty替代Akka，在Netty上加了一层封装，为实现对Spark的定制开发，所以了解Spark中RPC的原理还是有必要的Akka是一个异步的消息框架，所谓的异步，简言之就是消息发送方发送出消息，不用阻塞等待结果，接收方处理完返回结果即可。Akka支持百万级的消息传递，特别适合复杂的大规模分布式系统。Akka基于Actor模型，提供用于创建可扩展，弹性，快速响应的应用程序的平台。Actor封装了状态和行为的对象，不同的Actor之间可通过消息交换实现通信，每个Actor都有自己的消息收件箱。Akka可以简化并发场景下的开发，其异步，高性能的事件驱动模型，轻量级的事件处理可大大方便用于开发复杂的分布式系统。早期Spark大量采用Akka作为RPC。Netty也是一个知名的高性能，异步消息框架，Spark早期便使用它解决大文件传输问题，用来克服Akka的短板。根据社区的说法，因为很多Spark用户饱受Akka复杂依赖关系的困扰，所以后来干脆就直接用Netty代替了Akka。2. Spark 1.6+ 中的RPC2.1 RpcEndpoint用来通讯的个体（Master，Worker，Diver），一个RpcEndpoint的生命周期包括：onStart()-&amp;gt;recevie-&amp;gt;onStop()，当然也还有很多其他方法，后面会有介绍2.2 RpcEndpointRefRpcEndpoint的一个引用，当我们需要向一个具体的RpcEndpoint发送消息时，一般我们需要获取到该RpcEndpoint的引用，然后通过该应用发送消息。2.3 RpcAddressRpcEndpointRef的地址，Host + Port。2.4 RpcEnvRpcEnv为RpcEndpoint提供处理消息的环境。RpcEnv负责RpcEndpoint整个生命周期的管理，包括：注册endpoint，endpoint之间消息的路由，以及停止endpoint。3. RPC网络通信的抽象图 核心的RpcEnv是一个特质（trait），它主要提供了停止，注册，获取endpoint等方法的定义，而NettyRpcEnv提供了该特质的一个具体实现。 通过工厂RpcEnvFactory来产生一个RpcEnv，而NettyRpcEnvFactory用来生成NettyRpcEnv的一个对象。 当我们调用RpcEnv中的setupEndpoint来注册一个endpoint到rpcEnv的时候，在NettyRpcEnv内部，会将该endpoint的名称与其本省的映射关系，rpcEndpoint与rpcEndpointRef之间映射关系保存在dispatcher对应的成员变量中。 3. Master具体实现3.1 Master类的定义private[deploy] class Master( override val rpcEnv: RpcEnv, address: RpcAddress, webUiPort: Int, val securityMgr: SecurityManager, val conf: SparkConf) extends ThreadSafeRpcEndpoint with Logging with LeaderElectable { ......可以看到Master继承ThreadSafeRpcEndpoint，而ThreadSafeRpcEndpoint继承RpcEndpoint，跟上面的抽象图差不多3.2 Master启动 def startRpcEnvAndEndpoint( host: String, port: Int, webUiPort: Int, conf: SparkConf): (RpcEnv, Int, Option[Int]) = { val securityMgr = new SecurityManager(conf) val rpcEnv = RpcEnv.create(SYSTEM_NAME, host, port, conf, securityMgr) val masterEndpoint = rpcEnv.setupEndpoint(ENDPOINT_NAME, new Master(rpcEnv, rpcEnv.address, webUiPort, securityMgr, conf)) val portsResponse = masterEndpoint.askWithRetry[BoundPortsResponse](BoundPortsRequest) (rpcEnv, portsResponse.webUIPort, portsResponse.restPort) }会通过NettyRpcEnvFactory创建一个RpcEnv，rcpEnv在创建一个Endpoint，也就是Master了3.3 RpcEndpoint特质master的启动会创建一个RpcEnv并将自己注册到其中。继续看RpcEndpoint特质的定义：private[spark] trait RpcEndpoint { //当前RpcEndpoint注册到的RpcEnv主子，可以类比为Akka中的actorSystem val rpcEnv: RpcEnv //直接用来发送消息的RpcEndpointRef，可以类比为Akka中的actorRef final def self: RpcEndpointRef = { require(rpcEnv != null, &quot;rpcEnv has not been initialized&quot;) rpcEnv.endpointRef(this) } //处理来自RpcEndpointRef.send或者RpcCallContext.reply的消息 def receive: PartialFunction[Any, Unit] = { case _ =&amp;gt; throw new SparkException(self + &quot; does not implement &#39;receive&#39;&quot;) } //处理来自RpcEndpointRef.ask的消息，会有相应的回复 def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = { case _ =&amp;gt; context.sendFailure(new SparkException(self + &quot; won&#39;t reply anything&quot;)) } //篇幅限制，其余onError，onConnected，onDisconnected，onNetworkError， //onStart，onStop，stop方法此处省略}3.4 RpcEnv抽象类private[spark] abstract class RpcEnv(conf: SparkConf) { private[spark] val defaultLookupTimeout = RpcUtils.lookupRpcTimeout(conf) //返回endpointRef private[rpc] def endpointRef(endpoint: RpcEndpoint): RpcEndpointRef //返回RpcEnv监听的地址 def address: RpcAddress //注册一个RpcEndpoint到RpcEnv并返回RpcEndpointRef def setupEndpoint(name: String, endpoint: RpcEndpoint): RpcEndpointRef //通过uri异步地查询RpcEndpointRef def asyncSetupEndpointRefByURI(uri: String): Future[RpcEndpointRef] //通过uri查询RpcEndpointRef，这种方式会产生阻塞 def setupEndpointRefByURI(uri: String): RpcEndpointRef = { defaultLookupTimeout.awaitResult(asyncSetupEndpointRefByURI(uri)) } //通过address和endpointName查询RpcEndpointRef，这种方式会产生阻塞 def setupEndpointRef(address: RpcAddress, endpointName: String): RpcEndpointRef = { setupEndpointRefByURI(RpcEndpointAddress(address, endpointName).toString) } //关掉endpoint def stop(endpoint: RpcEndpointRef): Unit //关掉RpcEnv def shutdown(): Unit //等待结束 def awaitTermination(): Unit //没有RpcEnv的话RpcEndpointRef是无法被反序列化的，这里是反序列化逻辑 def deserialize[T](deserializationAction: () =&amp;gt; T): T //返回文件server实例 def fileServer: RpcEnvFileServer //开一个针对给定URI的channel用来下载文件 def openChannel(uri: String): ReadableByteChannel}另外RpcEnv有一个伴生对象，实现了create方法：private[spark] object RpcEnv { def create( name: String, host: String, port: Int, conf: SparkConf, securityManager: SecurityManager, clientMode: Boolean = false): RpcEnv = { val config = RpcEnvConfig(conf, name, host, port, securityManager, clientMode) new NettyRpcEnvFactory().create(config) }}这就是在master启动方法中的create具体实现，可以看到调用了Netty工厂方法NettyRpcEnvFactory，该方法是对Netty的具体封装。3.5 master中消息处理上文可以看到，在RpcEndpoint中最核心的便是receive和receiveAndReply方法，定义了消息处理的核心逻辑，master中也有相应的实现：override def receive: PartialFunction[Any, Unit] = { case ElectedLeader =&amp;gt; case CompleteRecovery =&amp;gt; case RevokedLeadership =&amp;gt; case RegisterApplication(description, driver) =&amp;gt; case ExecutorStateChanged(appId, execId, state, message, exitStatus) =&amp;gt; case DriverStateChanged(driverId, state, exception) =&amp;gt; case Heartbeat(workerId, worker) =&amp;gt; case MasterChangeAcknowledged(appId) =&amp;gt; case WorkerSchedulerStateResponse(workerId, executors, driverIds) =&amp;gt; case WorkerLatestState(workerId, executors, driverIds) =&amp;gt; case UnregisterApplication(applicationId) =&amp;gt; case CheckForWorkerTimeOut =&amp;gt;}这里定义了master一系列的消息处理逻辑，而receiveAndReply中，override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = { case RegisterWorker( id, workerHost, workerPort, workerRef, cores, memory, workerWebUiUrl) =&amp;gt; case RequestSubmitDriver(description) =&amp;gt; case RequestKillDriver(driverId) =&amp;gt; case RequestDriverStatus(driverId) =&amp;gt; case RequestMasterState =&amp;gt; case BoundPortsRequest =&amp;gt; case RequestExecutors(appId, requestedTotal) =&amp;gt; case KillExecutors(appId, executorIds) =&amp;gt;}定义了对需要回复的消息组的处理逻辑。在看看Worker的实现3.6 Workerprivate[deploy] class Worker( override val rpcEnv: RpcEnv, webUiPort: Int, cores: Int, memory: Int, masterRpcAddresses: Array[RpcAddress], endpointName: String, workDirPath: String = null, val conf: SparkConf, val securityMgr: SecurityManager) extends ThreadSafeRpcEndpoint with Logging {与Master一样3.7 worker启动方法def startRpcEnvAndEndpoint( host: String, port: Int, webUiPort: Int, cores: Int, memory: Int, masterUrls: Array[String], workDir: String, workerNumber: Option[Int] = None, conf: SparkConf = new SparkConf): RpcEnv = { // The LocalSparkCluster runs multiple local sparkWorkerX RPC Environments val systemName = SYSTEM_NAME + workerNumber.map(_.toString).getOrElse(&quot;&quot;) val securityMgr = new SecurityManager(conf) val rpcEnv = RpcEnv.create(systemName, host, port, conf, securityMgr) val masterAddresses = masterUrls.map(RpcAddress.fromSparkURL(_)) rpcEnv.setupEndpoint(ENDPOINT_NAME, new Worker(rpcEnv, webUiPort, cores, memory, masterAddresses, ENDPOINT_NAME, workDir, conf, securityMgr)) rpcEnv}和Master一样，但是Worker会在onStart()向Master进行注册3.8 worker注册到master RpcEnvprivate def registerWithMaster(masterEndpoint: RpcEndpointRef): Unit = { masterEndpoint.ask[RegisterWorkerResponse](RegisterWorker( workerId, host, port, self, cores, memory, workerWebUiUrl)) .onComplete { // This is a very fast action so we can use &quot;ThreadUtils.sameThread&quot; case Success(msg) =&amp;gt; Utils.tryLogNonFatalError { handleRegisterResponse(msg) } case Failure(e) =&amp;gt; logError(s&quot;Cannot register with master: ${masterEndpoint.address}&quot;, e) System.exit(1) }(ThreadUtils.sameThread)}masterEndpoint.ask是核心，发送了一个RegisterWorker消息到masterEndpoint并期待对方的RegisterWorkerResponse，对response做出相应的处理。这样worker就成功和master建立了连接，它们之间可以互相发送消息进行通信。3.9worker到master的通信worker和master之间是一个主从关系，worker注册到master之后，master就可以通过消息传递实现对worker的管理，在worker中有一个方法：private def sendToMaster(message: Any): Unit = { master match { case Some(masterRef) =&amp;gt; masterRef.send(message) case None =&amp;gt; logWarning( s&quot;Dropping $message because the connection to master has not yet been established&quot;) }}一目了然，就是干的发送消息到master的活儿，在worker中很多地方都用到这个方法，比如handleExecutorStateChanged(executorStateChanged:ExecutorStateChanged)方法中，sendToMaster(executorStateChanged)就向masterRef发送了executorStateChanged消息，前文中master中的recevie方法中，就有一个对ExecutorStateChanged消息的处理逻辑。3.10master到worker的通信同样的，master要对worker实现管理也是通过发送消息实现的，比如launchExecutor(worker: WorkerInfo, exec: ExecutorDesc)方法中：private def launchExecutor(worker: WorkerInfo, exec: ExecutorDesc): Unit = { logInfo(&quot;Launching executor &quot; + exec.fullId + &quot; on worker &quot; + worker.id) worker.addExecutor(exec) //向worker发送LaunchExecutor消息 worker.endpoint.send(LaunchExecutor(masterUrl, exec.application.id, exec.id, exec.application.desc, exec.cores, exec.memory)) exec.application.driver.send( ExecutorAdded(exec.id, worker.id, worker.hostPort, exec.cores, exec.memory))}master向worker发送了LaunchExecutor消息告诉worker应该启动executor了，而worker中的receive方法中对LaunchExecutor消息进行处理并完成master交代给自己的任务。" }, { "title": "Spark源码分析 - start-all", "url": "/posts/Spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-start-all/", "categories": "大数据框架, spark", "tags": "spark", "date": "2019-01-01 00:54:31 +0800", "snippet": "版本spark-1.6过程分析start-all.sh# 加载环境. &quot;${SPARK_HOME}/sbin/spark-config.sh&quot;# 启动Master&quot;${SPARK_HOME}/sbin&quot;/start-master.sh $TACHYON_STR# 启动Worker&quot;${SPARK_HOME}/sbin&quot;/start-slaves.sh $TACHYON_STRstart-master.sh......# 类名CLASS=&quot;org.apache.spark.deploy.master.Master&quot;# 加载该类if [[ &quot;$@&quot; = *--help ]] || [[ &quot;$@&quot; = *-h ]]; then echo &quot;Usage: ./sbin/start-master.sh [options]&quot; pattern=&quot;Usage:&quot; pattern+=&quot;\\|Using Spark&#39;s default log4j profile:&quot; pattern+=&quot;\\|Registered signal handlers for&quot; &quot;${SPARK_HOME}&quot;/bin/spark-class $CLASS --help 2&amp;gt;&amp;amp;1 | grep -v &quot;$pattern&quot; 1&amp;gt;&amp;amp;2 exit 1fi......org.apache.spark.deploy.master.Master 让我们先来看看main()方法 def main(argStrings: Array[String]) { // 日志注册 SignalLogger.register(log) val conf = new SparkConf val args = new MasterArguments(argStrings, conf) // 注册消息处理的环境以及Master的通讯工具，具体通讯方式会另外补充一篇博文 val (rpcEnv, _, _) = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, conf) // 注册完成后等待接受消息 rpcEnv.awaitTermination() } PS : Spark-1.6以后RPC默认使用Netty替代Akka，在Netty上加了一层封装，为实现对Spark的定制开发。 具体是为什么，见Enable Spark user applications to use different versions of AkkastartRpcEnvAndEndpoint() def startRpcEnvAndEndpoint( host: String, port: Int, webUiPort: Int, conf: SparkConf): (RpcEnv, Int, Option[Int]) = { val securityMgr = new SecurityManager(conf) //注册通讯环境 val rpcEnv = RpcEnv.create(SYSTEM_NAME, host, port, conf, securityMgr) // 注册Master通讯节点 val masterEndpoint = rpcEnv.setupEndpoint(ENDPOINT_NAME, new Master(rpcEnv, rpcEnv.address, webUiPort, securityMgr, conf)) val portsResponse = masterEndpoint.askWithRetry[BoundPortsResponse](BoundPortsRequest) (rpcEnv, portsResponse.webUIPort, portsResponse.restPort) }然后到Workerstart-slave.sh# 流程与Master一样CLASS=&quot;org.apache.spark.deploy.worker.Worker&quot;if [[ $# -lt 1 ]] || [[ &quot;$@&quot; = *--help ]] || [[ &quot;$@&quot; = *-h ]]; then echo &quot;Usage: ./sbin/start-slave.sh [options] &amp;lt;master&amp;gt;&quot; pattern=&quot;Usage:&quot; pattern+=&quot;\\|Using Spark&#39;s default log4j profile:&quot; pattern+=&quot;\\|Registered signal handlers for&quot; &quot;${SPARK_HOME}&quot;/bin/spark-class $CLASS --help 2&amp;gt;&amp;amp;1 | grep -v &quot;$pattern&quot; 1&amp;gt;&amp;amp;2 exit 1fi依然是先来看看main()方法 def main(argStrings: Array[String]) { //与Master里面如出一辙 SignalLogger.register(log) val conf = new SparkConf val args = new WorkerArguments(argStrings, conf) val rpcEnv = startRpcEnvAndEndpoint(args.host, args.port, args.webUiPort, args.cores, args.memory, args.masters, args.workDir, conf = conf) rpcEnv.awaitTermination() }但是如果大家都是在等待消息的话可定是无法完成交互的，也就是说在启动这些的时候肯定还有其他东西也有运行，于是查看了下Worker继承的类ThreadSafeRpcEndpoint 里面包含一个onStart()方法， /** * Invoked before [[RpcEndpoint]] starts to handle any message. * 会在RpcEndingpoint创建前被调用，用于处理信息 */ def onStart(): Unit = { // By default, do nothing. }于是找到Worker.onStart() override def onStart() { assert(!registered) //日志记录 logInfo(&quot;Starting Spark worker %s:%d with %d cores, %s RAM&quot;.format( host, port, cores, Utils.megabytesToString(memory))) logInfo(s&quot;Running Spark version ${org.apache.spark.SPARK_VERSION}&quot;) logInfo(&quot;Spark home: &quot; + sparkHome) //创建Worker目录 createWorkDir() shuffleService.startIfEnabled() webUi = new WorkerWebUI(this, workDir, webUiPort) webUi.bind() //向Master进行注册 registerWithMaster() metricsSystem.registerSource(workerSource) metricsSystem.start() // Attach the worker metrics servlet handler to the web ui after the metrics system is started. metricsSystem.getServletHandlers.foreach(webUi.attachHandler) }我们继续往后看Worker.registerWithMaster()private def registerWithMaster() { // onDisconnected may be triggered multiple times, so don&#39;t attempt registration // if there are outstanding registration attempts scheduled. registrationRetryTimer match { case None =&amp;gt; registered = false // 会向所有的Master进行注册， // 需要补充的是在Spark中也存在Master的单点故障，所以也可以进行HA配置 registerMasterFutures = tryRegisterAllMasters() connectionAttemptCount = 0 ...... } }继续查看 tryRegisterAllMasters()方法 private def tryRegisterAllMasters(): Array[JFuture[_]] = { ..... // 继续向Master注册 registerWithMaster(masterEndpoint) ...... }registerWithMaster(masterEndpoint: RpcEndpointRef) private def registerWithMaster(masterEndpoint: RpcEndpointRef): Unit = { // 这里就是向Maser发送消息了，发送了一个RegisterWorker对象 masterEndpoint.ask[RegisterWorkerResponse](RegisterWorker( workerId, host, port, self, cores, memory, webUi.boundPort, publicAddress)) .onComplete { // This is a very fast action so we can use &quot;ThreadUtils.sameThread&quot; case Success(msg) =&amp;gt; Utils.tryLogNonFatalError { handleRegisterResponse(msg) } case Failure(e) =&amp;gt; logError(s&quot;Cannot register with master: ${masterEndpoint.address}&quot;, e) System.exit(1) }(ThreadUtils.sameThread) } 发送完消息多半就是去找Master接受消息的方法了果然在Master中找到了一个receiveAndReply()方法 override def receiveAndReply(context: RpcCallContext): PartialFunction[Any, Unit] = { //这里就是接受Work注册消息的地方了 case RegisterWorker( id, workerHost, workerPort, workerRef, cores, memory, workerUiPort, publicAddress) =&amp;gt; { // 首先日志输出一些Work的资源信息 logInfo(&quot;Registering worker %s:%d with %d cores, %s RAM&quot;.format( workerHost, workerPort, cores, Utils.megabytesToString(memory))) // 由于是对所有的Master发送的消息，所以存在StandBy接受了消息 // 这里是Master对自己的状态进行判断，如果自己是StandBy，则什么也不做 if (state == RecoveryState.STANDBY) { context.reply(MasterInStandby) } else if (idToWorker.contains(id)) { //如果该Worker已经在注册表里面，同样是什么也没做 context.reply(RegisterWorkerFailed(&quot;Duplicate worker ID&quot;)) } else { //最后经过重重判断，对传过来的Work信息进行注册 val worker = new WorkerInfo(id, workerHost, workerPort, cores, memory, workerRef, workerUiPort, publicAddress) if (registerWorker(worker)) { persistenceEngine.addWorker(worker) context.reply(RegisteredWorker(self, masterWebUiUrl)) schedule() } else { val workerAddress = worker.endpoint.address logWarning(&quot;Worker registration failed. Attempted to re-register worker at same &quot; + &quot;address: &quot; + workerAddress) context.reply(RegisterWorkerFailed(&quot;Attempted to re-register worker at same address: &quot; + workerAddress)) } } }其实到这里Start-All基本上就就结束了，后续的就是一些资源的调度了，会在后面继续进行分析" }, { "title": "spark源码分析 - submit", "url": "/posts/spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-submit/", "categories": "大数据框架, spark", "tags": "spark", "date": "2018-12-29 07:57:10 +0800", "snippet": "下图大致描述了整个过程spark-submitif [ -z &quot;${SPARK_HOME}&quot; ]; then export SPARK_HOME=&quot;$(cd &quot;`dirname &quot;$0&quot;`&quot;/..; pwd)&quot;fi# disable randomized hash for string in Python 3.3+export PYTHONHASHSEED=0# 调用bin目录中的spark-class 参数为org.apache.spark.deploy.SparkSubmitexec &quot;${SPARK_HOME}&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;会先经历spark-class org.apache.spark.deploy.SparkSubmit处理，里面包括一些基本环境配置，然后运行再看看 org.apache.spark.deploy.SparkSubmit的main函数 def main(args: Array[String]): Unit = { val appArgs = new SparkSubmitArguments(args) if (appArgs.verbose) { // scalastyle:off println printStream.println(appArgs) // scalastyle:on println } // 在这里进行操作的匹配，在这里我们肯定是进入submit() appArgs.action match { case SparkSubmitAction.SUBMIT =&amp;gt; submit(appArgs) case SparkSubmitAction.KILL =&amp;gt; kill(appArgs) case SparkSubmitAction.REQUEST_STATUS =&amp;gt; requestStatus(appArgs) } }submit()private def submit(args: SparkSubmitArguments): Unit = { //先是初始化环境，包括建立合适的环境变量，系统配置，应用参数 val (childArgs, childClasspath, sysProps, childMainClass) = prepareSubmitEnvironment(args) .....}prepareSubmitEnvironment(args)/** * Prepare the environment for submitting an application. * This returns a 4-tuple: * (1) the arguments for the child process, * (2) a list of classpath entries for the child, * (3) a map of system properties, and * (4) the main class for the child * Exposed for testing. * 这些都是spark-submit 里面的一些配置，例如启动模式、环境变量。这个方法就是根据这些启动模式来做 * 出对应的处理，由于一般情况下会在yarn进行任务运行，所以这次运行的模式--master yarn --deploy- * mode cluster */ private[deploy] def prepareSubmitEnvironment(args: SparkSubmitArguments) : (Seq[String], Seq[String], Map[String, String], String) = { ...... //前面都是一些判断，直接进入正题 if (isYarnCluster) { //当为yarn 的cluster模式时 会调用org.apache.spark.deploy.yarn.Client类 childMainClass = &quot;org.apache.spark.deploy.yarn.Client&quot; //是否使用Python if (args.isPython) { childArgs += (&quot;--primary-py-file&quot;, args.primaryResource) if (args.pyFiles != null) { childArgs += (&quot;--py-files&quot;, args.pyFiles) } childArgs += (&quot;--class&quot;, &quot;org.apache.spark.deploy.PythonRunner&quot;) //或者使用R } else if (args.isR) { val mainFile = new Path(args.primaryResource).getName childArgs += (&quot;--primary-r-file&quot;, mainFile) childArgs += (&quot;--class&quot;, &quot;org.apache.spark.deploy.RRunner&quot;) } else { //最后是默认情况，也就是我们这次任务执行的模式 if (args.primaryResource != SPARK_INTERNAL) { childArgs += (&quot;--jar&quot;, args.primaryResource) } childArgs += (&quot;--class&quot;, args.mainClass) } if (args.childArgs != null) { args.childArgs.foreach { arg =&amp;gt; childArgs += (&quot;--arg&quot;, arg) } } } //最后通过筛选，返回这些参数 (childArgs, childClasspath, sysProps, childMainClass) }再回到submit(),会执行runMain(childArgs, childClasspath, sysProps, childMainClass, args.verbose)，也就是传入我们刚刚返回的参数private def runMain( childArgs: Seq[String], childClasspath: Seq[String], sysProps: Map[String, String], childMainClass: String, verbose: Boolean): Unit = { //前面会根据传入的参数进行环境配置，参数导入以及日志的打印 ...... try { // 加载我们传入的类，也就是 org.apache.spark.deploy.yarn.Client mainClass = Utils.classForName(childMainClass) } catch { case e: ClassNotFoundException =&amp;gt; e.printStackTrace(printStream) .......//捕捉异常信息，这里就忽略了 } //获取传入类的main函数 val mainMethod = mainClass.getMethod(&quot;main&quot;, new Array[String](0).getClass) ...... try { // 调用main函数 mainMethod.invoke(null, childArgs.toArray) }}然后来看看org.apache.spark.deploy.yarn.Client.main() def main(argStrings: Array[String]) { if (!sys.props.contains(&quot;SPARK_SUBMIT&quot;)) { logWarning(&quot;WARNING: This client is deprecated and will be removed in a &quot; + &quot;future version of Spark. Use ./bin/spark-submit with \\&quot;--master yarn\\&quot;&quot;) } // Set an env variable indicating we are running in YARN mode. // Note that any env variable with the SPARK_ prefix gets propagated to all (remote) processes System.setProperty(&quot;SPARK_YARN_MODE&quot;, &quot;true&quot;) val sparkConf = new SparkConf val args = new ClientArguments(argStrings, sparkConf) // to maintain backwards-compatibility if (!Utils.isDynamicAllocationEnabled(sparkConf)) { sparkConf.setIfMissing(&quot;spark.executor.instances&quot;, args.numExecutors.toString) } //在一顿初始化与判断后，初始化自己然后调用run()方法 new Client(args, sparkConf).run() }run()def run(): Unit = { //提交应用 this.appId = submitApplication() if (!launcherBackend.isConnected() &amp;amp;&amp;amp; fireAndForget) { ...... } else { val (yarnApplicationState, finalApplicationStatus) = monitorApplication(appId) ...... } }submitApplication(),来看看提交应用里面做了什么 def submitApplication(): ApplicationId = { var appId: ApplicationId = null try { //launcherBackend在前面已经进行了初始化，launcherBackend是一个底层使用Socket用来传递信息的抽象类 launcherBackend.connect() // Setup the credentials before doing anything else, // so we have don&#39;t have issues at any point. setupCredentials() yarnClient.init(yarnConf) yarnClient.start() logInfo(&quot;Requesting a new application from cluster with %d NodeManagers&quot; .format(yarnClient.getYarnClusterMetrics.getNumNodeManagers)) // Get a new application from our RM // 通过yarn api 创建一个application val newApp = yarnClient.createApplication() val newAppResponse = newApp.getNewApplicationResponse() appId = newAppResponse.getApplicationId() reportLauncherState(SparkAppHandle.State.SUBMITTED) launcherBackend.setAppId(appId.toString()) // Verify whether the cluster has enough resources for our AM // 检测集群是否有足够的资源可以调用 verifyClusterResources(newAppResponse) // 初始化环境用于启动ApplicationManager // Set up the appropriate contexts to launch our AM val containerContext = createContainerLaunchContext(newAppResponse) val appContext = createApplicationSubmissionContext(newApp, containerContext) //最后提交应用，并返回appId // Finally, submit and monitor the application logInfo(s&quot;Submitting application ${appId.getId} to ResourceManager&quot;) yarnClient.submitApplication(appContext) appId } catch { case e: Throwable =&amp;gt; if (appId != null) { cleanupStagingDir(appId) } throw e } }提交完任务后在回到 run() ，通过代码可以看出，他会继续执行monitorApplication(appId)，也就是监视任务的进行/** * Report the state of an application until it has exited, either successfully or * due to some failure, then return a pair of the yarn application state (FINISHED, FAILED, * KILLED, or RUNNING) and the final application state (UNDEFINED, SUCCEEDED, FAILED, * or KILLED). * * @param appId ID of the application to monitor. * @param returnOnRunning Whether to also return the application state when it is RUNNING. * @param logApplicationReport Whether to log details of the application report every iteration. * @return A pair of the yarn application state and the final application state. */ def monitorApplication( appId: ApplicationId, returnOnRunning: Boolean = false, logApplicationReport: Boolean = true): (YarnApplicationState, FinalApplicationStatus) = { val interval = sparkConf.getLong(&quot;spark.yarn.report.interval&quot;, 1000) var lastState: YarnApplicationState = null while (true) { Thread.sleep(interval) val report: ApplicationReport = try { getApplicationReport(appId) } catch { case e: ApplicationNotFoundException =&amp;gt; logError(s&quot;Application $appId not found.&quot;) return (YarnApplicationState.KILLED, FinalApplicationStatus.KILLED) case NonFatal(e) =&amp;gt; logError(s&quot;Failed to contact YARN for application $appId.&quot;, e) return (YarnApplicationState.FAILED, FinalApplicationStatus.FAILED) } val state = report.getYarnApplicationState if (logApplicationReport) { logInfo(s&quot;Application report for $appId (state: $state)&quot;) // If DEBUG is enabled, log report details every iteration // Otherwise, log them every time the application changes state if (log.isDebugEnabled) { logDebug(formatReportDetails(report)) } else if (lastState != state) { logInfo(formatReportDetails(report)) } } if (lastState != state) { state match { case YarnApplicationState.RUNNING =&amp;gt; reportLauncherState(SparkAppHandle.State.RUNNING) case YarnApplicationState.FINISHED =&amp;gt; reportLauncherState(SparkAppHandle.State.FINISHED) case YarnApplicationState.FAILED =&amp;gt; reportLauncherState(SparkAppHandle.State.FAILED) case YarnApplicationState.KILLED =&amp;gt; reportLauncherState(SparkAppHandle.State.KILLED) case _ =&amp;gt; } } if (state == YarnApplicationState.FINISHED || state == YarnApplicationState.FAILED || state == YarnApplicationState.KILLED) { cleanupStagingDir(appId) return (state, report.getFinalApplicationStatus) } if (returnOnRunning &amp;amp;&amp;amp; state == YarnApplicationState.RUNNING) { return (state, report.getFinalApplicationStatus) } lastState = state }" }, { "title": "spark源码分析 - shell", "url": "/posts/spark%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-shell/", "categories": "大数据框架, spark", "tags": "spark", "date": "2018-12-28 05:58:14 +0800", "snippet": "spark-shellfunction main() {# 对当前系统进行判断，通过spark-submits.sh 启动 org.apache.spark.repl.Main if $cygwin; then stty -icanon min 1 -echo &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 export SPARK_SUBMIT_OPTS=&quot;$SPARK_SUBMIT_OPTS -Djline.terminal=unix&quot; &quot;${SPARK_HOME}&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot; stty icanon echo &amp;gt; /dev/null 2&amp;gt;&amp;amp;1 else export SPARK_SUBMIT_OPTS &quot;${SPARK_HOME}&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot; fi}org.apache.spark.repl.Maindef main(args: Array[String]) { //初始化SparkILoop，调用process方法 _interp = new SparkILoop _interp.process(args)}SparkILoop.processprivate def process(settings: Settings): Boolean = savingContextLoader { ...... //前面内容很多，大致就是判断一些参数、初始化解释器之类的，例如运行的模式， //然后就运行主要的两个方法 addThunk(printWelcome()) addThunk(initializeSpark()) ...... //后面的也不是很重要}printWelcome//打印Spark 中的版本等信息，也就是每次启动Spark-shell显示的欢迎界面def printWelcome() { echo(&quot;&quot;&quot;Welcome to ____ __ / __/__ ___ _____/ /__ _\\ \\/ _ \\/ _ `/ __/ &#39;_/ /___/ .__/\\_,_/_/ /_/\\_\\ version %s /_/&quot;&quot;&quot;.format(SPARK_VERSION)) import Properties._ val welcomeMsg = &quot;Using Scala %s (%s, Java %s)&quot;.format( versionString, javaVmName, javaVersion) echo(welcomeMsg) echo(&quot;Type in expressions to have them evaluated.&quot;) echo(&quot;Type :help for more information.&quot;) }initializeSparkdef initializeSpark() { intp.beQuietDuring { //创建SparkContex，也就是创建spark运行环境，以及下面的SparkSql运行环境 command(&quot;&quot;&quot; @transient val sc = { val _sc = org.apache.spark.repl.Main.interp.createSparkContext() println(&quot;Spark context available as sc.&quot;) _sc } &quot;&quot;&quot;) command(&quot;&quot;&quot; @transient val sqlContext = { val _sqlContext = org.apache.spark.repl.Main.interp.createSQLContext() println(&quot;SQL context available as sqlContext.&quot;) _sqlContext } &quot;&quot;&quot;) command(&quot;import org.apache.spark.SparkContext._&quot;) command(&quot;import sqlContext.implicits._&quot;) command(&quot;import sqlContext.sql&quot;) command(&quot;import org.apache.spark.sql.functions._&quot;) } }createSparkContext//初始化SparkContex，初始化createSQLContext就不贴了def createSparkContext(): SparkContext = { val execUri = System.getenv(&quot;SPARK_EXECUTOR_URI&quot;) val jars = SparkILoop.getAddedJars val conf = new SparkConf() .setMaster(getMaster()) .setJars(jars) .set(&quot;spark.repl.class.uri&quot;, intp.classServerUri) .setIfMissing(&quot;spark.app.name&quot;, &quot;Spark shell&quot;) if (execUri != null) { conf.set(&quot;spark.executor.uri&quot;, execUri) } sparkContext = new SparkContext(conf) logInfo(&quot;Created spark context..&quot;) sparkContext }" }, { "title": "Spark内部原理", "url": "/posts/Spark%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86/", "categories": "大数据框架, spark", "tags": "spark", "date": "2018-12-26 06:43:50 +0800", "snippet": "1.Shuffle1.1 什么是ShuffleSpark是分布式计算系统，数据块在不同节点执行，但是一些操作，例如join，需要将不同节点上相同的Key对应的Value聚集到一起，Shuffle便应运而生。Shuffle是连接map和reduce之间的桥梁，它将map的输出对应到reduce输入中，这期间涉及到序列化反序列化、跨节点网络IO以及磁盘读写IO等，所以说Shuffle是整个应用程序运行过程中非常昂贵的一个阶段，理解Spark Shuffle原理有助于优化Spark应用程序。1.2 Spark Shuffle的基本原理与特性与MapReduce 中的Shuffle类似 在DAG阶段以shuffle为界，划分Stage，上游为 map task，下游为reduce task 每个map task将计算结果数据分成多份，每一份对应到下游stage的每个partition中，并将其临时写到磁盘，该过程叫做shuffle write 每个reduce task通过网络拉取上游stage中所有map task的指定分区结果数据，该过程叫做shuffle read 在map阶段，除了map的业务逻辑外，还有shuffle write的过程，这个过程涉及到序列化、磁盘IO等耗时操作；在reduce阶段，除了reduce的业务逻辑外，还有前面shuffle read过程，这个过程涉及到网络IO、反序列化等耗时操作，所以整个shuffle过程是极其昂贵的1.4 Spark Shuffle实现演进1.4.1 Hash Shuffle v1和上述流程类似，假如一个executor上运行 M 个map task，下游reduce 有 N 个分区，则executor 会生成M*N个临时文件，生成文件时需要申请文件描述符，当partition很多时，并行化运行task时可能会耗尽文件描述符、消耗大量内存。因此后来Hash Shuffle进一步变成了如下版本。1.4.2 Sort Shuffle 在map阶段，会先按照partition id、每个partition内部按照key对中间结果进行排序。所有的partition数据写在一个文件里，并且通过一个索引文件记录每个partition的大小和偏移量。这样并行运行时每个core只要2个文件，一个executor上最多2m个文件。。 在reduce阶段，reduce task拉取数据做combine时不再使用HashMap而是ExternalAppendOnlyMap。如果内存不足会写次磁盘。但是排序会导致性能损失。1.4.3 Unsafe Shuffle从spark 1.5.0开始，spark开始了钨丝计划(Tungsten)，目的是优化内存和CPU的使用，进一步提升spark的性能。为此，引入Unsafe Shuffle，它的做法是将数据记录用二进制的方式存储，直接在序列化的二进制数据上sort而不是在java 对象上，这样一方面可以减少memory的使用和GC的开销，另一方面避免shuffle过程中频繁的序列化以及反序列化。在排序过程中，它提供cache-efficient sorter，使用一个8 bytes的指针，把排序转化成了一个指针数组的排序，极大的优化了排序性能。但是使用Unsafe Shuffle有几个限制，shuffle阶段不能有aggregate操作，分区数不能超过一定大小( 2^{24} -1，这是可编码的最大parition id)，所以像reduceByKey这类有aggregate操作的算子是不能使用Unsafe Shuffle，它会退化采用Sort Shuffle。从spark-1.6.0开始，把Sort Shuffle和Unsafe Shuffle全部统一到Sort Shuffle中，如果检测到满足Unsafe Shuffle条件会自动采用Unsafe Shuffle，否则采用Sort Shuffle。从spark-2.0.0开始，spark把Hash Shuffle移除，可以说目前spark-2.0中只有一种Shuffle，即为Sort Shuffle。2. 宽依赖&amp;amp;&amp;amp;窄依赖2.1 RDD LineagesRDD也是一个DAG的任务集合，一个DAG代表了一个RDD的计算过程。每个DAG都会记住创建该数据集需要哪些操作，跟踪记录RDD的继承关系，这个关系在Spark中叫做Lineages。2.2 宽依赖&amp;amp;&amp;amp;窄依赖 窄依赖：父分区对应一个子分区。对于窄依赖，只需通过重新计算丢失的那一块数据来恢复，容错成本较小。 宽依赖：分区对应多个子分区 。对于宽依赖，会对父分区进行重新计算，造成冗余计算。 B -&amp;gt;G 中的join是窄依赖，因为之前的groupby已经将B中的数据通过shuffle进行了分区 所以join操作已有窄依赖已有宽依赖如何判断是宽依赖还是窄依赖每个RDD对象都有一个dependencies，通过获取这个属性，可以判断他是宽依赖还是窄依赖宽依赖： ShuffleDependency窄依赖： OneToOneDependency PruneDependency RangeDependency也可以通过 toDebugString 属性，查看整个RDD Lineages2.3 RDD容错当出现数据丢失时，会通过RDD之间的血缘关系（Lineages）进行重新计算，但是如果错误发生在一个复杂的宽依赖的时候，重新计算任然会消耗掉很多资源。2.4 缓存如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。当然缓存也有缓存到内存或者是硬盘上，默认情况下是缓存到内存更多的缓存方式请点这里3. 共享变量 在Spark执行时，每个task之前无法进行数据交换的，但是有时却需要统计一些公共的值，譬如计数之类的，该怎么告呢？ 这时候就要用到Spark 中的共享变量了。Spark中一共有两个共享变量：Broadcast Variables、Accumulators Broadcast Variables 广播变量是一个只读变量，存放后，在集群中任何节点都可以访问 Accumulators 累加器，功能和名字差不多，可以在并行的情况下高效的进行累加 参考 Spark Shuffle 原理 Spark Shuffle原理及相关调优 官方文档" }, { "title": "Spark入门", "url": "/posts/Spark%E5%85%A5%E9%97%A8/", "categories": "大数据框架, spark", "tags": "spark", "date": "2018-12-24 08:10:05 +0800", "snippet": "Spark入门1.什么是Sark Apache Spark是一个开源集群运算框架。 相对于Hadoop的MapReduce会在运行完工作后将中介数据存放到磁盘中，Spark使用了存储器内运算技术，能在数据尚未写入硬盘时即在存储器内分析运算。Spark在存储器内运行程序的运算速度能做到比Hadoop MapReduce的运算速度快上100倍，即便是运行程序于硬盘时，Spark也能快上10倍速度。 Spark允许用户将数据加载至集群存储器，并多次对其进行查询，非常适合用于机器学习算法。2. Spark部件 Driver Program：一个独立的进程，主要是做一些job的初始化工作，包括job的解析，DAG的构建和划分并提交和监控task Cluster Manager：一个进程，用于负责整个集群的资源调度、分配、监控等职责 Work Node：负责管理本节点的资源，定期向Master汇报心跳，接收Master的命令 Executor：启动多个Task执行RDD操作，Task用于执行RDD操作 Cache：用于缓存数据 具体流程：3. RDD&amp;amp;&amp;amp;RDD操作3.1 什么是RDD 弹性分布式数据集（Resilient Distributed Datasets ，RDDs）是一个可以并行操作的容错元素集合，由多个Partition组成3.2 RDD怎么创建RDD一共有两个创建方式： 并行化（parallelize）一个程序中现有的集合 引用一个外部数据集（HDFS, HBase, or any data source offering a Hadoop InputFormat）//并行化一个现有集合val data = Array(1, 2, 3, 4, 5)val distData = sc.parallelize(data)//从HDFS文件中读取一个文件val sc = new SparkContext(conf)val f = sc.textFile(&quot;hdfs://root/user&quot;)f: RDD[String] = MappedRDD@1d4cee083.3 RDD常用操作Transformation：进行数据的转换，即将一个RDD转换成另一个RDD，这类转换并不触发提交作业，完成作业中间过程处理。 map：将集合中的每个对象进行遍历操作，传入的匿名函数即为遍历的每个元素的操作 filter：传入你个返回为Boolean的匿名函数，返回 返回值为True的对象 flatMap：将处理返回的迭代类容构建成一个新的RDDlist=[&quot;im am a good man&quot;,&quot;you are a bad girl&quot;]parallelize = sc.parallelize(list)flat_map = parallelize.flatMap(lambda x: x.split(&quot; &quot;))# 输出结果 [&#39;im&#39;, &#39;am&#39;, &#39;a&#39;, &#39;good&#39;, &#39;man&#39;, &#39;you&#39;, &#39;are&#39;, &#39;a&#39;, &#39;bad&#39;, &#39;girl&#39;]# 与map不同的地方在与，map输出结果# [[&#39;im&#39;, &#39;am&#39;, &#39;a&#39;, &#39;good&#39;, &#39;man&#39;],[&#39;you&#39;, &#39;are&#39;, &#39;a&#39;, &#39;bad&#39;, &#39;girl&#39;]] groupByKey：传入的必须是一个键值对（长度为2就完事了），根据键进行分组 注意：在实际使用的时候能使用reduceByKey或者aggregateByKey就用这两个，可以有效减少shufflelist=[(&quot;m&quot;,10),(&quot;m&quot;,20),(&quot;c&quot;,18)]listRDD=sc.parallelize(list).groupByKey()# listRDD：[(&#39;m&#39;, (10,20), (&#39;c&#39;, (18))] reduceByKey：groupByKey+reduce，对传入的键值对进行分组并进行reduce计算 sortByKey：根据键值对的Key进行排序 join：跟SQL中的Join差不多 cogroup：跟join差不多，不过join后返回的是一个可以迭代的对象 union：将两个RDD合并，不去重 intersection：取两个RDD交集，并去重 distinct：去重 aggregateByKey：有点麻烦 参考 aggregateByKeyAction：计算，对RDD数据进行计算，会触发SparkContext提交Job作业。 reduce：通过传入的func函数聚集RDD中所有的元素 val arr=Array(1,2,3,4,5,6) val value = sc.parallelize(arr)val i = value.reduce(_+_) collect：以数组的形式返回所有的元素 count：返回RDD的个数 first：返回RDD的第一个元素 take：取出RDD前N个元素，以数组的形式返回 saveAsTextFile：将RDD保存为一个文件 countByKey：分组计数 参考 Cluster Mode Overview 从Spark组件来看Spark的执行流程" }, { "title": "Scala使用", "url": "/posts/Scala%E4%BD%BF%E7%94%A8/", "categories": "编程语言, scala", "tags": "scala", "date": "2018-12-18 07:52:24 +0800", "snippet": "1.概述Scala是一门主要以Java虚拟机(JVM)为目标运行环境并将面向对象和函数式编程语言的最佳特性综合在一起的编程语言。你可以使用Scala编写出更加精简的程序，同时充分利用并发的威力。由于Scala默认运行于JVM之上，因此 它可以访问任何Java类库并且与Java框架进行互操作，比如Scala可以被编译成JavaScript代码，让我们更便捷、高效地开发Web应用。Scala解释器读到一个表达式，对它进行求值，将它打印出来，接着再继续读下一个表达式。 这个过程被称作“读取-求值-打印”循环(read-­eval-print loop),即REPL。2. Scala基础 Scala有两种变量，val和var，val类似于Java中的final//age可修改、name不可修改var age = 18val name = &quot;Tom&quot; 指定变量类型（默认情况下Scala会自己识别）var age:int = 18var name:String = &quot;Alice&quot;3.基本类型&amp;amp;&amp;amp;操作基础类型 Scala可以兼容Java中的类型，所以字符串类型用的依然是java.lang.String，其他类型均为Scala自己的成员类型转换操作符&amp;amp;&amp;amp;方法1 + 1 =2(1).+(1)=2 在Scala中任何操作符均为函数，即可调用，也可当做操作符使用对象相等由上可知，Scala中所有的操作符均为函数，所以与Java不同的在与，Scala中没有equal函数，全由==，!=代替4.条件判断单行if语句var result=if (100&amp;gt;70) true else falseprint(result)多行if语句if (100&amp;gt;70){ var result = true}else{ var result = false}5. 循环while循环var count = 0while(count &amp;lt; 10){ count += 1}do-whilevar count=0do{count+=1println(count)}while(count&amp;lt;10)for 循环（单行）val arr = Array(1,2,3,4,5)for(obj &amp;lt;- arr) println(&quot;obj:&quot; + obj)for循环（多行）val arr = Array(1,2,3,4,5)for(obj &amp;lt;- arr){ println(&quot;obj:&quot; + obj)}函数式val arr = Array(1,2,3,4,5)//逐步省略，Scala会自动识别arr.foreach((x:Int) =&amp;gt; println(x))//省略类型arr.foreach(x =&amp;gt; println(x))//省略参数arr.foreach(println(_))//省略括号arr.foreach(println)Range//输出 1-10for(i&amp;lt;- 0 to 10) println i//输出 1-9for(i&amp;lt;- 0 until 10) println iFilter// 输出 偶数for(i&amp;lt;- 0 to 10 if i % 2 == 0) println(i)yield// 相当于Python中的生成器val result = for(x&amp;lt;- 0 to 10) yield x*2result.foreach(println)lazy// 懒加载，只有在file被使用时才回去加载资源lazy val file =new File(&quot;demo.txt&quot;)file.mkString匹配表达式//相当于Java中的Switchval choice = &quot;1&quot;choice match{ case &quot;1&quot; =&amp;gt; println(&quot;选择的是 ：1&quot;) case &quot;2&quot; =&amp;gt; println(&quot;选择的是 ：2&quot;) case _ =&amp;gt; println(&quot;选择的是 ：其他&quot;)}6. 函数Unit函数// Unit 相当于Java 中的Voiddef main(args: Array[String]): Unit = { println(&quot;Hello World&quot;)}单行函数def demo1(id:Int):String = &quot;my id:&quot;+id.toString头等函数（匿名函数|单行）val dou = (x:Int) =&amp;gt; x*xprintln(dou(5))头等函数 （多行）val mul = (x:Int,y:Int) =&amp;gt;{ x*y}println(mul(2,3))占位符用法// 在一个方法中包含另一个方法，里面的方法如果参数只是用一次，则可以用`_`来代替val arr = Array(1,2,3,4,5,6)val mul = (x:Int,y:Int) =&amp;gt;{ println(x,y) x*y}println(arr.reduce(mul(_,_)))变长参数def demo(id:Int,name:String,others:String*): Unit = { others.foreach(println)}7. 类&amp;amp;&amp;amp;对象辅助构造器&amp;amp;主构造器// 主构造器class User(ids:Int) { private var id:Int= ids private var name:String= _ //辅助构造器 def this(id:Int,name:String){ // 当存在主构造器时，辅助构造器可以配合主构造器一起使用 this(id) this.name=name } override def toString: String = { &quot;id:&quot;+id + &quot;\\t name:&quot;+name }}方法重写override def toString: String = { name}先决条件class User(ids:Int) { private var id:Int= ids private var name:String= _ //先决条件，可以在参数设置之前进行判断 require(id&amp;gt;18) def this(id:Int,name:String){ this(id) this.name=name }}继承&amp;amp;&amp;amp;特质 继承和Java差不多，都是单继承，但是可以多实现，在scala里面没有接口，只有特质（Trait） 特质的定义除了使用trait关键字之外，与类无异Object对象 在Java或C++中，通常会用到既有实例方法又有静态方法的类。在Scala中，可以通过类和与类同名的伴生对象来达到同样的目的。 类和它的伴生对象可以相互访问私有特性，但必须存在同一个源文件中class User(ids:Int) { private var id:Int= ids private var name:String= &quot;Tim&quot;}object User{ private var num:Int =0 def getNum():Int={ num+=1 num }}数组操作定长数组//Array 使用（）访问,而不是[]//创建并初始化var arr = Array(1,2,3,4,5)//初始化数组var arr = Array[String](10)变长数组import scala.collection.mutable.ArrayBuffer//变长数组 跟Java ArrayList差不多val arr = ArrayBuffer[Int](10)arr+=10映射（Map）// 构建 映射val m = Map(&quot;a&quot;-&amp;gt;10,&quot;b&quot;-&amp;gt;12)var userList:Map[String,String] = Map()// 添加新的键值对userList+=(&quot;a&quot;-&amp;gt; &quot;123&quot;)userList(&quot;b&quot;)=&quot;333&quot;// 获取对应值println(m.get(&quot;a&quot;))//便利 映射m.keys.foreach(println)m.values.foreach(println)m.foreach(println)元组元组是不同类型值的集合，和Python一样不可修改//初始化元组，不指定元组个数，Scala会自动识别val t1=(1,2,3)val t2=Tuple3(2,3,4)//输出println(t1._1,t1._2,t1._3)样例类 快速建立一些简单的类object Objects { case class Message(Content:String,Sender:Actor) case class User(userName:String,password:String) case class Message1(countent:String,flag:Boolean)}泛型class Pair[K,V](val k:K,val v:V){ var key=k var value=v}隐式转换略…Actor并发编程 Scala中的Actor是一种不共享数据，依赖于消息传递的一种并发编程模式，避免了死锁、资源争夺的情况。 Scala中的Actor会不断循环自己的邮箱，并通过receive偏函数进行消息的模式匹配并进行相应的处理。如果Actor A与Actor B需要相互通信，首先A要给B发送一个消息，B会有一个收件箱，然后B会不断循环自己的收件箱，若看见A发送的消息，B就会解析A的消息并执行，处理完后将结果以邮件的方式发送给A。一个简单的Actor Demo// 使用Actor只需要继承一个Actor类，并实现act()方法即可class ActorDemo extends Actor{ override def act(): Unit = { receive{ case msg:String =&amp;gt; println(&quot;Message: &quot;+ msg) } }}//主类调用 val a1=new ActorDemoa1.starta1 ! &quot;Hello？&quot;两个Actor之间的交互//只需要将需要交互的Actor在发送信息的时候带上自己即可class ActorDemo1(name:String) extends Actor{ override def act(): Unit = { while (true){ receive{ case Message(cont,send)=&amp;gt;{ println(name+&quot; receive message :&quot;+ cont) val str = readLine(name+&quot; send message : &quot;) send ! Message(str,this) } } } }}//主类调用val a2 = new ActorDemo1(&quot;小王&quot;)val a3 = new ActorDemo1(&quot;小张&quot;)" }, { "title": "Hive概念以及架构介绍", "url": "/posts/Hive%E6%A6%82%E5%BF%B5%E4%BB%A5%E5%8F%8A%E6%9E%B6%E6%9E%84%E4%BB%8B%E7%BB%8D/", "categories": "大数据框架, hive", "tags": "hive", "date": "2018-12-16 23:56:41 +0800", "snippet": "Hive概念以及架构0. 目录 什么是Hive Hive 体系介绍 Hive 执行任务的流程1. 什么是Hive Hive是Hadoop工具家族中一个重要成员，可以将结构化的数据文件（HDFS）映射为一张数据库表。 Hive 定义了简单的类 SQL 查询语言，被称为 HQL，实现方便高效的数据查询 Hive的本质是将HQL，转换成MapReduce任务，完成整个的数据的ETL，减少编写MapReduce的复杂度2.Hive体系介绍Hive架构包括如下组件：CLI（command line interface）、JDBC/ODBC、Thrift Server、Hive WEB Interface（HWI）、Metastore和Driver（Complier、Optimizer和Executor） Driver：核心组件。整个Hive的核心，该组件包括Complier、Optimizer和Executor，它的作用是将我们写的HQL语句进行解析、编译优化，生成执行计划，然后调用底层的MapReduce计算框架。 Metastore： 元数据服务组件。这个组件存储Hive元数据，放在关系型数据库中，支持derby、mysql。 ThriftServers：提供JDBC和ODBC接入的能力,它用来进行可扩展且跨语言的服务的开发，hive集成了该服务，能让不同的编程语言调用hive的接口。 CLI：command line interface，命令行接口 Hive WEB Interface（HWI）：hive客户端提供了一种通过网页的方式访问hive所提供的服务。这个接口对应hive的hwi组件（hive web interface）3. Hive执行流程 UI调用Drive的execute接口（1） Drive创建一个查询的Session事件并发送这个查询到Compiler，Compiler收到Session生成执行计划（2） Compiler从MetaStore中获取一些必要的数据（3，4） 在整个Plan Tree中，MetaStore用于查询表达式的类型检查，以及根据查询谓语（query predicates）精简partitions 。该Plan由Compiler生成，是一个DAG（Directed acyclic graph，有向无环图）执行步骤，里面的步骤包括map/reduce job、metadata 操作、HDFS上的操作，对于map/reduce job，里面包含map operator trees和一个reduce operator tree（5） 提交执行计划到Excution Engine，并由Execution Engine将各个阶段提交个适当的组件执行（6，6.1，6.2 ， 6.3） 在每个任务（mapper / reducer）中，表或者中间输出相关的反序列化器从HDFS读取行，并通过相关的操作树进行传递。一旦这些输出产生，将通过序列化器生成零时的的HDFS文件（这个只发生在只有Map没有reduce的情况），生成的HDFS零时文件用于执行计划后续的Map/Reduce阶段。对于DML操作，零时文件最终移动到表的位置。该方案确保不出现脏数据读取（文件重命名是HDFS中的原子操作），对于查询，临时文件的内容由Execution Engine直接从HDFS读取，作为从Driver Fetch API的一部分（7，8，9）" }, { "title": "Sqoop 常用操作", "url": "/posts/Sqoop%E6%93%8D%E4%BD%9C/", "categories": "大数据框架, sqoop", "tags": "sqoop", "date": "2018-12-13 00:00:00 +0800", "snippet": "Sqoop常用命令1.预备环境 Hadoop Zookeeper MySql Hive HBaseor CDH2. import简介该工具可以将单个关系型数据库的表导入到HDFS上常用参数 参数名称 功能 --connect &amp;lt;jdbc-uri&amp;gt; jdbc链接(例：jdbc:mysql://host_name/table_name) --help 帮助 --password &amp;lt;password&amp;gt; 密码 --username &amp;lt;username&amp;gt; 账号 --direct import工具将会使用JDBC提供的高性能工具例如MySql中的mysqldump） --fields-terminated-by &amp;lt;char&amp;gt; 设置导出内容不同列的分隔符（默认 ‘,’） --delete-target-dir 如果文件已经存在则删除 --target-dir &amp;lt;dir&amp;gt; 指定目录输出 last-value 上次导出的主键（增量导出时使用） sqoop import --connect jdbc:mysql://nhadoop1/test_user \\--username root \\--password 123456 \\--table user \\--direct \\--num-mappers 1 \\--fields-terminated-by ,\\--delete-target-dir导入到Hive 参数 功能 --hive-import 指定导入到hive --hive-database &amp;lt;database-name&amp;gt; 指定导入到数据库 --hive-table &amp;lt;table-name&amp;gt; 指定导入的表 sqoop import --connect jdbc:mysql://nhadoop1/test_user \\--username root \\--password 123456 \\--table user \\--direct \\--num-mappers 1 \\--fields-terminated-by , \\--delete-target-dir--hive-import \\--hive-database database-name \\--hive-table table-name原生环境可能存在的坑 java.lang.ClassNotFoundExceptionSqoop 的lib中缺少Hive 的jar包，从Hive 中找的缺少的jar包到Sqoop中即可 缺少配置文件拷贝hive/conf/hive-site.xml到sqoop/conf下export 参数 功能` --export-dir 指定输出目录 -- input-fields-terminated-by &amp;lt;char&amp;gt; 指定文件每行的分隔符 --update-key 使用update进行输出（默认insert），后面接匹配的键（例如 ID） --columns &amp;lt;col,col,col…&amp;gt; 指定输出的栏目名 # 更新操作sqoop export --connect jdbc:mysql://nhadoop1/test_user \\--username root \\--password 123456 \\--table jobs_k \\--columns rowkey,salary \\--export-dir /user/hive/warehouse/test.db/hbase_map_p \\--input-fields-terminated-by &#39;\\001&#39; \\--update-key rowkey补充1. 执行文件中的命令# 原语句$ sqoop import --connect jdbc:mysql://localhost/db --username foo --table TEST# 执行文件中的语句$ sqoop --options-file /users/homer/work/import.txt --table TEST# 文件中的内容import--connectjdbc:mysql://localhost/db--usernamefoo" }, { "title": "Hadoop协同框架-Flume", "url": "/posts/Hadoop%E5%8D%8F%E5%90%8C%E6%A1%86%E6%9E%B6-Flume/", "categories": "大数据框架, flume", "tags": "flume", "date": "2018-12-13 00:00:00 +0800", "snippet": "Flume结构 Source : 用户配置采集数据的方式（Http、LocalFileSystem、Tcp） Channel ——中间件 Memory Channel：临时存放到内存 FIle Channel ：临时存放到本地磁盘 Sink ：将数据存放目的地（HDFS、本地文件系统、Logger、Http）常用配置# 每个组件的名称a1.sources = r1a1.sinks = k1a1.channels = c1# netcat监控方式、监控的ip：localhost、端口：44444a1.sources.r1.type = netcata1.sources.r1.bind = localhosta1.sources.r1.port = 44444# sink 的方式 loggera1.sinks.k1.type = logger# 写入到内存、a1.channels.c1.type = memory# 绑定source和sink到channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1SourceExec source 用于监控Linux命令a1.sources = r1a1.channels = c1# 指定类型、命令a1.sources.r1.type = execa1.sources.r1.command = tail -F /var/log/securea1.sources.r1.channels = c1Exec Source详细参数Spooling Directory Source 用于监控文件，比Exec监控更加可靠a1.channels = ch-1a1.sources = src-1fs.sources.r3.type=spooldirfs.sources.r3.spoolDir=/opt/modules/apache-flume-1.6.0-bin/flume_templatefs.sources.r3.fileHeader=truefs.sources.r3.ignorePattern=^(.)*\\\\.out$ # 过滤out结尾的文件Spooling Directory Source 详细参数ChannelMemory Channel 中间文件存放在内存中a1.channels = c1a1.channels.c1.type = memorya1.channels.c1.capacity = 10000a1.channels.c1.transactionCapacity = 10000a1.channels.c1.byteCapacityBufferPercentage = 20a1.channels.c1.byteCapacity = 800000Memory Channel 详细参数File Channel 中间文件存放在文件中a1.channels = c1a1.channels.c1.type = filea1.channels.c1.checkpointDir = /mnt/flume/checkpointa1.channels.c1.dataDirs = /mnt/flume/dataFile Channel 详细参数SinkLogger Sink 在INFO级别记录文件，通常用于调试a1.channels = c1a1.sinks = k1a1.sinks.k1.type = loggera1.sinks.k1.channel = c1Logger Sink 详细参数HDFS Sink 记录文件写入到HDFS中a1.channels = c1a1.sinks = k1a1.sinks.k1.type = hdfsa1.sinks.k1.channel = c1a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/%Sa1.sinks.k1.hdfs.filePrefix = events-a1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 10a1.sinks.k1.hdfs.roundUnit = minute详细参数" }, { "title": "Hive与Hbase之间的区别与关系", "url": "/posts/Hive%E4%B8%8EHbase%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E5%85%B3%E7%B3%BB/", "categories": "大数据框架", "tags": "hbase, hive", "date": "2018-11-17 00:00:00 +0800", "snippet": "1. 区别 Hbase：Hadoop database，也就是基于Hadoop的数据库，是一种NoSQL的数据库，主要用于海量数据的实时随机查询，例如：日志明细，交易清单等。 Hive： Hive是hadoop的数据仓库，跟数据库有点差，主要是通过SQL语句对HDFS上结构化的数据进行计算和处理，适用于离线批量数据处理 通过元数据对HDFS上的数据文件进行描述，也就是通过定义一张表来描述HDFS上的结构化文本，包括各列的数据名称、数据类型，方便数据的处理 基于上面一点，通过SQL来处理和计算HDFS的数据，Hive会将SQL翻译为Mapreduce来处理数据 2. 关系在大数据架构中，通常HBase和Hive是协作关系： 通过ETL（Extract-Transform-Load，提取、转换、加载）工具将数据源抽取到HDFS上存储 通过Hive清洗、处理和计算源数据 如果清洗过后的数据是用于海量数据的随机查询，则可将数据放入Hbase 数据应用从Hbase中查询数据参考 Hive和Hbase之间的差异？" }, { "title": "HBase设计结构和原理", "url": "/posts/HBase%E5%8E%9F%E7%90%86/", "categories": "大数据框架, hbase", "tags": "hbase", "date": "2018-11-14 00:00:00 +0800", "snippet": "1. 数据模型1.1 数据模型相关概念 表：HBase采用表来组织数据，表由行和列组成，列划分为若干个列族。 行：每个HBase表都由若干行组成，每个行由行键（row key）来标识。 列族：一个HBase表被分组成许多“列族”（Column Family）的集合，它是基本的访问控制单元。 列限定符：列族里的数据通过列限定符（或列）来定位。 单元格：在HBase表中，通过行、列族和列限定符确定一个“单元格”（cell），单元格中存储的数据没有数据类型，总被视为字节数组byte[]。 时间戳：每个单元格都保存着同一份数据的多个版本，这些版本采用时间戳进行索引。1.2 数据坐标HBase中需要根据行键、列族、列限定符和时间戳来确定一个单元格2. HBase 系统架构2.1 HBase功能组件 Client 包含访问HBase的接口，同时在缓存中维护着已经访问过的Region位置信息，用来加快后续数据访问过程 通过与Zookeeper通信在获得Region的存储位置信息后，直接从Region Server上读取数据 与Hmaster通信进行管理类操作 一个Master Server 维护元数据信息 在Region分裂或合并后，负责重新调整Region的分布。 对发生故障失效的Region服务器上的Region进行迁移。 多个Region Server 负责存储和维护分配给自己的Region，处理来自客户端的读写请求 Region由RegionServer管理。所有用户数据的读写请求，都是和RegionServer上的Region进行交互 Region可以在RegionServer之间迁移 Zookeeper服务器 选举出一个Master作为集群的总管，并保证在任何时刻总有唯一一个Master在运行，这就避免了Master的“单点失效”问题 获得Region位置信息返回给客户端，大多数客户端甚至从来不和Master通信，这种设计方式使得Master负载很小。 2.2 Region Server 概念 Region ：用于存放数据信息 Store：一个Region由一个或多个Store组成。每个Store对应图中的一个Column Family。 MemStore：一个Store包含一个MemStore，客户端向Region插入的数据缓存到MemStore。 StoreFile：MemStore的数据Flush到HDFS后成为StoreFile HFile：HBase中keyvalue数据的存储格式，HFile是Hadoop二进制格式文件，实际上是storefile对hfile做了轻量级包装 HLog：HBase中WAL的存储格式，物理上是Hadoop的sequence file。保证了当RegionServer故障的情况下用户写入的数据不丢失。RegionServer的即可以多个Region共享一个相同的Hlog，也可以每个Region拥有一个Hlog。2.2.1 Store 工作原理 Store是Region服务器的核心。 storefile的数量增长到一定的阈值会触发compact合并操作 单个StoreFile过大时，即当前达到Region设置的阈值，会触发split操作，即把当前的region分成2个region HBase只是增加数据，更新和删除操作都是在compact阶段做 是为了减少同一个Region中的同一个ColumnFamily下面的小文件（HFile）数目，从而提升读取的性能2.2.2 Hlog工作原理 分布式环境必须要考虑系统出错。HBase采用HLog保证系统恢复。 HBase系统为每个Region服务器配置了一个HLog文件，它是一种预写式日志（Write Ahead Log，WAL） 用户更新数据先写入MemStore缓存再写入日志 出现故障 Zookeeper会实时监测每个Region服务器的状态，当某个Region服务器发生故障时，Zookeeper会通知Master Master首先会处理该故障Region服务器上面遗留的HLog文件，这个遗留的HLog文件中包含了来自多个Region对象的日志记录 系统会根据每条日志记录所属的Region对象对HLog数据进行拆分，分别放到相应Region对象的目录下，然后，再将失效的Region重新分配到可用的Region Server中，并把与该Region对象相关的HLog日志记录也发送给相应的Region Server Region Servrt获得分配给自己的Region对象以及与之相关的HLog日志记录以后，会重新做一遍日志记录中的各种操作，把日志记录中的数据写入到MemStore缓存中，然后，刷新到磁盘的StoreFile文件中，完成数据恢复。 2.3 写入操作 Client通过访问ZK来获取到HBase:meta.的地址信息 ZK通过访问HBase:meta.表来获取具体的RS返回给Client Client端获取到目标地址（RegionServer、Region、Rowkey）后，然后直接向该地址发送数据请求。 向指定RS的对应region进行数据写入 获取Region操作锁。（读写锁） 一次获取各行行锁。 写入到MemStore中。（一个内存排序集合） 释放以获取的行锁。 写数据到WAL中。（Write-Ahead-Log） 释放Region锁。 既然是Write-Ahead-Log，为何先写内存再写WAL？ 先写内存的原因：HBase提供了一个MVCC机制，来保障些数据阶段的数据可见性。先写MemStore再写WAL，是为了一些特殊场景下，内存中的数据能够更及时的返回。如果先写WAL失败的话，MemStore助攻的数据会被回滚。 Flush 达到Region设置MemStore的阈值 MemStore占用内存的总量和RegionServer总内存的比值超出来了预设的阈值大小 HBase定期刷新MemStore WALs中文件数量达到阈值 通过shell命令分别对一个表或者一个Region进行Flush Compaction Compaction分为Minor、Major两类： Minor合并：多个小文件重写为数量较少的大文件。有最少和最大的数量限制，通常会选择一些连续时间范围内的小文件进行合并，受磁盘IO的影响 Major合并：将Region中的一个列族的所有hfile重写为一个新的hfile，过程如下图 扫描所有的Row Key，顺序重写全部数据 重写数据的过程中可能删除掉标记过的数据和超出版本号的数据，删不删除根据实际情景而定 Region Split Region的大小超出了预设的阈值，则需要将该Region自动分裂成为两个Region 分裂过程中，被分裂的Region会暂停读写服务。父Region的数据文件并不会真正的分裂，而是仅仅通过更改引用方式，来实现快速分裂，即通过新的访问方式访问源文件，HBase后台会自己进行分裂操作 客户端册所缓存的父Region的路由信息需要被更新 2.4 读操作 客户端发起请求 Get操作在精准的Key值的情形下，读取单行用户数据。 Sacn操作时为了批量扫描限定KEy值范围的用户数据。 Scanner定位Region(读取顺序) memstore blockacache(每个Region Server只有一个) hfile 2.5 Region 概念 将一个表的数据按RowKey的范围划分为一个或多个子表，实现分布式存储 每个子表在Region Server中被称为Region 每一个Region都关联一个Key值范围，即一个使用StartKey和EndKey描述的区间，其实只需要记录StarKey就完事了 Region是HBase分布式存储的最基本单元 Region分为元数据Region以及用户Region两类 Meta Region记录了每一个User Region的路由信息 补充：hbase:meta表相关介绍1. 基本介绍 HBase 0.96 以前 -root- 表位置信息存放在Zookeeper上，内容为meta的存放信息 .meta存放在regionserver上，存储用户表的region信息。 HBase 0.96以后 移除root表，用hbase:meta 代表 .meta 表，hbase:meta表的存放信息直接存放在zookeeper的/hbase/meta-region-server上 2. 表内容2.1 hbase:meta 表结构 存储的是用户的表的region信息Key：Region key的格式是：[table],[region start key],[region id]Value:info:regioninfo: 序列化的当前region的HRegionInfo实例。info:server：存储这个region的regionserver的server:portinfo:serverstartcode:该Regionserver拥用该region的起始时间2.2 数据访问流程 0.9.6以前的版本Client———&amp;gt;zookeeper———&amp;gt;-ROOT-(单region)—–&amp;gt;.META.————-&amp;gt;用户表region 0.9.6及以后的版本Client———&amp;gt;zookeeper——–&amp;gt;hbase:meta———&amp;gt;用户表regionClient的会从Zookeeper找到hbase:meta的位置，然后扫描该表找到我们需要请求的region信息，直接跟存储该region的regionserver建立连接，请求数据，而不是经由master。这些信息会被客户端缓存，避免多次请求3. Shell操作Shell 命令操作参考博客 HBase原理和设计 HBase技术原理 4. 预分区默认情况下，创建一个表，hbase会为其自动分区，即Region Server会不断工作，导致Region Server负载过大，所以比较好的办法是根据业务提前对表进行分区，例如有5个region被多个Region Server管理，在插入数据的时候，会向5个region中分别插入，负载均衡创建分区的方法： hbase&amp;gt; create ‘ns1:t1’, ‘f1’, SPLITS =&amp;gt; [‘10’, ‘20’, ‘30’, ‘40’]​ hbase&amp;gt; create ‘t1’, ‘f1’, SPLITS =&amp;gt; [‘10’, ‘20’, ‘30’, ‘40’]​ hbase&amp;gt; create ‘t1’, ‘f1’, SPLITS_FILE =&amp;gt; ‘splits.txt’, OWNER =&amp;gt; ‘johndoe’​ hbase&amp;gt; create ‘t1’, {NAME =&amp;gt; ‘f1’, VERSIONS =&amp;gt; 5}, METADATA =&amp;gt; { ‘mykey’ =&amp;gt; ‘myvalue’ }​ hbase&amp;gt; # Optionally pre-split the table into NUMREGIONS, using​ hbase&amp;gt; # SPLITALGO (“HexStringSplit”, “UniformSplit” or classname)​ hbase&amp;gt; create ‘t1’, ‘f1’, {NUMREGIONS =&amp;gt; 15, SPLITALGO =&amp;gt; ‘HexStringSplit’}​ hbase&amp;gt; create ‘t1’, ‘f1’, {NUMREGIONS =&amp;gt; 15, SPLITALGO =&amp;gt; ‘HexStringSplit’, REGION_REPLICATION =&amp;gt; 2, CONFIGURATION =&amp;gt; {‘hbase.hregion.scan.loadColumnFamiliesOnDemand’ =&amp;gt; ‘true’}}​ hbase&amp;gt; create ‘t1’, {NAME =&amp;gt; ‘f1’, DFS_REPLICATION =&amp;gt; 1}​ 如：​ 1.create ‘logs’,’info’,SPLITS =&amp;gt; [‘20181010’,’20181020’,’20181030’]​ 2.指定分隔文件​ create ‘logs’,’info’,SPLITS =&amp;gt; ‘opt/datas/logs_split.txt’​ 3.指定多少分区，十六进制字符串分割​ create ‘t1’, ‘f1’, {NUMREGIONS =&amp;gt; 3, SPLITALGO =&amp;gt; ‘HexStringSplit’}" }, { "title": "Zooker选举算法", "url": "/posts/Zooker%E9%80%89%E4%B8%BE%E7%AE%97%E6%B3%95/", "categories": "大数据框架, zookeeper", "tags": "hadoop, zookeeper", "date": "2018-11-13 00:00:00 +0800", "snippet": "1. Leader选举算法可通过electionAlg配置项设置Zookeeper用于领导选举的算法。到3.4.10版本为止，可选项有 0 基于UDP的LeaderElection 1 基于UDP的FastLeaderElection 2 基于UDP和认证的FastLeaderElection 3 基于TCP的FastLeaderElection在3.4.10版本中，默认值为3，也即基于TCP的FastLeaderElection。另外三种算法已经被弃用，并且有计划在之后的版本中将它们彻底删除而不再支持。2. FastLeaderElection2.1 服务器状态 LOOKING 不确定Leader状态。该状态下的服务器认为当前集群中没有Leader，会发起Leader选举 FOLLOWING 跟随者状态。表明当前服务器角色是Follower，并且它知道Leader是谁 LEADING 领导者状态。表明当前服务器角色是Leader，它会维护与Follower间的心跳 OBSERVING 观察者状态。表明当前服务器角色是Observer，与Folower唯一的不同在于不参与选举，也不参与集群写操作时的投票2.2 选票数据结构每个服务器在进行领导选举时，会发送如下关键信息 logicClock 每个服务器会维护一个自增的整数，名为logicClock，它表示这是该服务器发起的第多少轮投票 state 当前服务器的状态 self_id 当前服务器的myid self_zxid 当前服务器上所保存的数据的最大zxid vote_id 被推举的服务器的myid vote_zxid 被推举的服务器上所保存的数据的最大zxid2.3 投票流程自增选举轮次Zookeeper规定所有有效的投票都必须在同一轮次中。每个服务器在开始新一轮投票时，会先对自己维护的logicClock进行自增操作。初始化选票每个服务器在广播自己的选票前，会将自己的投票箱清空。该投票箱记录了所收到的选票。例：服务器2投票给服务器3，服务器3投票给服务器1，则服务器1的投票箱为(2, 3), (3, 1), (1, 1)。票箱中只会记录每一投票者的最后一票，如投票者更新自己的选票，则其它服务器收到该新选票后会在自己票箱中更新该服务器的选票。发送初始化选票每个服务器最开始都是通过广播把票投给自己。接收外部投票服务器会尝试从其它服务器获取投票，并记入自己的投票箱内。如果无法获取任何外部投票，则会确认自己是否与集群中其它服务器保持着有效连接。如果是，则再次发送自己的投票；如果否，则马上与之建立连接。判断选举轮次收到外部投票后，首先会根据投票信息中所包含的logicClock来进行不同处理 外部投票的logicClock大于自己的logicClock。说明该服务器的选举轮次落后于其它服务器的选举轮次，立即清空自己的投票箱并将自己的logicClock更新为收到的logicClock，然后再对比自己之前的投票与收到的投票以确定是否需要变更自己的投票，最终再次将自己的投票广播出去。 外部投票的logicClock小于自己的logicClock。当前服务器直接忽略该投票，继续处理下一个投票。 外部投票的logickClock与自己的相等。当时进行选票PK。选票PK选票PK是基于(self_id, self_zxid)与(vote_id, vote_zxid)的对比 外部投票的logicClock大于自己的logicClock，则将自己的logicClock及自己的选票的logicClock变更为收到的logicClock 若logicClock一致，则对比二者的vote_zxid，若外部投票的vote_zxid比较大，则将自己的票中的vote_zxid与vote_myid更新为收到的票中的vote_zxid与vote_myid并广播出去，另外将收到的票及自己更新后的票放入自己的票箱。如果票箱内已存在(self_myid, self_zxid)相同的选票，则直接覆盖 若二者vote_zxid一致，则比较二者的vote_myid，若外部投票的vote_myid比较大，则将自己的票中的vote_myid更新为收到的票中的vote_myid并广播出去，另外将收到的票及自己更新后的票放入自己的票箱统计选票如果已经确定有过半服务器认可了自己的投票（可能是更新后的投票），则终止投票。否则继续接收其它服务器的投票。更新服务器状态投票终止后，服务器开始更新自身状态。若过半的票投给了自己，则将自己的服务器状态更新为LEADING，否则将自己的状态更新为FOLLOWING3. 几种领导选举场景3.1 集群启动领导选举 集群刚刚启动，所有的LogicClock都为1，zxid为0 服务器初始化以后，都将票投给自己 图上(1, 1, 0)解释， 第一位选票的服务器的logicClock 第二位被推荐的服务器的myid 推荐的服务器的最大的zxid 由于该步骤中所有选票都投给自己，所以第二位的myid即是自己的myid，第三位的zxid即是自己的zxid。 Server1收到Server2的选票（1, 2, 0）（1, 3, 0），由于所有的logicClock都相等，所有的zxid都相等，根据选举原则，应该将自己的选票按照服务器3的选票更新为（1, 3, 0），并将自己的票箱全部清空，再将服务器3的选票与自己的选票存入自己的票箱，接着将自己更新后的选票广播出去。此时服务器1票箱内的选票为(1, 3)，(3, 3)。 Server2收到Server3的选票后也将自己的选票更新为（1, 3, 0）并存入票箱然后广播。此时Server2票箱内的选票为(2, 3)，(3, ,3)。 Server3根据上述规则，无须更新选票，自身的票箱内选票仍为（3, 3） 由于三个服务器最新选票都相同，最后三者的票箱内都包含三张投给服务器3的选票。 三个Server一致认为此时Server3应该是Leader。因此Server1和2都进入FOLLOWING状态，而Server3进入LEADING状态。之后Leader发起并维护与Follower间的心跳。3.2 Follower重启 Follower重启，或者发生网络分区后找不到Leader，会进入LOOKING状态并发起新的一轮投票。 Server3收到Server1的投票后，将自己的状态LEADING以及选票返回给Server1。Server2收到Server1的投票后，将自己的状态FOLLOWING及选票返回给Server1。 Server1知道Server3是Leader，并且通过Server2与Server3的选票可以确定Server3确实得到了超过半数的选票。因此服务器1进入FOLLOWING状态3.3 Leader重启 Leader（服务器3）宕机后，Follower（服务器1和2）发现Leader不工作了，因此进入LOOKING状态并发起新的一轮投票，并且都将票投给自己。 因此进入LOOKING状态并发起新的一轮投票，并且都将票投给自己 服务器1和2根据外部投票确定是否要更新自身的选票。这里有两种情况 服务器1和2的zxid相同。例如在服务器3宕机前服务器1与2完全与之同步。此时选票的更新主要取决于myid的大小 服务器1和2的zxid不同。在旧Leader宕机之前，其所主导的写操作，只需过半服务器确认即可，而不需所有服务器确认。换句话说，服务器1和2可能一个与旧Leader同步（即zxid与之相同）另一个不同步（即zxid比之小）。此时选票的更新主要取决于谁的zxid较大 服务器1的zxid为11，而服务器2的zxid为10，因此服务器2将自身选票更新为（3, 1, 11） 经过上一步选票更新后，Server1与Server12均将选票投给Server11，因此Server12成为Follower，而Server11成为新的Leader并维护与Server2的心跳。 当旧的Leader回复以后，重复Follower的步骤参考文章 技术世界" }, { "title": "Zookeeper 简述", "url": "/posts/Zookeeper-%E7%AE%80%E8%BF%B0/", "categories": "大数据框架, zookeeper", "tags": "hadoop, zookeeper", "date": "2018-11-13 00:00:00 +0800", "snippet": "1. Zookeeper 概述ZooKeeper 是一个典型的分布式数据一致性解决方案，分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。ZooKeeper 一个最常用的使用场景就是用于担任服务生产者和服务消费者的注册中心。2. Zookeeper 的一些重要概念2. 1概念总结 Zookeeper是一个分布式程序 只要半数节点存活，Zookeeper就可以正常运行。每个Server上都保存着一份相同的副本 Zookeeper数据保存在内存中 为了保证高吞吐和低延迟 Zookeeper是高性能的 在“读”多于“写”的情况下，高性能尤为明显。因为写操作会涉及到服务器间同步状态 Zookeeper底层只提供了两个功能 1. 管理（存储、读取）用户提交的数据 2. 为用户监听提交的数据2.2 会话（session）Session 指的是 ZooKeeper 服务器与客户端会话。在 ZooKeeper 中，一个客户端连接是指客户端和服务器之间的一个 TCP 长连接。客户端启动的时候，首先会与服务器建立一个 TCP 连接，从第一次连接建立开始，客户端会话的生命周期也开始了。通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向 Zookeeper 服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的 Watch 事件通知。Session 的 sessionTimeout 值用来设置一个客户端会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在 sessionTimeout 规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效。在为客户端创建会话之前，服务端首先会为每个客户端都分配一个 sessionID。由于 sessionID 是 Zookeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的。因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。2.3 Znode节点通常分为两类： 第一类同样是指构成集群的机器，我们称之为机器节点。 第二类则是指数据模型中的数据单元，我们称之为数据节点一ZNode。ZooKeeper 将所有数据存储在内存中，数据模型是一棵树（Znode Tree)，由斜杠（/）的进行分割的路径，就是一个 Znode，例如/foo/path1。每个上都会保存自己的数据内容，同时还会保存一系列属性信息。在 Zookeeper 中，Node 可以分为持久节点和临时节点两类。 持久节点是指一旦这个 ZNode 被创建了，除非主动进行 ZNode 的移除操作，否则这个 ZNode 将一直保存在 ZooKeeper 上。 临时节点就不一样了，它的生命周期和客户端会话绑定，一旦客户端会话失效，那么这个客户端创建的所有临时节点都会被移除。 另外，ZooKeeper 还允许用户为每个节点添加一个特殊的属性：SEQUENTIAL。 一旦节点被标记上这个属性，那么在这个节点被创建的时候，ZooKeeper 会自动在其节点名后面追加上一个整型数字，这个整型数字是一个由父节点维护的自增数字。2.4 版本在前面我们已经提到，Zookeeper 的每个 ZNode 上都会存储数据，对应于每个 ZNode，Zookeeper 都会为其维护一个叫作 Stat 的数据结构。 cZxid：这是导致创建znode更改的事务ID。 mZxid：这是最后修改znode更改的事务ID。 pZxid：这是用于添加或删除子节点的znode更改的事务ID。 ctime：表示从1970-01-01T00:00:00Z开始以毫秒为单位的znode创建时间。 mtime：表示从1970-01-01T00:00:00Z开始以毫秒为单位的znode最近修改时间。 dataVersion：表示对该znode的数据所做的更改次数。 cversion：这表示对此znode的子节点进行的更改次数。 aclVersion：表示对此znode的ACL进行更改的次数。 ephemeralOwner：如果znode是ephemeral类型节点，则这是znode所有者的 session ID。 如果znode不是ephemeral节点，则该字段设置为零。 dataLength：这是znode数据字段的长度。 numChildren：这表示znode的子节点的数量。2.5 WatcherWatcher（事件监听器），是 ZooKeeper 中的一个很重要的特性。ZooKeeper 允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。3. Zookeeper设计目标3.1 数据模型ZooKeeper 允许分布式进程通过共享的层次结构命名空间进行相互协调，这与标准文件系统类似。名称空间由 ZooKeeper 中的数据寄存器组成，称为 Znode，这些类似于文件和目录。与为存储设计的典型文件系统不同，ZooKeeper 数据保存在内存中，这意味着 ZooKeeper 可以实现高吞吐量和低延迟。3.2 可构建集群为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。客户端在使用 ZooKeeper 时，需要知道集群机器列表，通过与集群中的某一台机器建立 TCP 连接来使用服务。客户端使用这个 TCP 链接来发送请求、获取结果、获取监听事件以及发送心跳包。如果这个连接异常断开了，客户端可以连接到另外的机器上。ZooKeeper 官方提供的架构图：4. ZooKeeper 集群角色介绍 ZooKeeper 集群中的所有机器通过一个 Leader 选举过程来选定一台称为 “Leader” 的机器。 Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 和 Observer 都只能提供读服务。 当Client向Follow、Observer写入数据时，写入操作会同步到Leader，Leader会将同步的信息分发到其他节点上，当大多数节点写入完成后，会向Client返回响应 Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。5.Shell命令 zkCli.sh / zookeeper-client 进入 help 查看帮助信息 ls / 查看 “/” 节点下的信息 ls2 / 查看 “/” 节点下的详细信息 create /hive “[数据信息]” 创建节点 get /bigdata 获取节点信息 create -e /bigdata “aaa” 创建临时节点（退出之后删除 创建序号节点 create -s /bigdata/Impala “node1” create -s /bigdata/Impala “node2” set /bigdata “Flink” 修改节点信息 监听 get /bigdata [watch] 当该节点值改变时，会得到响应 ls /bigdata [watch] 当节信息改变时，会得到响应 注册一次，监听一次 delete /bigdata/Hive 删除节点 rmr /bigdata 递归删除 stat / 查看状态 6. 监听器原理 main()线程启动 创建zookeeper-client，创建两个线程connect和listener 通过connect线程将注册信息发送给zookeeper zookeeper监听器将注册监听的事件添加到注册的监听事件列表 例如：get /bigdata watch client zookeeper监听到有数据或路径的变化，就会发送消息到listener线程 listener线程内部调用process()方法，即自己编写的对应措施参看文章 51CTO 官方文档" }, { "title": "Hive Update、Delete操作配置", "url": "/posts/Hive-Update-Delete%E6%93%8D%E4%BD%9C%E9%85%8D%E7%BD%AE/", "categories": "大数据框架, hive", "tags": "hadoop, hive", "date": "2018-11-13 00:00:00 +0800", "snippet": "Hive Update、Delete操作配置条件 只支持ORC存储格式 表必须分桶 更新指定配置文件创建存储为ORC的分桶表CREATE TABLE table_name ( id int, name string)CLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORCTBLPROPERTIES (&quot;transactional&quot;=&quot;true&quot;);修改配置文件修改添加hive中的hive-site.xml配置文件&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hive.support.concurrency&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hive.enforce.bucketing&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hive.exec.dynamic.partition.mode&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;nonstrict&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hive.txn.manager&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;org.apache.hadoop.hive.ql.lockmgr.DbTxnManager&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hive.compactor.initiator.on&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;如果是CDH，如图所示如果上述配置文件不生效，则在hive-site.xml 的 Hive Metastore Server 高级配置代码段（安全阀）与上述相同的配置" }, { "title": "Impala 详解", "url": "/posts/Impala-%E8%AF%A6%E8%A7%A3/", "categories": "大数据框架, impala", "tags": "hadoop, impala", "date": "2018-11-12 00:00:00 +0800", "snippet": "Impala 简介 基于Google的Dremel 为原型的查询引擎，Cloudera公司推出，提供对HDFS、HBase数据的高性能、低延迟的交互式SQL查询功能 Impala是一个分布式、大规模并行处理（MPP）的服务引擎 使用内存进行Hive，兼顾数据仓库、实时、批处理、多并发等优点Impala各进程角色 State Store Daemon 负责收集分布在各个ImpalaD进程的资源信息、各节点健康状况，同步节点信息 负责调度Query Catalog Daemon 主要跟踪各个节点上对元数据的变更操作，并且通知到每个ImpalaD。 接受来自StateStore的所有请求 Impala Daemon Query Planner接收来自SQL APP和ODBC的查询，然后将查询转换为许多子查询 Query Coordinator将这些子查询分发到各个节点上 各个节点上的Query Exec Engine负责子查询的执行，然后返回子查询的结果，这些中间结果经过聚集之后最终返回给用户。 Impala查询数据流程 注册&amp;amp;订阅：当Impala启动时，所有Impalad节点会在Impala State Store中注册并订阅各个节点最新的健康信息以及负载情况。 提交查询：接受此次查询的ImpalaD作为此次的Coordinator，对查询的SQL语句进行分析，生成并执行任务树，不同的操作对应不同的PlanNode，如：SelectNode、 ScanNode、 SortNode、AggregationNode、HashJoinNode等等。 获取元数据与数据地址：Coordinator通过查询数据库，或者HDFS文件获取到此次查询的数据库所在的具体位置，以及存储方式的信息 分发查询任务：执行计划树里的每个原子操作由Plan Fragment组成，通常一句SQL由多个Fragment组成。Coordinator节点根据执行计划以及获取到的数据存储信息，通过调度器将Plan Fragment分发给各个Impalad实例并行执行。 汇聚结果：Coordinator节点会不断向各个Impalad执行子节点获取计算结果，直到所有执行计划树的叶子节点执行完毕，并向父节点返回计算结果集。 Coordinator节点即执行计划数的根节点，汇聚所有查询结果后返回给用户。查询执行结束，注销此次查询服务。Impala优缺点优点： Impala直接在内存进行计算不需要把中间结果写入磁盘，省掉了大量的I/O开销； 支持Data locality的I/O调度机制，尽可能地将数据和计算分配在同一台机器上进行，减少了网络开销； Impala直接通过应用的服务进程来进行任务调度，省掉了MR作业启动的开销 使用C++实现，并做了硬件优化缺点： 对内存需求过高，没有内存溢出写入外存的机制； 不支持数据的UPDATE/DELETE，对配置类数据的处理不好； 对数据挖掘类的操作处理还不够丰富，但已能满足日常大部分的统计分析需求。Impala Join操作Broadcast Join Impala将较小的表通过网络分发到执行任务的Impala后台进程中 小表数据分发并缓存完成后，大表的数据就流式地通过内存中小表的哈希表。每个Impala进程负责大表的一部分数据，扫面读入，并用哈希连接的函数计算值。 大表的数据一般由Impala进程从本地磁盘读入从而减少网络开销。由于小表的数据已经缓存在每个节点中，因此在此阶段唯一可能的网络传输就是将结果发送给查询计划中的另一个连接节点。Partitioned Hash Join分区哈希连接需要更多的网络开销，但可以允许大表的连接而不要求整个表的数据都能放到一个节点的内存中。当统计数据显示表太大而无法放到一个节点的内存中或者有查询提示时就会使用分区哈希连接。进行分区哈希连接时（也称为shuffle join），每个Impala进程读取两个表的本地数据，使用一个哈希函数进行分区并把每个分区分发到不同的Impala进程。正如上图所示，大表的数据也通过相同的哈希函数就行分区并把分区发送能和小表相应数据进行连接的结点。注意，和广播连接不同的是，广播连接只有小表的数据需要通过网络分发，而分区哈希连接需要通过网络分发大表和小表的数据，因此需要更高的网络开销。 Impala有两种连接策略：广播连接，需要更多的内存并只适用于大小表连接。分区连接，需要更多的网络资源，性能比较低，但是能进行大表之间的连接。Impala中的资源管理 静态资源池 CDH中将各服务彼此隔开，分配专用的资源 动态资源池 用于配置及用于在池中运行的yarn或impala查询之间安排资源的策略 Impala2.3之前使用的是yarn作为资源调度，2.3之后自身的资源调度策略Long-Lived Application Master，即LIAMAImpala使用 查看当前语句所需资源explain [sql语句]explain select count(*) from action; 设置资源池名称set request_pool = impala100; 设置最大内存使用限制set mem_limit=100m 设置执行计划显示信息详细程度，等级越高越详细set explain_level=3; 同步HIVE所有的数据库信息到ImpalaINVALIDATE METADATA; 同步某张表信息REFRESH [table_name]Impala优化（待补充） 维度建模 维度表 事实表 星型模型 雪花模型 文件存储格式 Parquet Text Avro Rcfile SQL Tuning Explain Join 数据裁剪 " }, { "title": "Yarn 详解", "url": "/posts/YARN-%E8%AF%A6%E8%A7%A3/", "categories": "大数据框架, yarn", "tags": "hadoop, yarn", "date": "2018-11-08 00:00:00 +0800", "snippet": "1. YARN 的组件以及架构1.1 ContainerContainer是Yarn框架的计算单元，是具体执行应用task（如map task、reduce task）的基本单位。Container和集群节点的关系是：一个节点会运行多个Container，但一个Container不会跨节点。任何一个job或application必须运行在一个或多个Container中，在Yarn框架中，ResourceManager只负责告诉ApplicationMaster哪些Containers可以用，ApplicationMaster还需要去找NodeManager请求分配具体的Container。1.2 NodeManagerNodeManager 运行在集群的节点上，每个节点都有自己的NodeManager。他负责接受ResourceManager的资源分配请求，分配给具体的Container应用。同时负责将自身的检测信息传输给ResourceManager。通过和ResourceManager配合，NodeManager负责整个Hadoop集群中的资源分配工作。ResourceManager是一个全局的进程，而NodeManager只是每个节点上的进程，管理这个节点上的资源分配和监控运行节点的健康状态。当一个节点启动时，它会向ResourceManager进行注册并告知ResourceManager自己有多少资源可用。在运行期，通过NodeManager和ResourceManager协同工作，这些信息会不断被更新并保障整个集群发挥出最佳状态。NodeManager只是负责监控Container，并不知道里面运行着什么 接收ResourceManager的请求，分配Container给应用的某个任务 和ResourceManager交换信息以确保整个集群平稳运行。ResourceManager就是通过收集每个NodeManager的报告信息来追踪整个集群健康状态的，而NodeManager负责监控自身的健康状态。 管理每个Container的生命周期 管理每个节点上的日志 执行Yarn上面应用的一些额外的服务，比如MapReduce的shuffle过程1.3 ResourceManagerResourceManager的主要主件有两个：Scheduler，ApplicationManager Scheduler：资源调度器，主要协调集群中个资源的分配，保证整个集群运行效率。Scheduler只是一个调度器，只是负责分配Container，并不会监控以及管理Container的状态以及运行的任务。同样，也不会处理任务失败硬件错误等等； ApplicationManager：主要负责任务的提交，为应用分配一个Container用来运行ApplicationMaster，同时负责监控ApplicationMaster，在任务失败时会重启ApplicationMaster；1.4 ApplicationMasterApplicationMaster负责向ResourceManager申请资源并和NodeManager协同工作来运行各个应用，同时跟踪他们的状态以及每个任务的运行，遇到失败后负责重启他们在MR1中，JobTracker即负责资源请求调度，同时还负责对Job的监控。在MR2中对任务的监控交给了ApplicationMaster。yarn相对与MR1来说优势有哪些呢？ 这个设计大大减小了 ResourceManager 的资源消耗，并且让监测每一个 Job 子任务 (tasks) 状态的程序分布式化了，更安全、更优美。 在新的 Yarn 中，ApplicationMaster 是一个可变更的部分，用户可以对不同的编程模型写自己的 AppMst，让更多类型的编程模型能够跑在 Hadoop 集群中，可以参考 hadoop Yarn 官方配置模板中的 mapred-site.xml 配置。 对于资源的表示以内存为单位 ( 在目前版本的 Yarn 中，没有考虑 cpu 的占用 )，比之前以剩余 slot 数目更合理。 老的框架中，JobTracker 一个很大的负担就是监控 job 下的 tasks 的运行状况，现在，这个部分就扔给 ApplicationMaster 做了，而 ResourceManager 中有一个模块叫做 ApplicationsManager，它是监测 ApplicationMaster 的运行状况，如果出问题，会将其在其他机器上重启。 Container 是 Yarn 为了将来作资源隔离而提出的一个框架。这一点应该借鉴了 Mesos 的工作，目前是一个框架，仅仅提供 java 虚拟机内存的隔离 ,hadoop 团队的设计思路应该后续能支持更多的资源调度和控制 , 既然资源表示成内存量，那就没有了之前的 map slot/reduce slot 分开造成集群资源闲置的尴尬情况。2. Yarn中的资源请求2.1应用提交过程 Client向Resource Manager提交任务并请求创建一个ApplicationMaster ResourceManager会找到一个可以运行Container的NodeManager创建一个Container并运行ApplicationMaster ApplicationMaster创建完成以后会向ResourceManager进行注册，注册完成后Client就可以查询ResourceManager来获取ApplicationMaster的详细信息，以及和ApplicationMaster进行交互 运行起来后做些什么依赖程序本身，可能进行一个简单的计算后返回给客户端，或者向ResourceManager请求更多的Container用于分布式计算。以下是请求更多资源的流程 Yarn允许程序指定依照本地化对Node Manager进行限制 当Container被成功分配之后，ApplicationMaster通过向NodeManager发送container-launch-specification信息来启动Container， container-launch-specification信息包含了能够让Container和ApplicationMaster交流所需要的资料 应用程序的代码在启动的Container中运行，并把运行的进度、状态等信息通过application-specific协议发送给ApplicationMaster 在应用程序运行期间，提交应用的客户端主动和ApplicationMaster交流获得应用的运行状态、进度更新等信息，交流的协议也是application-specific协议 一但应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster向ResourceManager取消注册然后关闭，用到所有的Container也归还给系统2.2 Resource Request和ContainerYarn的设计目标就是允许我们的各种应用以共享、安全、多租户的形式使用整个集群。并且，为了保证集群资源调度和数据访问的高效性，Yarn还必须能够感知整个集群拓扑结构。为了实现这些目标，ResourceManager的调度器Scheduler为应用程序的资源请求定义了一些灵活的协议，通过它就可以对运行在集群中的各个应用做更好的调度，因此，这就诞生了Resource Request和Container。具体来讲，一个应用先向ApplicationMaster发送一个满足自己需求的资源请求，然后ApplicationMaster把这个资源请求以resource-request的形式发送给ResourceManager的Scheduler，Scheduler再在这个原始的resource-request中返回分配到的资源描述Container。每个ResourceRequest可看做一个可序列化Java对象，包含的字段信息如下： resource-name：资源名称，现阶段指的是资源所在的host和rack，后期可能还会支持虚拟机或者更复杂的网络结构 priority：资源的优先级 resource-requirement：资源的具体需求，现阶段指内存和cpu需求的数量 number-of-containers：满足需求的Container的集合number-of-containers中的Containers就是ResourceManager给ApplicationMaster分配资源的结果。Container就是授权给应用程序可以使用某个节点机器上CPU和内存的数量。2.3 各组件之间心跳信号ApplicationMaser &amp;amp;&amp;amp; ResourceManager AM -&amp;gt; RM 对Container资源请求和优先级 已完成的Container RM -&amp;gt;AM 新申请到的Container信息 ApplicationMaster &amp;amp;&amp;amp; NodeManager AM -&amp;gt; NM 启动Container请求 NM -&amp;gt; AM Container状态 NodeManager &amp;amp;&amp;amp; ResourceManager NM -&amp;gt; RM NodeManager上所有Container状态 RM -&amp;gt; NM 已删除和等待清理的Container列表 3. Yarn中的资源调度 FIFO Capacity Scheduler Fair SchedulerFIFO 调度器 FIFO调度器将所有的应用放置到一个队列当中，然后采用先进先出的顺序运行应用。首先第一个应用进来请求资源，直到请求资源被全部满足以后再进行对下一个任务的资源调度优点 简单易懂 方便配置缺点 资源利用率不高 不允许资源抢占 不适合在共享集群上使用Capacity Scheduler 容量调度器 允许多个组织共享一个Hadoop集群，每个组织分配到全部集群的一部分。每个组织有一个专门的队列，每个队列被配置为一定的集群资源。队列可以进一步划分，这样每个队列中的不用用户可以共享队列中所分配的资源。 保证每个任务都有最低资源保证 当队列空闲时，其他资源允许分配给其他队列使用 单个队列内部支持FIFO 综合考虑多方面因素，防止单个作业、用户或者队列占领全部资源 支持内存资源和CPU资源的调度Fair Scheduler 公平调度器 根据队列数量对资源进行公平分配" }, { "title": "HIVE:JOIN原理、优化", "url": "/posts/HIVEJOIN%E5%8E%9F%E7%90%86-%E4%BC%98%E5%8C%96/", "categories": "大数据框架, hive", "tags": "hadoop, hive", "date": "2018-11-07 00:00:00 +0800", "snippet": "1. Join原理 有两个表User、Order如上，进行Join操作 SELECT u.name, o.orderidFROM user uJOIN order o ON u.uid = o.uid; Hive会将On之后的条件作为Key，将Select的字段作为Value，构建（Key,Value），同时为每张表打上Tag标记用来标记自己是哪站表。 在Shuffle阶段按Key分组 在Reduce阶段进行数据整合 2. 各种Join操作2.1 Inner Join(内连接)SELECT u.name, o.orderidFROM my_user u[INNER] JOIN my_order o ON u.uid = o.uid;2.2 Left Outer Join(左外连接)SELECT u.name, o.orderidFROM my_user uLEFT OUTER JOIN my_order o ON u.uid = o.uid;2.3 Right Outer Join(右外连接)SELECT u.name, o.orderidFROM my_user uRIGHT OUTER JOIN my_order o ON u.uid = o.uid;2.4 Full Outer Join(全外连接)SELECT u.name, o.orderidFROM my_user uFULL OUTER JOIN my_order o ON u.uid = o.uid;2.5 Left Smei Join(左半开连接) 只能Select昨天表的内容，也只会输出左边表的内容SELECT *FROM my_user uLEFT SEMI JOIN my_order o ON u.uid = o.uid;3. Join优化 在正常生产环境下，上述Join操作虽然通用，但是会很浪费时间，因为不仅需要Map阶段，还需要Reduce阶段整合数据，所以上述Join操作也称作（Reduce Side Join）3.1 Map Side Join 省略Reduce端，直接在Map端进行整合数据 也就是将其中一张表分别放入每个Map端，这样就可以在Map端将两张表进行整合，但前提是能分别放入每个Map端的那张表必须足够小上面就是Map Side Join的原理了，可以看出每个Mapper里面都会有一个Small Table Data，这样就可以在Map端完成两张表的Join 默认情况下，25M一下的算小表，该属性由 hive.smalltable.filesize 决定。-- 使用方式一：-- 使用 /*+ MAPJOIN(tbl)*/ tbl为表名SELECT /*+ MAPJOIN(my_order)*/ u.name, o.orderidFROM my_user uLEFT OUTER JOIN my_order o ON u.uid = o.uid;-- 方式二：设置hive.auto.convert.join = true，这样hive会自动判断当前的join操作是否合适做map join，主要是找join的两个表中有没有小表。3.2 Bucket Map Join 但是当两张表都不是小表改怎么时，就需要使用Bucket Map JoinBucket Map Join 使用需求 两张表的连接字段必须为分桶字段 两张表的分桶数量必须相同或是倍数关系 设置属性hive.optimize.bucketmapjoin= true控制hive 执行bucket map join Map端接受N个小表的Hashtable，做连接操作时，只需要Hashtable放入内存，然后将大表对应的Hashtable进行连接，所以内存限制为最大的那张Hashtable 3.2 Sort Merge Bucket Map Join 如果对于Bucket Map Join中的两张分桶表是有序的，是可以进行Sort Merge Bucket Map Join由于两张表是有序的，那么在两张表每个桶局部连接时，只需要将每张表便利一次便可以完成整合操作，甚至不用把一个Bucket完整的加载成Hashtable使用设置set hive.optimize.bucketmapjoin= true;set hive.optimize.bucketmapjoin.sortedmerge = true;set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;参考资料Data Valley" }, { "title": "HDFS系统详解", "url": "/posts/HDFS%E7%B3%BB%E7%BB%9F%E8%AF%A6%E8%A7%A3/", "categories": "大数据框架, hdfs", "tags": "hadoop, hdfs", "date": "2018-11-07 00:00:00 +0800", "snippet": "目录 HDFS设计原理 HDFS核心概念 上传1. HDFS设计原则1.1 设计目标 存放非常大的文件 采用流式数据的访问方式；一点一点的读，而不是一次读全部 运行在商业集群上面1.2 HDFS不适用场景类型 低延迟访问 对延时要求在毫秒级别的应用，不适合采用HDFS。HDFS是为高吞吐数据传输设计的,因此可能牺牲延时HBase更适合低延时的数据访问。 大量小文件 由于每个文件的信心都会由Namenode记录，当小文件过多时，整个系统会受到内存限制，效率降低 多方读写，需要任意修改 2. HDFS核心概念2.1 Block每个文件都是由一个一个的Block组成（Block默认大小128M），例如一个300M的文件会被保存成3个Block，而一个3K的文件也统一会占用一个Block，只不过这个Block只会占用3KDataNode使用Block存放的原因： 方便大文件的存放 可用性更高 Block有规律的存放和读取鉴于DataNode的保存机制，在使用hdfs 的时候需要注意什么2.2 NameNode&amp;amp;&amp;amp;DataNode 整个HDFS采用两类节点管理，即一个NameNode和多个DataNode。2.2.1 Namenode管理整个文件系统的目录树以及所有的文件、目录和元数据。元数据持久化为两种形式： fsimage :整个Namenode的快照 edit log : 上次快照到目前为止的所有操作信息fsimage、edit log会在首次hdfs系统formate的时候创建，再以后的 formate 会对fsimage、editlog进行删除后重建，不会对整个系统文件产生影响。重启集群后DataNode会重新想NameNode发送Block信息，NameNode重新获得整个集群的数据2.2.2 DataNode数据节点负责存储和提取Block，读写请求可能来自namenode，也可能直接来自客户端。数据节点周期性向Namenode汇报自己节点上所存储的Block相关信息。2.3 HDFS Federation产生原因：由于在一个庞大集群当中会有很多操作，而将所有操作都记录到同一个节点的NameNode的editlog上时，可能存在内存不够用的情况。解决办法：HDFS Federation是一种横向拓展的方式，在HDFS Federation中，每个NameNode都只记录一个命名空间。例如：/usr，可以只交给一个NameNnode管理2.4 备份机制 在Namenode的上传保存一个文件时，是以Block的形式保存，默认会保存三份。备份机制 第一份会放在存储最少的节点上 放在另一个机架的节点上面 放在当前机架的另一个节点上面 备份的数量最高为当前节点的数量2.5 HDFS HA(High Availability)产生原因：当NameNode出现某些异常宕机时，整个系统将变得无法访问解决办法：HDFS HA(High Availability)，通过启动两个NameNode，分别处于Active-Standby。当Active节点失效时，Standby会顶替上，在处理的过程中也没有任何中断的迹象 NameNode之间需要通过High Availability共享实现编辑日志共享，Standby节点接管工作以后会读取日志文件，实现与Active节点状态同步 DataNode会同时向两个节点发送Block信息，因为数据块映射信息存放在内存当中，而非磁盘 High Availability实现共享的方式有两种 NFS过滤器 集群日志管理(QJM,quorum journal manager):QJM是一个专用的HDFS实现，为其提供一个高可用的编辑日志而设计，每一次的编辑都写入多个日志节点 文件上传文件上传流程 客户端找到Namenode进行上传 Namenode返回是否可以上传，以及上传的信息，包含分成的BLock以及每块上传的地址 客户端与DataNode建立连接上传数据 Namenode指挥DataNode对上传的文件进行备份下载雷同" }, { "title": "Hive基础操作", "url": "/posts/Hive%E6%93%8D%E4%BD%9C/", "categories": "大数据框架, hive", "tags": "hadoop, hive", "date": "2018-11-06 00:00:00 +0800", "snippet": "创建数据库cearte database [if not exists] db.hive;表的相关操作-- 创建表create table [if not exists] student( name string, age int, score int)row format delimited fileds terminated by &#39;\\t&#39;;-- 创建子表——从表中提取出所需要的字段create table if not exists db_hive.emp_catsasselect name, age, from student;-- 清除表数据，但不删除表truncate table dept_cats;-- 建立表结构相同的表create table if not exists student_likeliketest.student;-- 修改表的名称alter table dept_like rename to dept_like_rename;-- 删除表（好像会卡住drop table if exists dept_like_rename;-- 创建外部表-- 外部表在删除时不会删掉内置的数据create external table if not exists db_hive.emp_external(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)row format delimited fields terminated by &#39;\\t&#39;location &#39;/user/root/hive/warehouse/emp_external&#39;;-- 建立二分区表set hive.exec.dynamic.partition.mode=nonstrict; # 允许动态分区-- 静态分区 create external table if not exists db_hive.emp_partition2(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)PARTITIONED BY (month string, day string)row format delimited fields terminated by &#39;\\t&#39;location &#39;/user/root/hive/warehouse/emp_external&#39;;load data local inpath &#39;/root/data_emp/emp.txt&#39; into table db_hive.emp_partition2partition (month=&#39;201810&#39;, day=&#39;21&#39;);SELECT * FROM emp_partition2 where month=&#39;201810&#39; and day=&#39;21&#39;;-- 动态分区(分区的字段必须在后面)FROM raw_access_log INSERT OVERWRITE TABLEpartitioned_raw_access_logPARTITION(year,month,day)SELECT ip.....year,month,day-- 混合模式FROM raw_access_log INSERT OVERWRITE TABLEpartitioned_raw_access_logPARTITION(year=&quot;2007&quot;,month,day)SELECT ip.....month,day-- 分桶模式FROM table_testCLUSTERED BY (column_name,...) INTO 8 BUCKETS//与分区不同的是列名必须是已经存在的列名-- 将数据导入分桶表-- 首先设置reduce数量和Buckets数量一直，即最终决定产生多少个文件-- hive.enforce.sorting和hive.enforce.bucketing设置为true，免了dirtribute by &amp;amp; sort bySET hive.enforce.bucketing=true;SET mapred.reduce.tasks=200;FROM table_faINSERT OVERWRITE TABLE table_sonSELECT ...,request_dateWHERE ...[DISTRIBUTE BY ...][SORT BY ...]-- 修复分区MSCK REPAIR TABLE table_name;四个by-- order by 全局排序，仅仅只有一个reducer select * from emp order by empno desc;-- sort by 对每一个reduce内部进行排序，全局结果集不是排序的 set mapreduce.job.reduces=3; select * from emp sort by empno asc; insert overwrite local directory &#39;/opt/datas/sortby_res&#39; select * from emp sort by empno asc;-- distribute by 分区，类似mapreduce中partitioner，对数据分区， 通常结合sort by使用 insert overwrite local directory &#39;/opt/datas/distby_res&#39; select * from emp distribute by deptno sort by empno asc;-- cluster by 当distribute by的字段和sort by的字段相同时就可以用cluster by代替 insert overwrite local directory &#39;/opt/datas/clusterby_res&#39; select * from emp cluster by empno asc;获取表的信息-- 获取基本信息desc student;-- 获取详细信息desc formatted student数据导入将文件信息导入# 将本地数据信息载入数据表load data local inpath &#39;/data/student.txt&#39; into table db_hive.student;# 将HDFS数据载入数据表load data inpath &#39;/student.txt&#39; into table db_hive.student;将表中信息筛筛选导入hive&amp;gt; insert into table test &amp;gt; partition (age=&#39;25&#39;) &amp;gt; select id, name, tel &amp;gt; from wyp;数据导出 # 1.常见导出insert overwrite local directory &#39;/opt/datas/test.txt&#39; select * from emp; # 2.定义格式insert overwrite local directory &#39;/opt/datas/test1.txt&#39; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\\t&#39; COLLECTION ITEMS TERMINATED BY &#39;\\n&#39;select * from emp; # 3.hive -e &quot;select * from ...&quot; &amp;gt; /opt/datas/xx.txt # 4.保存到HDFS上 insert overwrite directory &#39;&#39; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#39;\\t&#39; COLLECTION ITEMS TERMINATED BY &#39;\\n&#39; select * from emp_partition; 5.sqoop" }, { "title": "Hive配置安装", "url": "/posts/%E5%AE%89%E8%A3%85hive/", "categories": "大数据框架, hive", "tags": "hadoop, hive", "date": "2018-10-26 00:00:00 +0800", "snippet": "1. 安装Mysql 在集群中选择一台节点进行安装mysql yum -y install mariadb-server mariadb 开启服务并配置开机自启 systemctl start mariadb.servicesystemctl enable mariadb.service 设置密码，第一次登录时密码为空，之后设置使用sql语句设置密码 mysql -u root -p# 登录之后，先查看databases是否正常，之后sql语句设置密码&amp;gt; use mysql;&amp;gt; update user set password=password( &#39;123456&#39; ) where user= &#39;root&#39; ;# 然后设置root用户可以从任何主机登陆，对任何的库和表都有访问权限&amp;gt; grant all privileges on *.* to root@&#39;%&#39; identified by &#39;123456&#39;;&amp;gt; grant all privileges on *.* to root@&#39;hadoop1&#39; identified by &#39;123456&#39;;&amp;gt; grant all privileges on *.* to root@&#39;localhost&#39; identified by &#39;123456&#39;;&amp;gt; FLUSH PRIVILEGES; 修改mariadb的数据地址，只是集群节点中必要的设置 # 1.停止服务 systemctl stop mariadb.service# 2.复制原来的配置到系统盘外的磁盘(举例是/data01) cp -r /var/lib/mysql/ /data01/# 3.备份原来的设置 mv /var/lib/mysql/ /var/lib/mysql.bak/# 4.修改磁盘中文件夹的所属权限 chown -R mysql:mysql /data01/mysql# 5.创建软连接 ln -s /data01/mysql/ /var/lib/mysql# 6.重启Mariadb systemctl restart mariadb 所有节点安装mysql-connector驱动 yum -y install mysql-connector-java 安装之后的路径为/usr/share/java/mysql-connector-java.jar 安装其他依赖包 yum -y install psmisc yum -y install perl yum -y install nfs-utils portmapsystemctl start rpcbind systemctl enable rpcbind 创建数据库和用户 create database hive DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database amon DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database hue DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database monitor DEFAULT CHARSET utf8 COLLATE utf8_general_ci;create database oozie DEFAULT CHARSET utf8 COLLATE utf8_general_ci;CREATE USER &#39;hive&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;hive&#39;; GRANT ALL PRIVILEGES ON hive.* TO &#39;hive&#39;@&#39;localhost&#39;; CREATE USER &#39;hive&#39;@&#39;%&#39; IDENTIFIED BY &#39;hive&#39;; GRANT ALL PRIVILEGES ON hive.* TO &#39;hive&#39;@&#39;%&#39;; CREATE USER &#39;hive&#39;@&#39;hadoop1&#39;IDENTIFIED BY &#39;hive&#39;; GRANT ALL PRIVILEGES ON hive.* TO &#39;hive&#39;@&#39;hadoop1&#39;; CREATE USER &#39;oozie&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;oozie&#39;; GRANT ALL PRIVILEGES ON oozie.* TO &#39;oozie&#39;@&#39;localhost&#39;; CREATE USER &#39;oozie&#39;@&#39;%&#39; IDENTIFIED BY &#39;oozie&#39;; GRANT ALL PRIVILEGES ON oozie.* TO &#39;oozie&#39;@&#39;%&#39;; CREATE USER &#39;oozie&#39;@&#39;hadoop1&#39;IDENTIFIED BY &#39;oozie&#39;; GRANT ALL PRIVILEGES ON oozie.* TO &#39;oozie&#39;@&#39;hadoop1&#39;; CREATE USER &#39;monitor&#39;@&#39;localhost&#39; IDENTIFIED BY &#39;monitor&#39;; GRANT ALL PRIVILEGES ON monitor.* TO &#39;monitor&#39;@&#39;localhost&#39;; CREATE USER &#39;monitor&#39;@&#39;%&#39; IDENTIFIED BY &#39;monitor&#39;; GRANT ALL PRIVILEGES ON monitor.* TO &#39;monitor&#39;@&#39;%&#39;; CREATE USER &#39;monitor&#39;@&#39;hadoop1&#39;IDENTIFIED BY &#39;monitor&#39;; GRANT ALL PRIVILEGES ON monitor.* TO &#39;monitor&#39;@&#39;hadoop1&#39;; FLUSH PRIVILEGES; 2. Hive的配置 hive只需要在一节点安装（服务端）即可 根据对应hadoop版本下载软件包 上传并解压到节点 配置环境变量 修改配置文件 cd ../hive/conf# 去掉后面的模板后缀# 1.hive-env.sh中添加信息: export JAVA_HOME=... export HADOOP_HOME=. export HIVE_HOME=...# 2.hive-log4j.properties 修改 log4j.appender.EventCounter=org.apache.hadoop.log.metrics.EventCounter# 3.hive-site.xml： &amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;javax.jdo.option.ConnectionURL&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;jdbc:mysql://hadoop1:3306/hive?createDatabaseIfNotExist=true&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;javax.jdo.option.ConnectionDriverName&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;com.mysql.jdbc.Driver&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;javax.jdo.option.ConnectionUserName&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;root&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;javax.jdo.option.ConnectionPassword&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;123456&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;/configuration&amp;gt;# 5.把hive/lib下的jline2.12拷贝到hadoop下的share/hadoop/yarn/lib/下，若存在旧版本就替换掉# 6.把mysql-connector这个jar包，拷贝到hive下的lib下 3. 启动Hivehive" }, { "title": "Hadoop集群配置", "url": "/posts/hadoop%E9%9B%86%E7%BE%A4%E5%AE%89%E8%A3%85/", "categories": "大数据框架, hadoop", "tags": "hadoop", "date": "2018-10-24 16:44:13 +0800", "snippet": "[TOC]1.多台机器ssh免密配置修改用户名# 1.更改hostnamehostnamectl --static set-hostname &amp;lt;主机名&amp;gt;scp传输文件scp &amp;lt;文件路径&amp;gt; &amp;lt;目标账号@地址&amp;gt;: 目标路径 scp /etc/hosts root@hadoop2: /etc/ssh免密登录# 配置公钥ssh-keygen # 配置免密登录ssh-copy-id &amp;lt;目标ip&amp;gt;2. 多台主机时间核对所有机器安装ntpyum -y install ntp修改主机配置文件1. 备份配置文件cp /etc/ntp.conf /etc/ntp.conf.bak2. 修改主机配置文件vim /etc/ntp.conf # server 0.centos.pool.ntp.org iburst# server 1.centos.pool.ntp.org iburst# server 2.centos.pool.ntp.org iburst# server 3.centos.pool.ntp.org iburstservice 127.127.1.1 #核对时间的一个IP3.重启ntpd进程，设置开机自启systemctl restart ntpdsystemctl enable ntpd配置其他主机# 1. 校对其他主机时间ntpdate hadoop1 #与主机校对时间systemctl start ntpd #启动服务crontab # 配置定是脚本# 2.修改其他主机配置文件vim /etc/ntp.conf # server 0.centos.pool.ntp.org iburst# server 1.centos.pool.ntp.org iburst# server 2.centos.pool.ntp.org iburst# server 3.centos.pool.ntp.org iburstserver 192.168.121.10 #主主机IP# 3. 在其他主机上设置开机自启systemctl restart ntpdsystemctl enable ntpd3. Hdfs 环境变量配置# 配置JDK CDH中spark会默认到/usr/java/default目录下去找jdk，所以一般就安装在/usr/java目录下vim etc/hadoop/hadoop-env.shexport JAVA_HOME=/usr/java/latest# 配置文件etc/hadoop/core-site.xmlvim etc/hadoop/core-site.xml&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hdfs://hadoop1:9000&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;/usr/local/hadoop-2.6.0/data/tmp&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;# 配置文件vim etc/hadoop/hdfs-site.xml&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.replication&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;1&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;# python操作hdfs时需要获取权限&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.permissions&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;false&amp;lt;/value&amp;gt; ## 指定secondarynamenode节点&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.namenode.secondary.http-address&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;节点地址:50090&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;# 配置下Hadoop的环境变量（略# NameNode格式化 hdfs namenode -format# hdfs 的web监控端口50070# 服务启动hadoop-daemon.sh start namenodehadoop-daemon.sh start datanode# 创建文件夹hdfs dfs -mkdir /test# 上传文件hdfs dfs -put /etc/profile /file4. 配置Yarn# 配置文件etc/hadoop/mapred-site.xmlvim etc/hadoop/mapred-site.xml&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;mapreduce.framework.name&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;yarn&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;# 配置文件etc/hadoop/yarn-site.xml&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.nodemanager.aux-services&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;mapreduce_shuffle&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;yarn.resourcemanager.hostname&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop1&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;# slaves设置vim etc/hadoop/slaves本机ip# 启动yarnsh start-yarn.sh#启动后的Web端口为80885. 配置ZooKeeper 上传文件（zookeeper.tar.gz 解压至指定文件夹 创建data/zkData文件夹 修改配置文件 cd conf cp zoo_sample.cfg zoo.cfg vim zoo.cfg # 修改文件目录 datadir=/usr/local/zookeeper/data/zkData # 配置各节点以及端口信息(选举节点：通信节点) server.1=hadoop1:2888:3888 server.2=hadoop2:2888:3888 server.3=hadoop3:2888:3888 将zookeeper整个文件夹发送到其他节点 在data/zkData文件夹下，创建myid myid里面指定当前节点的id 即hadoop1 里面的内容为1 hadoop2里面的内容为2 修改环境部变量（选 开启服务 # 开启服务bin/zkServer.sh start# 查看角色分配以及当前状态bin/zkServer.sh status# 进入虚拟文件系统的shellbin/zkCli.sh 6. 配置Hadoop-HAhdfs-site.xml配置vim etc/hadoop/hdfs-site.xml&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; # 对整个文件系统的一个统称 &amp;lt;name&amp;gt;dfs.nameservices&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;ns1&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; # 指定这个文件系统的namenode有哪些 &amp;lt;name&amp;gt;dfs.ha.namenodes.ns1&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;nn1,nn2&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; # 指定nn1是哪个 &amp;lt;name&amp;gt;dfs.namenode.rpc-address.ns1.nn1&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop1:8020&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; # 指定nn2 是哪个 &amp;lt;name&amp;gt;dfs.namenode.rpc-address.ns1.nn2&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop2:8020&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; # 指定nn1 的访问端口 &amp;lt;name&amp;gt;dfs.namenode.http-address.ns1.nn1&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop1:50070&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; # 指定nn2的访问端口 &amp;lt;name&amp;gt;dfs.namenode.http-address.ns1.nn2&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop2:50070&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; # 共享在journalnode上的共享端口 &amp;lt;name&amp;gt;dfs.namenode.shared.edits.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;qjournal://hadoop1:8485;hadoop2:8485;hadoop3:8485/ns1&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; # 配置eidts在journalnode上的保存地址 &amp;lt;name&amp;gt;dfs.journalnode.eidts.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;/usr/local/hadoop-2.6.0/data/dfs/jn&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; # 配置proxy代理客户端 &amp;lt;name&amp;gt;dfs.client.failover.proxy.provide.nsl&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;配置core-site.xml# 配置两个namennode的隔离策略# sshfence方式# 使用这种方式，必须实现ssh无密码登录vim etc/hadoop/core-site.xml&amp;lt;configuration&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.ha.fencing.methods&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;sshfence&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.ha.fencing.ssh.private-key-files&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;/root/.ssh/id_rsa&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;fs.defaultFS&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hdfs://ns1&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt; &amp;lt;property&amp;gt; &amp;lt;name&amp;gt;hadoop.tmp.dir&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;/usr/local/hadoop-2.6.0/data/tmp&amp;lt;/value&amp;gt; &amp;lt;/property&amp;gt;&amp;lt;/configuration&amp;gt;启动集群 关闭所有其他相关的进程 启动所有的journalnode hadoop-daemon.sh start journalnode nn1 格式化并启动 hdfs namenode -formate hadoop-daemon.sh start namenode 在nn2上同步nn1的元数据信息 hdfs namenode -bootstrapStandby 启动nn2-hadoop-daemon.sh start namenode 启动所有datanode hadoop-daemon.sh start datanode 调整一个nameno为active hdfs haadmin -help (查看相关命令) hdfs haadmin -transitionToActive nn1(将nn1调整为active) 借助zookeeper，配置自动故障转移 启动时都是standby，选取一个为active 监控 ZKFC（它负责整体的故障转移控制配置# 打开自动故障转移# hdfs-site.xml:&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;dfs.ha.automatic-failover.enabled&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;true&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;# core-site.xml:# 添加zookeeper的服务&amp;lt;property&amp;gt; &amp;lt;name&amp;gt;ha.zookeeper.quorum&amp;lt;/name&amp;gt; &amp;lt;value&amp;gt;hadoop1:2181,hadoop2:2181,hadoop3:2181&amp;lt;/value&amp;gt;&amp;lt;/property&amp;gt;启动 保证所有节点文件相同，关闭所有hdfs服务 启动zookeeper集群 zkServer.sh start 初始化HA在zookeeper中的状态 hdfs zkfc -formatZK -force 启动hdfs服务 start-hdfs.sh 如果直接一起启动出现通信错误，造成namenode停止，则需要先启动journalnode，再启动其他 namenode节点启动zkfc服务 hadoop-daemon.sh start zkfc " } ]
